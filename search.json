[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geoinformática",
    "section": "",
    "text": "Prefacio\nPara nosotros en CentroGeo, la computación no es sólo una herramienta para ayudarnos a resolver diferentes problemas geoespaciales; la computación es una parte integral del proceso de análisis y una forma de pensar en geografía. Ser capaz de programar nos permite liberarnos de los algoritmos, técnicas y configuraciones que se incluyen en el software (comercial o abierto) para el análisis de datos geográficos y pensar los problemas de forma diferente. Abrir la posibilidad de automatizar los procesos de análisis no sólo hace más eficiente nuestro trabajo, sino que también nos permite pensar en los problemas de forma diferente, de forma computacional. Con el fin de ayudar a nuestros estudiantes (y a estudiantes o profesionales de otras instituciones) a adquirir las herramientas técnicas básicas para poder llevar a cabo tareras de análisis de datos geoespaciales en Python o en R hemos creado este libro.\nEste libro es (por lo pronto, pretende ser) una compilación de materiales educativos sobre Geoinformática. Busca funcionar como un apoyo para profesores interesados en impartir cursos relacionados con el uso de herramientas de programación para el análisis de datos geográficos o bien, para estudiantes independientes que busquen complementar su formación de manera autodidacta.\nEl libro y el material incluido se distribuye bajo una licencia Creative Commons, de forma que todo mundo es libre de utilizarlo y modificarlo de acuerdo a sus propiuas necesidades, siempre citando la fuente original.\nEste libro fue creado con Quarto.\nPara aprender más sobre quarto, visita: https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#organización-del-libro",
    "href": "intro.html#organización-del-libro",
    "title": "Introducción",
    "section": "Organización del libro",
    "text": "Organización del libro\nEl libro está (estará) organizado en dos grandes secciones: una dedicada a Python y otra a R. Está organizado como un conjunto de talleres. En cada taller se revisarán algunas ideas detrás del análisis de datos geoespaciales con énfasis en las herramientas y técnicas computacionales. Cada taller contiene todo el código necesario y las explicaciones básicas."
  },
  {
    "objectID": "parte_1.html",
    "href": "parte_1.html",
    "title": "Geoinformática: las herramientas básicas",
    "section": "",
    "text": "En esta parte del libro vamos a tratar de cubrir los fundamentos técnicos del procesaminto, análisis y visualización de datos geoespaciales con Python.\nPara seguir los talleres vas a necesitar varios conjuntos de datos que puedes descargar de aqui.\nEl libro está desarrollado a partir de Notebooks de Jupyter, de forma que lo más natural es que vayas siguiendo el desarrollo del libro utilizando estos notebooks.\nLa forma más sencilla de instalar Jupyter y las librerías que estaremos utilizando es utilizando el gestor de paquetes conda, que nos permite instalar fácilmente paquetes de Python sin preocuparnos por dependencias del sistema.\nExisten varis formas de instalar y trabajar con conda. Para usuarios de Windows quizá lo más sencillo sea instalar el paquete de cómputo científico Anaconda. Anaconda contiene, además del gestor de paquetes conda, muchas librerías ya preinstaladas por lo que puede resultar un poco excesivo en tamaño.\nPara trabajar de mejor forma en Python es recomendable crear environments de trabajo. Un environment es algo así como una instalación independiente de Python que contiene todo lo necesario para el desarrollo de un proyecto específico. A continuación les dejo un par de tutoriales en video para aprender a trabajar con environments de conda:\nAnaconda Beginners Guide for Linux and Windows - Python Working Environments Tutorial\nMaster the basics of Conda environments in Python\nFinalmente, conda viene configurado por defecto para utilizar los repositorios de Anaconda, Inc.. La epmpresa provee acceso a sus repositorios sin ningún costo, sin embargo en este repositorio no siempre se encuantran las versiones más actuañlizadoas y completas que vamos a necesitar. Para evitar dificultades les recomiendo utilizar los repositorios de conda-forge, acá les dejo un tutorial:\nTutorial conda-forge"
  },
  {
    "objectID": "parte_1/01_transformacion.html#conjunto-de-datos",
    "href": "parte_1/01_transformacion.html#conjunto-de-datos",
    "title": "1  Transformación de datos",
    "section": "1.1 Conjunto de Datos",
    "text": "1.1 Conjunto de Datos\nVamos a utlizar los datos del Censo de Poblacioń y Vivienda 2020 de INEGI. Trabajaremos con los datos a nivel AGEB para la Ciudad de México. Una AGEB se define como un Área Geográfica ocupada por un conjunto de manzanas perfectamente delimitadas por calles, avenidas, andadores o cualquier otro rasgo de fácil identificación en el terreno y cuyo uso de suelo es principamete habitacional, industrial, de servicios, etc.. Las AGEB’s son la unidad básica de representatividad del Marco Geoestadístico Nacional, son lo suficientemente pequeñas para representar la variabilidad espacial, pero lo suficientemente grandes para mantener la privacidad de la población y disminuir efectos de ruido estadístico.\nLos datos son publicados por INEGI en un archivo en formato csv que contiene diferentes agregaciones geográficas en el mismo archivo. Para entenderlo bien, vamos a abrirlo:\n\n\n\n\n\n\nNote\n\n\n\nEl archivo con los datos lo encuentras en la caropeta de datos del libro con el nombre conjunto_de_datos_ageb_urbana_09_cpv2020.zip\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDentro de este libro, la convención es que los datos están guardados en la carpeta datos/ relativa al notebook que se esté ejecutando.\n\n\n\ndb = pd.read_csv('datos/conjunto_de_datos_ageb_urbana_09_cpv2020.zip',\n                 dtype={'ENTIDAD': object,\n                        'MUN':object,\n                        'LOC':object,\n                        'AGEB':object})\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      0\n      09\n      Ciudad de México\n      000\n      Total de la entidad Ciudad de México\n      0000\n      Total de la entidad\n      0000\n      0\n      9209944\n      4805017\n      ...\n      1898265\n      2536523\n      2084156\n      1290811\n      957162\n      568827\n      46172\n      77272\n      561128\n      10528\n    \n    \n      1\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0000\n      Total del municipio\n      0000\n      0\n      432205\n      227255\n      ...\n      96128\n      123961\n      105899\n      66399\n      50965\n      31801\n      1661\n      2869\n      22687\n      322\n    \n    \n      2\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total de la localidad urbana\n      0000\n      0\n      432205\n      227255\n      ...\n      96128\n      123961\n      105899\n      66399\n      50965\n      31801\n      1661\n      2869\n      22687\n      322\n    \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183\n      1695\n      ...\n      741\n      772\n      692\n      313\n      221\n      145\n      8\n      14\n      148\n      5\n    \n    \n      4\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Azcapotzalco\n      0010\n      1\n      159\n      86\n      ...\n      45\n      42\n      39\n      18\n      13\n      6\n      *\n      0\n      9\n      0\n    \n  \n\n5 rows × 230 columns\n\n\n\nLa librería Pandas es la que provee la funcionalidad para trabajar con datos tabulares en Python. La estructura fundamental de Pandas es el DataFrame, podemos pensar en los DataFrames como hojas de Excel, con columnas nombradas que funcionan como indices para las variables y filas para las observaciones.\nPara leer el archivo utilizamos el método read_csv() de los DataFrames de Pandas. El parámetro dtype que le pasamos a la función nos asegura que ciertas columnas se lean con un tipo de datos especial, en este caso como object, para asegurarnos que no se lean como números y perdamos identificadores, vamos a regresar a esto más adelante.\nLa columna que nos interesa ahorita es NOM_LOC, esta nos ayuda a distinguiir los datos que vienen en cada fila: las filas etiquetadas con Total AGEB urbana contienen los conteos para cada AGEB de todas las variables, entonces, nuestra primera tarea es filtrar la base y quedarnos sólo con las columnas que en la columna NOM_LOC dice Total AGEB urbana.\n\ndb = db.loc[db['NOM_LOC'] == 'Total AGEB urbana']\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183\n      1695\n      ...\n      741\n      772\n      692\n      313\n      221\n      145\n      8\n      14\n      148\n      5\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593\n      2915\n      ...\n      1373\n      1510\n      1203\n      478\n      349\n      238\n      28\n      68\n      393\n      14\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235\n      2232\n      ...\n      965\n      1049\n      878\n      361\n      339\n      247\n      5\n      12\n      250\n      *\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768\n      2551\n      ...\n      1124\n      1237\n      1076\n      481\n      452\n      294\n      10\n      17\n      254\n      *\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176\n      1115\n      ...\n      517\n      562\n      507\n      276\n      260\n      153\n      4\n      3\n      70\n      0\n    \n  \n\n5 rows × 230 columns\n\n\n\nLo que hicimos aquí fue utlizar el selector loc de pandas para seleccionar las filas que queremos, pasándole el filtro que nos interesa, en este caso db['NOM_LOC'] == 'Total AGEB urbana'"
  },
  {
    "objectID": "parte_1/01_transformacion.html#limpieza-de-los-datos",
    "href": "parte_1/01_transformacion.html#limpieza-de-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.2 Limpieza de los datos",
    "text": "1.2 Limpieza de los datos\nHasta aquí lo que tenemos es un DataFrame con todas las variables del censo agregadas por AGEB. Ahora, para poder realizar análisis a partir de esta base de datos, necesitamos asegurarnos de que los datos son del tipo correcto, es decir, si vamos a hacer cuentas, los datos deben ser de tipo float o int. Utlicemos entonces la propiedad db.dtypes para preguntar los tipos de datos.\n\ndb.dtypes\n\nENTIDAD        object\nNOM_ENT        object\nMUN            object\nNOM_MUN        object\nLOC            object\n                ...  \nVPH_CVJ        object\nVPH_SINRTV     object\nVPH_SINLTC     object\nVPH_SINCINT    object\nVPH_SINTIC     object\nLength: 230, dtype: object\n\n\nComo podemos ver, no sólo las columnas que pedimos que leyera como object las leyó así, también las demás columnas. Esto se puede deber a que tienen codificados valores faltantes con caracteres especiales, por lo que pandas no pudo convertirlos automáticamente en números.\nPara entender esto un poco mejor, vamos a leer el diccionario de datos del censo.\n\n\n\n\n\n\nNote\n\n\n\nTambién pueden explorar el archivo en excel, para verlo con más calma\n\n\n\ndiccionario = pd.read_csv('datos/diccionario_datos_ageb_urbana_09_cpv2020.csv', skiprows=3)\ndiccionario\n\n\n\n\n\n  \n    \n      \n      Núm.\n      Indicador\n      Descripción\n      Mnemónico\n      Rangos\n      Longitud\n    \n  \n  \n    \n      0\n      1\n      Clave de entidad federativa\n      Código que identifica a la entidad federativa....\n      ENTIDAD\n      00…32\n      2\n    \n    \n      1\n      2\n      Entidad federativa\n      Nombre oficial de la entidad federativa.\n      NOM_ENT\n      Alfanumérico\n      50\n    \n    \n      2\n      3\n      Clave de municipio o demarcación territorial\n      Código que identifica al municipio o demarcaci...\n      MUN\n      000…570\n      3\n    \n    \n      3\n      4\n      Municipio o demarcación territorial\n      Nombre oficial del municipio o demarcación ter...\n      NOM_MUN\n      Alfanumérico\n      50\n    \n    \n      4\n      5\n      Clave de localidad\n      Código que identifica a la localidad al interi...\n      LOC\n      0000…9999\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      225\n      218\n      Viviendas particulares habitadas que disponen ...\n      Viviendas particulares habitadas que tienen co...\n      VPH_CVJ\n      0…999999999\n      9\n    \n    \n      226\n      219\n      Viviendas particulares habitadas sin radio ni ...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINRTV\n      0…999999999\n      9\n    \n    \n      227\n      220\n      Viviendas particulares habitadas sin línea tel...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINLTC\n      0…999999999\n      9\n    \n    \n      228\n      221\n      Viviendas particulares habitadas sin computado...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINCINT\n      0…999999999\n      9\n    \n    \n      229\n      222\n      Viviendas particulares habitadas sin tecnologí...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINTIC\n      0…999999999\n      9\n    \n  \n\n230 rows × 6 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFíjense como pasamos skiprows=3 para leer el diccionario del censo. Esto le dice a pandas que el header (los nombres de las columnas), vienen en el cuarto renglón.\n\n\nA partir de este diccionario podemos ver que hay varias formas de codificar valores faltantes: ‘999999999’, ‘99999999’, ’*’ y ‘N/D’.\nPara poder convertir todas estas columnas en numéricas tenemos que reemplazar todos esos valores por la forma en la que se expresan los datos faltantes en Pandas, utilizando el valor Not a Number de numpy. Para hacer este reemplazo vamos a usar la función replace de Pandas, que toma como argumento el valor que queremos reemplazar y el valor por el cual lo queremos reemplazar:\n\ndb = (db  \n      .replace('999999999', np.nan)\n      .replace('99999999', np.nan)\n      .replace('*', np.nan)\n      .replace('N/D', np.nan))\n\n¡Esta fue una instrucción complicada!\nPero no es realmente difícil. Como hemos visto hasta aquí, los métodos de los DataFrames en general regresan otros DataFrames con el resultado de la operación, esto nos permite encadenar métodos, de forma que cuando hacemos db..replace('999999999', np.nan)..replace('99999999', np.nan), el segundo replace opera sobre el resultado del primero y así sucesivamente. Este encadenamiento de métodos nos ayuda a escribir código más fácil de leer.\nAhora ya tenemos todos los valores faltantes codificados adecuadamente, sin embargo aún nos falta convertirlos a números ¿verdad?\n\ndb.dtypes\n\nENTIDAD        object\nNOM_ENT        object\nMUN            object\nNOM_MUN        object\nLOC            object\n                ...  \nVPH_CVJ        object\nVPH_SINRTV     object\nVPH_SINLTC     object\nVPH_SINCINT    object\nVPH_SINTIC     object\nLength: 230, dtype: object\n\n\nLa forma normal de cambiar el tipo de datos de una columna es utilizar el método astype\n\ndb['VPH_CVJ'].astype('float').dtypes\n\ndtype('float64')\n\n\n\n\n\n\n\n\nNote\n\n\n\nAquí no estamos asignando el resultado de la operación a ninguna variable, el resultado de esta operación no modifica el valor de los datos.\n\n\nAsí podríamos ir cambiando columna por columna, pero como estamos programando ¡nos gusta hacer las cosas en bruto!\nEn el diccionario de datos tenemos los nombres de todas las variables, entonces podemos utilizar estos nombres para seleccionar todas las columnas que contienen datos numéricos y cambiar su tipo en el DataFrame. Fíjense que las primeras 8 filas del diccionario contienen los identificadores geográficos:\n\ndiccionario.head(8)\n\n\n\n\n\n  \n    \n      \n      Núm.\n      Indicador\n      Descripción\n      Mnemónico\n      Rangos\n      Longitud\n    \n  \n  \n    \n      0\n      1\n      Clave de entidad federativa\n      Código que identifica a la entidad federativa....\n      ENTIDAD\n      00…32\n      2\n    \n    \n      1\n      2\n      Entidad federativa\n      Nombre oficial de la entidad federativa.\n      NOM_ENT\n      Alfanumérico\n      50\n    \n    \n      2\n      3\n      Clave de municipio o demarcación territorial\n      Código que identifica al municipio o demarcaci...\n      MUN\n      000…570\n      3\n    \n    \n      3\n      4\n      Municipio o demarcación territorial\n      Nombre oficial del municipio o demarcación ter...\n      NOM_MUN\n      Alfanumérico\n      50\n    \n    \n      4\n      5\n      Clave de localidad\n      Código que identifica a la localidad al interi...\n      LOC\n      0000…9999\n      4\n    \n    \n      5\n      6\n      Localidad\n      Nombre con el que se reconoce a la localidad d...\n      NOM_LOC\n      Alfanumérico\n      70\n    \n    \n      6\n      7\n      Clave del AGEB\n      Clave que identifica al AGEB urbana, al interi...\n      AGEB\n      001...999; 0...9 o A-P\n      4\n    \n    \n      7\n      8\n      Clave de manzana\n      Clave que identifica a la manzana, al interior...\n      MZA\n      001...999\n      3\n    \n  \n\n\n\n\nLas demás filas contienen los nombres (y descripciones) de las variables del Censo.\n\ncampos_datos = diccionario.loc[8:,]['Mnemónico']\ncampos_datos\n\n8           POBTOT\n9           POBFEM\n10          POBMAS\n11           P_0A2\n12         P_0A2_F\n          ...     \n225        VPH_CVJ\n226     VPH_SINRTV\n227     VPH_SINLTC\n228    VPH_SINCINT\n229     VPH_SINTIC\nName: Mnemónico, Length: 222, dtype: object\n\n\nAquí utilizamos una vez más el método loc para seleccionar filas en nuestros datos. En esta ocasión seleccionamos las filas por índice (en este momento nuestro índice es simplemente el número de fila, más adelante usaremos índices diferentes), la selección loc[8:,] simplemente quiere decir todas las columnas para las filas de la 9 en adelante.\nTambién estamos seleccionando una única columna al hacer ['Mnemónico'], el resultado de esta selección ya no es un DataFrame, es una Serie. Las series son las estructuras que usa Pandas para guardar una sóla columna (o fila).\nLas Series se pueden utilizar (igual que las listas) para seleccionar columnas de un DataFrame, entoinces, ahora sí podemos cambiar todos los tipos de datos de una sola vez.\n\ndb[campos_datos] = db[campos_datos].astype('float')\ndb.dtypes\n\nENTIDAD         object\nNOM_ENT         object\nMUN             object\nNOM_MUN         object\nLOC             object\n                ...   \nVPH_CVJ        float64\nVPH_SINRTV     float64\nVPH_SINLTC     float64\nVPH_SINCINT    float64\nVPH_SINTIC     float64\nLength: 230, dtype: object"
  },
  {
    "objectID": "parte_1/01_transformacion.html#descripciones-de-los-datos",
    "href": "parte_1/01_transformacion.html#descripciones-de-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.3 Descripciones de los datos",
    "text": "1.3 Descripciones de los datos\nPandas nos provee una serie de métodos para obtener descripciones generales de la tabla. Podemos usar el método info para obtener una descripción general de la estructura de la tabla y el espacio que ocupa en la memoria:\n\ndb.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2433 entries, 3 to 68915\nColumns: 230 entries, ENTIDAD to VPH_SINTIC\ndtypes: float64(222), int64(1), object(7)\nmemory usage: 4.3+ MB\n\n\nPara obtener las estadísticas descriptivas podemos usar el método describe:\n\ndb.describe()\n\n\n\n\n\n  \n    \n      \n      MZA\n      POBTOT\n      POBFEM\n      POBMAS\n      P_0A2\n      P_0A2_F\n      P_0A2_M\n      P_3YMAS\n      P_3YMAS_F\n      P_3YMAS_M\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      count\n      2433.0\n      2433.000000\n      2422.000000\n      2423.00000\n      2406.000000\n      2392.000000\n      2390.000000\n      2423.000000\n      2422.000000\n      2423.000000\n      ...\n      2416.000000\n      2420.000000\n      2418.000000\n      2415.000000\n      2415.000000\n      2410.000000\n      2251.000000\n      2235.000000\n      2405.000000\n      1801.000000\n    \n    \n      mean\n      0.0\n      3758.993835\n      1970.647812\n      1804.64837\n      109.901912\n      54.471990\n      56.089121\n      3661.372678\n      1914.832370\n      1747.329757\n      ...\n      783.982616\n      1041.995455\n      859.506617\n      533.200000\n      395.840580\n      235.558506\n      20.068858\n      33.803579\n      229.281081\n      5.181011\n    \n    \n      std\n      0.0\n      2433.068753\n      1254.533102\n      1186.95856\n      85.636899\n      42.286817\n      43.908616\n      2347.050678\n      1215.700184\n      1147.281855\n      ...\n      525.413812\n      690.331581\n      601.110222\n      426.577764\n      390.905691\n      204.624708\n      16.611861\n      30.598161\n      191.422212\n      6.154989\n    \n    \n      min\n      0.0\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.0\n      2045.000000\n      1083.500000\n      974.00000\n      46.250000\n      23.000000\n      24.000000\n      2018.000000\n      1053.000000\n      942.500000\n      ...\n      456.750000\n      590.000000\n      488.000000\n      271.000000\n      173.000000\n      118.250000\n      8.000000\n      10.000000\n      79.000000\n      0.000000\n    \n    \n      50%\n      0.0\n      3396.000000\n      1783.500000\n      1616.00000\n      91.000000\n      45.000000\n      46.000000\n      3304.000000\n      1730.500000\n      1566.000000\n      ...\n      698.500000\n      921.500000\n      749.000000\n      442.000000\n      300.000000\n      189.000000\n      16.000000\n      25.000000\n      185.000000\n      4.000000\n    \n    \n      75%\n      0.0\n      4992.000000\n      2617.500000\n      2391.00000\n      152.000000\n      75.000000\n      77.000000\n      4852.000000\n      2539.000000\n      2315.000000\n      ...\n      992.500000\n      1348.250000\n      1083.000000\n      671.000000\n      476.000000\n      288.750000\n      27.000000\n      50.000000\n      336.000000\n      7.000000\n    \n    \n      max\n      0.0\n      21198.000000\n      11128.000000\n      10616.00000\n      709.000000\n      350.000000\n      393.000000\n      20530.000000\n      10774.000000\n      10551.000000\n      ...\n      6196.000000\n      7867.000000\n      7512.000000\n      5717.000000\n      5903.000000\n      3056.000000\n      149.000000\n      290.000000\n      1488.000000\n      66.000000\n    \n  \n\n8 rows × 223 columns"
  },
  {
    "objectID": "parte_1/01_transformacion.html#creación-de-variables",
    "href": "parte_1/01_transformacion.html#creación-de-variables",
    "title": "1  Transformación de datos",
    "section": "1.4 Creación de variables",
    "text": "1.4 Creación de variables\nMuchas veces vamos a querer crear nuevas columnas a partir de las ya existentes. Por ejemplo, podemos estar interesados en el porcentaje de población femenina en cada AGEB.\n\npct_fem = db['POBFEM'] / db['POBTOT']\npct_fem.head()\n\n3      0.532516\n30     0.521187\n82     0.527037\n116    0.535025\n163    0.512408\ndtype: float64\n\n\nFíjense cómo usamos / para dividir dos columnas. El resultado de la operación lo guardamos en la variable pct_fem ¿De qué tipo será esta variable?\n\npct_fem.info()\n\n<class 'pandas.core.series.Series'>\nInt64Index: 2433 entries, 3 to 68915\nSeries name: None\nNon-Null Count  Dtype  \n--------------  -----  \n2405 non-null   float64\ndtypes: float64(1)\nmemory usage: 38.0 KB\n\n\nEs una serie, es decir una columna en nuestro caso. Como esta columna comparte el mismo índice que los datos originales (es resultado de una operación renglón por renglón), entonces la podemos agregar al DataFrame original facilmente:\n\ndb['pct_fem'] = pct_fem\ndb['pct_fem'].head()\n\n/tmp/ipykernel_5237/2610780181.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  db['pct_fem'] = pct_fem\n\n\n3      0.532516\n30     0.521187\n82     0.527037\n116    0.535025\n163    0.512408\nName: pct_fem, dtype: float64\n\n\n\n1.4.1 Modificar valores\nDe la misma forma que podemos agregar columnas (o filas) a nuestro DataFrame, podemos también modificar los valores existentes. Para explorar esto, vamos a crear una nueva columna y llenarla con valores nulos:\n\n# Nueva columna llena de sólamente el número 1\ndb['Nueva'] = None\ndb['Nueva'].head()\n\n/tmp/ipykernel_5237/463547730.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  db['Nueva'] = None\n\n\n3      None\n30     None\n82     None\n116    None\n163    None\nName: Nueva, dtype: object\n\n\nPodemos fácilmente cambiar los valores de todas las filas:\n\ndb['Nueva'] = 1\ndb['Nueva'].head()\n\n3      1\n30     1\n82     1\n116    1\n163    1\nName: Nueva, dtype: int64\n\n\nO también cambiar el valor sólo para una fila específica:\n\ndb.loc[3, 'Nueva'] = 10\ndb['Nueva'].head()\n\n3      10\n30      1\n82      1\n116     1\n163     1\nName: Nueva, dtype: int64\n\n\n\n\n1.4.2 Eliminar columnas\nEliminar columnas es igualmente fácil usando el método drop:\n\ndb = db.drop(columns=['Nueva'])\n'Nueva' in db.columns\n\nFalse\n\n\n¡Fíjense como preguntamos al final si ya habíamos eliminado la columna!\n\n\n1.4.3 Buscando datos\nMuchas veces queremos encontrar observaciones que cumplan con uno o más criterios. Una vez más, el método loc es nuestro amigop para seleccionar datos. Supongamos que queremos encontrar aquelas AGEBs que tengan una población de ‘65 años o más’ mayor a 1,000 personas.\n\ndb_seleccion = db.loc[db['POB65_MAS'] > 1000, :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      444\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0186\n      0\n      11139.0\n      5776.0\n      ...\n      3299.0\n      2878.0\n      1731.0\n      1407.0\n      994.0\n      54.0\n      47.0\n      470.0\n      4.0\n      0.518538\n    \n    \n      3617\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0107\n      0\n      6992.0\n      3673.0\n      ...\n      2205.0\n      2022.0\n      1478.0\n      1117.0\n      650.0\n      21.0\n      26.0\n      256.0\n      4.0\n      0.525315\n    \n    \n      4075\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0287\n      0\n      8213.0\n      4526.0\n      ...\n      2373.0\n      2226.0\n      1503.0\n      1309.0\n      616.0\n      43.0\n      40.0\n      241.0\n      6.0\n      0.551078\n    \n    \n      4886\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0573\n      0\n      12827.0\n      6653.0\n      ...\n      3437.0\n      2878.0\n      1727.0\n      1409.0\n      863.0\n      59.0\n      82.0\n      669.0\n      6.0\n      0.518672\n    \n  \n\n5 rows × 231 columns\n\n\n\nSimplemente pasamos la condición que nos interesa al selector y listo.\nLos criterios de búsquera pueden ser tan sofisticados como se requiera, por ejemplo, podemos seleccionar los AGEBs en los cuales la población de 0 a 14 años sea menor a un cuarto de la población total:\n\ndb_seleccion = db.loc[(db['POB0_14'] / db['POBTOT']) < 0.25, :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183.0\n      1695.0\n      ...\n      772.0\n      692.0\n      313.0\n      221.0\n      145.0\n      8.0\n      14.0\n      148.0\n      5.0\n      0.532516\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235.0\n      2232.0\n      ...\n      1049.0\n      878.0\n      361.0\n      339.0\n      247.0\n      5.0\n      12.0\n      250.0\n      NaN\n      0.527037\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768.0\n      2551.0\n      ...\n      1237.0\n      1076.0\n      481.0\n      452.0\n      294.0\n      10.0\n      17.0\n      254.0\n      NaN\n      0.535025\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176.0\n      1115.0\n      ...\n      562.0\n      507.0\n      276.0\n      260.0\n      153.0\n      4.0\n      3.0\n      70.0\n      0.0\n      0.512408\n    \n  \n\n5 rows × 231 columns\n\n\n\nPodemos hacer combinaciones arbitrarias de selectores utilizando los operadores lógicos & (and) y | (or). Por ejemplo, podemos combinar nuestras selecciones anteriores para encontrar las AGEBs con menos de 50% de mujeres y población de 0 a 14 años sea menor a un cuarto de la población total\n\ndb_seleccion = db.loc[(db['pct_fem'] < 0.5) & \n                      ((db['POB0_14'] / db['POBTOT']) < 0.25), :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      2342\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0877\n      0\n      821.0\n      403.0\n      ...\n      174.0\n      135.0\n      56.0\n      44.0\n      34.0\n      3.0\n      23.0\n      70.0\n      NaN\n      0.490865\n    \n    \n      3292\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      1165\n      0\n      400.0\n      199.0\n      ...\n      91.0\n      51.0\n      26.0\n      26.0\n      15.0\n      3.0\n      8.0\n      49.0\n      NaN\n      0.497500\n    \n    \n      5321\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0770\n      0\n      326.0\n      160.0\n      ...\n      137.0\n      136.0\n      88.0\n      96.0\n      50.0\n      0.0\n      0.0\n      4.0\n      0.0\n      0.490798\n    \n    \n      6016\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1092\n      0\n      5787.0\n      2887.0\n      ...\n      1792.0\n      1744.0\n      1363.0\n      1206.0\n      614.0\n      11.0\n      5.0\n      55.0\n      NaN\n      0.498877\n    \n    \n      7919\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1660\n      0\n      3328.0\n      1653.0\n      ...\n      895.0\n      823.0\n      552.0\n      379.0\n      259.0\n      10.0\n      10.0\n      88.0\n      0.0\n      0.496695\n    \n  \n\n5 rows × 231 columns"
  },
  {
    "objectID": "parte_1/01_transformacion.html#ordenar-valores",
    "href": "parte_1/01_transformacion.html#ordenar-valores",
    "title": "1  Transformación de datos",
    "section": "1.5 Ordenar valores",
    "text": "1.5 Ordenar valores\nFinalmente, vamos a ver cómo ordenar los datos de acuerdo a los valores de un campo. Pensemos que queremos ver las 10 AGEBS más pobladas de la ciudad.\n\ndb.sort_values('POBTOT', ascending = False).head(10)\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      39932\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      Total AGEB urbana\n      0135\n      0\n      21198.0\n      11128.0\n      ...\n      7867.0\n      7512.0\n      5573.0\n      5568.0\n      3056.0\n      60.0\n      39.0\n      346.0\n      3.0\n      0.524955\n    \n    \n      63316\n      09\n      Ciudad de México\n      016\n      Miguel Hidalgo\n      0001\n      Total AGEB urbana\n      0444\n      0\n      18174.0\n      8931.0\n      ...\n      7294.0\n      7187.0\n      5717.0\n      5903.0\n      2640.0\n      149.0\n      9.0\n      144.0\n      NaN\n      0.491416\n    \n    \n      65102\n      09\n      Ciudad de México\n      016\n      Miguel Hidalgo\n      0001\n      Total AGEB urbana\n      1349\n      0\n      15549.0\n      8211.0\n      ...\n      4279.0\n      3756.0\n      2213.0\n      1482.0\n      944.0\n      70.0\n      208.0\n      831.0\n      21.0\n      0.528073\n    \n    \n      9394\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0020\n      Total AGEB urbana\n      0316\n      0\n      15087.0\n      7701.0\n      ...\n      3434.0\n      2289.0\n      1277.0\n      738.0\n      512.0\n      110.0\n      205.0\n      1301.0\n      23.0\n      0.510439\n    \n    \n      9090\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0001\n      Total AGEB urbana\n      0369\n      0\n      14609.0\n      7459.0\n      ...\n      5989.0\n      5970.0\n      5155.0\n      4826.0\n      2486.0\n      23.0\n      9.0\n      19.0\n      0.0\n      0.510576\n    \n    \n      9190\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0001\n      Total AGEB urbana\n      0373\n      0\n      14170.0\n      7457.0\n      ...\n      3293.0\n      2650.0\n      1743.0\n      1430.0\n      889.0\n      59.0\n      130.0\n      782.0\n      13.0\n      0.526253\n    \n    \n      6211\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1162\n      0\n      14061.0\n      7267.0\n      ...\n      3416.0\n      2780.0\n      1316.0\n      971.0\n      689.0\n      67.0\n      120.0\n      837.0\n      23.0\n      0.516820\n    \n    \n      52537\n      09\n      Ciudad de México\n      012\n      Tlalpan\n      0001\n      Total AGEB urbana\n      2121\n      0\n      13974.0\n      7345.0\n      ...\n      3954.0\n      3518.0\n      2477.0\n      2100.0\n      1259.0\n      34.0\n      61.0\n      476.0\n      8.0\n      0.525619\n    \n    \n      26177\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      Total AGEB urbana\n      1994\n      0\n      13946.0\n      3330.0\n      ...\n      1069.0\n      767.0\n      327.0\n      231.0\n      180.0\n      18.0\n      25.0\n      329.0\n      4.0\n      0.238778\n    \n    \n      42074\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      Total AGEB urbana\n      1171\n      0\n      13918.0\n      7438.0\n      ...\n      4387.0\n      3856.0\n      2895.0\n      2351.0\n      1244.0\n      51.0\n      96.0\n      655.0\n      12.0\n      0.534416\n    \n  \n\n10 rows × 231 columns\n\n\n\nEl método sort_values nos permite ordenar los datos de acuerdo al valor (o criterio) que queramos. El argumento ascending = False indica que los queremos ordenar de forma descendente."
  },
  {
    "objectID": "parte_1/01_transformacion.html#exploración-visual",
    "href": "parte_1/01_transformacion.html#exploración-visual",
    "title": "1  Transformación de datos",
    "section": "1.6 Exploración Visual",
    "text": "1.6 Exploración Visual\nYa que nos empezamos a familiarizar con el manejo de datos usando Pandas, podemos empezar a hacer cosas más divertidas, por ejemplo, explorar visualmente los datos.\nLa librería seaborn nos ofrece una serie de herramientas para la exploración visual de los datos. Podemos comenzar con un histograma para ver la distribución de los valores de una columna.\n\n_ = sns.histplot(db['POBTOT'], kde = False)\n\n\n\n\nLa función histplot de seaborn nos regresa el histograma, el argumento kde=False le dice que no queremos que ajuste una distribución empírica.\n\n\n\n\n\n\nNote\n\n\n\nCuando hicimos _ = sns.histplot(db['POBTOT'], kde = False) estamos asignando el resultado a la variable _, esto se hace comunmente cuando no queremos ya hacer nada más con ese resultado. Más adelante haremos operaciones sobre las gráficas.\n\n\n\n1.6.0.1 Densidad de Kernel\nOtra forma de representar la distribución de una variable es ajustando una densidad de kernel, que estima una distribución (empírica) de probabilidad a partir de nuestras observaciones.\n\n_ = sns.kdeplot(db['POBTOT'], fill = True)\n\n\n\n\nOtra visualización muy útil es la de la distribución conjunta de dos variables. Por ejemplo, supongamos que queremos comparar las distribuciones de la población masculina y femenina.\n\n_ = sns.jointplot(data=db, x='POBFEM', y='POBMAS')\n\n\n\n\nLa relación, como es de esperarse, es casi perfectamente lineal, pero ver las distribuciones conjuntas nos permite identificar algunas AGEBS con poblaciones masculinas desproporcionadamente grandes ¿Qué serán?.\nMuchas veces queremos visualizar la distribución conjunta de varias variables al mismo tiempo. Por ejemplo cuando queremos hacer ejercicios de regresión queremos explorar la correlación entre las covariables. Una forma de visualizar rápidamente estas distribuciones conjuntas es con un PairGrid. Utlicemos uno sencillo para ver las distribuciones de algunas variables.\n\nvars = ['P_0A2', 'P_15A17', 'PNACOE', 'P3YM_HLI']\ng = sns.PairGrid(db[vars])\ng = g.map(sns.scatterplot)\n\n\n\n\nLa función PairPlot sólo nos prepara la malla (un cuadrado del número de variables de los datos) y con el map llenamos esa malla con la gráfica que queramos, en nuestro caso un diagrama de dispersión.\nEn este caso la diagonal no es muy informativa, es un diagrama de dispersión de una variable consigo misma. PairPlot es muy flexible y nos permite mapear diferentes funciones para la diagonal y los demás elementos, por ejemplo:\n\ng = sns.PairGrid(db[vars])\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n\n<seaborn.axisgrid.PairGrid at 0x7f70a19c4040>"
  },
  {
    "objectID": "parte_1/01_transformacion.html#organizando-los-datos",
    "href": "parte_1/01_transformacion.html#organizando-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.7 Organizando los datos",
    "text": "1.7 Organizando los datos\nMuchos flujos de análisis requieren organizar los datos en una estructura particular conocida como Tidy Data (algo así como datos ordenados). La idea es tener una estructura estandarizada con principios comunes de manipulación que sirva como entrada a diferentes tipos de análisis.\nLas tres características fundamentales de un conjunto de datos bien ordenado de acuerdo a los principios tidy son:\n\nCada variable en una columna\nCada observación en una fila\nCada unidad de observación en una tabla\n\nPara mayor información sobre el concepto de Tidy Data, puede consultarse el Artículo Académico original (de Acceso Libre), así como el Repositorio Púlico asociado a él.\nTratemos de aplicar el concepto de Tidy Data a los datos de la práctica. Primero, recordando su estructura:\n\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183.0\n      1695.0\n      ...\n      772.0\n      692.0\n      313.0\n      221.0\n      145.0\n      8.0\n      14.0\n      148.0\n      5.0\n      0.532516\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235.0\n      2232.0\n      ...\n      1049.0\n      878.0\n      361.0\n      339.0\n      247.0\n      5.0\n      12.0\n      250.0\n      NaN\n      0.527037\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768.0\n      2551.0\n      ...\n      1237.0\n      1076.0\n      481.0\n      452.0\n      294.0\n      10.0\n      17.0\n      254.0\n      NaN\n      0.535025\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176.0\n      1115.0\n      ...\n      562.0\n      507.0\n      276.0\n      260.0\n      153.0\n      4.0\n      3.0\n      70.0\n      0.0\n      0.512408\n    \n  \n\n5 rows × 231 columns\n\n\n\nEsta base de datos no cumple con las características tidy. En efecto, tenemos las variables en columnas (sin contar los identificadores), pero:\n\nTenemos dos tipos de unidades: personas y viviendas. El principio tidy nos indica que necesitamos dos tablas para representar los datos.\nPara cada tipoi de unidad tenemos en la misma fila tantas observaciones como variables (del mismo tipo). Por ejemplo, el valor de la población para cada grupo de edad en cada AGEB es una observación.\n\nEntonces, vamos a trabajar en acomodar la tabla a los principios tidy. Para comenzar, trabajemos sólo con las variables que representan segmentos de edad de la población. Seleccionar sólo estas columnas puede ser engorroso, pero si nos fijamos en el diccionario, podemos observar que todas las variables que nos interesan empiezan con ‘P_’ Podemos usar esta observación para seleccionar, a partir de la lista de columnas, sólo las que nos interesan:\n\ncols_pob = [c for c in db.columns if c.startswith('P_')]\nprint(cols_pob)               \n\n['P_0A2', 'P_0A2_F', 'P_0A2_M', 'P_3YMAS', 'P_3YMAS_F', 'P_3YMAS_M', 'P_5YMAS', 'P_5YMAS_F', 'P_5YMAS_M', 'P_12YMAS', 'P_12YMAS_F', 'P_12YMAS_M', 'P_15YMAS', 'P_15YMAS_F', 'P_15YMAS_M', 'P_18YMAS', 'P_18YMAS_F', 'P_18YMAS_M', 'P_3A5', 'P_3A5_F', 'P_3A5_M', 'P_6A11', 'P_6A11_F', 'P_6A11_M', 'P_8A14', 'P_8A14_F', 'P_8A14_M', 'P_12A14', 'P_12A14_F', 'P_12A14_M', 'P_15A17', 'P_15A17_F', 'P_15A17_M', 'P_18A24', 'P_18A24_F', 'P_18A24_M', 'P_15A49_F', 'P_60YMAS', 'P_60YMAS_F', 'P_60YMAS_M']\n\n\nAhora, vamos a construir un identificador único de AGEB para cada fila concatenando los identificadores de entidad, municipio, localidad y ageb:\n\ndb['AGEB_cvgeo'] = db['ENTIDAD'] + db['MUN'] + db['LOC'] + db['AGEB']\ndb['AGEB_cvgeo'].head()\n\n3      0900200010010\n30     0900200010025\n82     090020001003A\n116    0900200010044\n163    0900200010097\nName: AGEB_cvgeo, dtype: object\n\n\nYa con este identificador, podemos eliminar de la tabla los identificadores que usamos para construirlo\n\ndb = db.drop(columns=['ENTIDAD', 'MUN', 'LOC', 'AGEB'])\n\nCopiamos las columnas que nos interesan a una nueva tabla\n\nrangos = db[['AGEB_cvgeo'] + cols_pob]\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      P_0A2\n      P_0A2_F\n      P_0A2_M\n      P_3YMAS\n      P_3YMAS_F\n      P_3YMAS_M\n      P_5YMAS\n      P_5YMAS_F\n      P_5YMAS_M\n      ...\n      P_15A17\n      P_15A17_F\n      P_15A17_M\n      P_18A24\n      P_18A24_F\n      P_18A24_M\n      P_15A49_F\n      P_60YMAS\n      P_60YMAS_F\n      P_60YMAS_M\n    \n  \n  \n    \n      3\n      0900200010010\n      60.0\n      32.0\n      28.0\n      3123.0\n      1663.0\n      1460.0\n      3074.0\n      1639.0\n      1435.0\n      ...\n      111.0\n      61.0\n      50.0\n      303.0\n      149.0\n      154.0\n      726.0\n      816.0\n      470.0\n      346.0\n    \n    \n      30\n      0900200010025\n      122.0\n      58.0\n      64.0\n      5470.0\n      2856.0\n      2614.0\n      5363.0\n      2805.0\n      2558.0\n      ...\n      214.0\n      97.0\n      117.0\n      521.0\n      263.0\n      258.0\n      1436.0\n      1293.0\n      732.0\n      561.0\n    \n    \n      82\n      090020001003A\n      88.0\n      49.0\n      39.0\n      4147.0\n      2183.0\n      1964.0\n      4065.0\n      2138.0\n      1927.0\n      ...\n      180.0\n      74.0\n      106.0\n      425.0\n      226.0\n      199.0\n      1067.0\n      931.0\n      546.0\n      385.0\n    \n    \n      116\n      0900200010044\n      110.0\n      49.0\n      61.0\n      4658.0\n      2502.0\n      2156.0\n      4560.0\n      2445.0\n      2115.0\n      ...\n      175.0\n      87.0\n      88.0\n      487.0\n      241.0\n      246.0\n      1215.0\n      1132.0\n      672.0\n      460.0\n    \n    \n      163\n      0900200010097\n      40.0\n      16.0\n      24.0\n      2136.0\n      1099.0\n      1037.0\n      2100.0\n      1076.0\n      1024.0\n      ...\n      90.0\n      45.0\n      45.0\n      204.0\n      96.0\n      108.0\n      508.0\n      562.0\n      311.0\n      251.0\n    \n  \n\n5 rows × 41 columns\n\n\n\nAhora vamos a reorganizar la tabla de forma que cada grupo de edad corresponda a una fila en lugar de una columna, de esta forma tenemos las observaciones en filas, de acuerdo al principio tidy.\nPara lograr esto lo que tenemos que hacer es la operación inversa de un pivote, es decir, un stack. El método stack hace justo lo que necesitamos, sólo tenemos que especificar el índice (lo que distingue a cada observación) que queremos utilizar para cada fila, en este caso AGEB_cvgeo.\n\nrangos = rangos.set_index('AGEB_cvgeo').stack()\nrangos\n\nAGEB_cvgeo               \n0900200010010  P_0A2           60.0\n               P_0A2_F         32.0\n               P_0A2_M         28.0\n               P_3YMAS       3123.0\n               P_3YMAS_F     1663.0\n                              ...  \n0901700011524  P_18A24_M      230.0\n               P_15A49_F     1111.0\n               P_60YMAS       706.0\n               P_60YMAS_F     394.0\n               P_60YMAS_M     312.0\nLength: 96555, dtype: float64\n\n\nPerfecto, eso se parece bastante a lo que buscamos, sólo que en lugar de un DataFrame lo que tenemos es una Serie. Fíjense que para cada valor del índice (AGEB_cvgeo), tenemos todos los valores de los grupos de población.\nPara convertir esto en un DataFrame lo más sencillo es quitar el índice que creamos con la función reset_index:\n\nrangos = rangos.reset_index()\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      level_1\n      0\n    \n  \n  \n    \n      0\n      0900200010010\n      P_0A2\n      60.0\n    \n    \n      1\n      0900200010010\n      P_0A2_F\n      32.0\n    \n    \n      2\n      0900200010010\n      P_0A2_M\n      28.0\n    \n    \n      3\n      0900200010010\n      P_3YMAS\n      3123.0\n    \n    \n      4\n      0900200010010\n      P_3YMAS_F\n      1663.0\n    \n  \n\n\n\n\nAhora tenemos un DataFrame en el que el valor de la columna AGEB_cvgeo viene repetido para cada observación. Ya sólo necesitamos renombrar las columnas restantes para que nos indiquen más claramente su contenido:\n\nrangos = rangos.rename(columns = {'level_1':'Grupo', 0:'Población'})\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      Grupo\n      Población\n    \n  \n  \n    \n      0\n      0900200010010\n      P_0A2\n      60.0\n    \n    \n      1\n      0900200010010\n      P_0A2_F\n      32.0\n    \n    \n      2\n      0900200010010\n      P_0A2_M\n      28.0\n    \n    \n      3\n      0900200010010\n      P_3YMAS\n      3123.0\n    \n    \n      4\n      0900200010010\n      P_3YMAS_F\n      1663.0\n    \n  \n\n\n\n\n!Ahora tenemos nuestra tabla acomodada a los principios tidy!"
  },
  {
    "objectID": "parte_1/01_transformacion.html#agrupamiento-transformación-y-agregación",
    "href": "parte_1/01_transformacion.html#agrupamiento-transformación-y-agregación",
    "title": "1  Transformación de datos",
    "section": "1.8 Agrupamiento, Transformación y Agregación",
    "text": "1.8 Agrupamiento, Transformación y Agregación\nUna ventaja de tener los datos estructurados de acuerdo a los principios tidy es la facilidad con la que podemos realizar procesos de transformación más sofisticados como agrupaciones y sumarios. Las agrupaciones consisten en agrupar observaciones en una tabla de acuerdo a sus valores (o expresiones) en una columna, a los datos agrupados se le pueden aplicar operaciones de agregación más o menos arbitrarias.\nDigamos, por ejemplo, que queremos obtener los totales de población para cada grupo etario a través de todas las AGEBs. Para hacer esto tenemos que agrupar las observaciones por cada Grupo y después obtener el valor agregado por la suma. Vamos por partes.\n\ngrupos = rangos.groupby('Grupo')\ngrupos\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f70945509d0>\n\n\nLa función groupby nos permite agrupar los datos de acuerdo a una (o más) columnas. El resultado, como pueden ver, no es un DataFrame sino un objeto de la clase especial pandas.core.groupby.generic.DataFrameGroupBy. Esta clase sirve para representar DataFrames agregados, estos objetos nos permiten obtener de forma fácil los valores que corresponden a diferentes funciones de agregación. Por ejemplo, para obtener el total de posblación por cada grupo, podemos agregar nuestro objeto con la función sum:\n\ngrupos.sum(numeric_only=True)\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      Grupo\n      \n    \n  \n  \n    \n      P_0A2\n      264424.0\n    \n    \n      P_0A2_F\n      130297.0\n    \n    \n      P_0A2_M\n      134053.0\n    \n    \n      P_12A14\n      364225.0\n    \n    \n      P_12A14_F\n      179955.0\n    \n    \n      P_12A14_M\n      184240.0\n    \n    \n      P_12YMAS\n      7864313.0\n    \n    \n      P_12YMAS_F\n      4141887.0\n    \n    \n      P_12YMAS_M\n      3722424.0\n    \n    \n      P_15A17\n      377178.0\n    \n    \n      P_15A17_F\n      185144.0\n    \n    \n      P_15A17_M\n      191984.0\n    \n    \n      P_15A49_F\n      2490275.0\n    \n    \n      P_15YMAS\n      7500071.0\n    \n    \n      P_15YMAS_F\n      3961914.0\n    \n    \n      P_15YMAS_M\n      3538155.0\n    \n    \n      P_18A24\n      975897.0\n    \n    \n      P_18A24_F\n      483893.0\n    \n    \n      P_18A24_M\n      491985.0\n    \n    \n      P_18YMAS\n      7122878.0\n    \n    \n      P_18YMAS_F\n      3776738.0\n    \n    \n      P_18YMAS_M\n      3346138.0\n    \n    \n      P_3A5\n      321650.0\n    \n    \n      P_3A5_F\n      158674.0\n    \n    \n      P_3A5_M\n      162933.0\n    \n    \n      P_3YMAS\n      8871506.0\n    \n    \n      P_3YMAS_F\n      4637724.0\n    \n    \n      P_3YMAS_M\n      4233780.0\n    \n    \n      P_5YMAS\n      8660874.0\n    \n    \n      P_5YMAS_F\n      4533469.0\n    \n    \n      P_5YMAS_M\n      4127403.0\n    \n    \n      P_60YMAS\n      1487004.0\n    \n    \n      P_60YMAS_F\n      850901.0\n    \n    \n      P_60YMAS_M\n      636074.0\n    \n    \n      P_6A11\n      685511.0\n    \n    \n      P_6A11_F\n      337113.0\n    \n    \n      P_6A11_M\n      348375.0\n    \n    \n      P_8A14\n      829786.0\n    \n    \n      P_8A14_F\n      408494.0\n    \n    \n      P_8A14_M\n      421280.0\n    \n  \n\n\n\n\nComo ve, al usar un agregador sobre el objeto agrupado obtenemos un DataFrame con los valores que corresponden a la agregación que utilizamos.\n\n\n\n\n\n\nNote\n\n\n\nEl parámetro numeric_only=True le dice al agregador que sólo calcule el resultado para las columnas de tipo numérico.\n\n\nEn este caso la función que usamos para agregar los datos es la suma, sin embargo es posible utilizar cualquier función que opere sobre grupos de observaciones, por ejemplo, el promedio:\n\ngrupos.mean(numeric_only=True)\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      Grupo\n      \n    \n  \n  \n    \n      P_0A2\n      109.901912\n    \n    \n      P_0A2_F\n      54.471990\n    \n    \n      P_0A2_M\n      56.089121\n    \n    \n      P_12A14\n      151.130705\n    \n    \n      P_12A14_F\n      74.701121\n    \n    \n      P_12A14_M\n      76.702748\n    \n    \n      P_12YMAS\n      3245.692530\n    \n    \n      P_12YMAS_F\n      1710.110239\n    \n    \n      P_12YMAS_M\n      1536.287247\n    \n    \n      P_15A17\n      156.310816\n    \n    \n      P_15A17_F\n      77.014975\n    \n    \n      P_15A17_M\n      79.993333\n    \n    \n      P_15A49_F\n      1028.614209\n    \n    \n      P_15YMAS\n      3095.365662\n    \n    \n      P_15YMAS_F\n      1635.802642\n    \n    \n      P_15YMAS_M\n      1460.237309\n    \n    \n      P_18A24\n      403.596774\n    \n    \n      P_18A24_F\n      200.038446\n    \n    \n      P_18A24_M\n      204.143154\n    \n    \n      P_18YMAS\n      2939.693768\n    \n    \n      P_18YMAS_F\n      1559.346821\n    \n    \n      P_18YMAS_M\n      1380.989682\n    \n    \n      P_3A5\n      133.298798\n    \n    \n      P_3A5_F\n      65.949293\n    \n    \n      P_3A5_M\n      67.775790\n    \n    \n      P_3YMAS\n      3661.372678\n    \n    \n      P_3YMAS_F\n      1914.832370\n    \n    \n      P_3YMAS_M\n      1747.329757\n    \n    \n      P_5YMAS\n      3574.442427\n    \n    \n      P_5YMAS_F\n      1871.787366\n    \n    \n      P_5YMAS_M\n      1703.426744\n    \n    \n      P_60YMAS\n      615.481788\n    \n    \n      P_60YMAS_F\n      353.511010\n    \n    \n      P_60YMAS_M\n      264.040681\n    \n    \n      P_6A11\n      284.326421\n    \n    \n      P_6A11_F\n      140.055256\n    \n    \n      P_6A11_M\n      144.734109\n    \n    \n      P_8A14\n      343.739022\n    \n    \n      P_8A14_F\n      169.218724\n    \n    \n      P_8A14_M\n      174.587650\n    \n  \n\n\n\n\nLas funciones que usamos para agregar (sum y mean) son funciones de numpyy podemos utiliizar cualquier función de agregación. También es posible calcular diferentes agregaciones al mismo tiempo:\n\ngrupos.aggregate([np.sum, np.mean, np.std])\n\n/tmp/ipykernel_5237/732611272.py:1: FutureWarning: ['AGEB_cvgeo'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n  grupos.aggregate([np.sum, np.mean, np.std])\n\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      \n      sum\n      mean\n      std\n    \n    \n      Grupo\n      \n      \n      \n    \n  \n  \n    \n      P_0A2\n      264424.0\n      109.901912\n      85.636899\n    \n    \n      P_0A2_F\n      130297.0\n      54.471990\n      42.286817\n    \n    \n      P_0A2_M\n      134053.0\n      56.089121\n      43.908616\n    \n    \n      P_12A14\n      364225.0\n      151.130705\n      111.565262\n    \n    \n      P_12A14_F\n      179955.0\n      74.701121\n      55.572013\n    \n    \n      P_12A14_M\n      184240.0\n      76.702748\n      56.746606\n    \n    \n      P_12YMAS\n      7864313.0\n      3245.692530\n      2056.644056\n    \n    \n      P_12YMAS_F\n      4141887.0\n      1710.110239\n      1073.566831\n    \n    \n      P_12YMAS_M\n      3722424.0\n      1536.287247\n      1001.153466\n    \n    \n      P_15A17\n      377178.0\n      156.310816\n      113.532155\n    \n    \n      P_15A17_F\n      185144.0\n      77.014975\n      56.107357\n    \n    \n      P_15A17_M\n      191984.0\n      79.993333\n      57.996607\n    \n    \n      P_15A49_F\n      2490275.0\n      1028.614209\n      692.206450\n    \n    \n      P_15YMAS\n      7500071.0\n      3095.365662\n      1955.668987\n    \n    \n      P_15YMAS_F\n      3961914.0\n      1635.802642\n      1023.764553\n    \n    \n      P_15YMAS_M\n      3538155.0\n      1460.237309\n      950.879515\n    \n    \n      P_18A24\n      975897.0\n      403.596774\n      279.378732\n    \n    \n      P_18A24_F\n      483893.0\n      200.038446\n      138.590941\n    \n    \n      P_18A24_M\n      491985.0\n      204.143154\n      143.125222\n    \n    \n      P_18YMAS\n      7122878.0\n      2939.693768\n      1853.201763\n    \n    \n      P_18YMAS_F\n      3776738.0\n      1559.346821\n      973.233847\n    \n    \n      P_18YMAS_M\n      3346138.0\n      1380.989682\n      899.958704\n    \n    \n      P_3A5\n      321650.0\n      133.298798\n      101.904268\n    \n    \n      P_3A5_F\n      158674.0\n      65.949293\n      50.305709\n    \n    \n      P_3A5_M\n      162933.0\n      67.775790\n      52.251282\n    \n    \n      P_3YMAS\n      8871506.0\n      3661.372678\n      2347.050678\n    \n    \n      P_3YMAS_F\n      4637724.0\n      1914.832370\n      1215.700184\n    \n    \n      P_3YMAS_M\n      4233780.0\n      1747.329757\n      1147.281855\n    \n    \n      P_5YMAS\n      8660874.0\n      3574.442427\n      2284.544513\n    \n    \n      P_5YMAS_F\n      4533469.0\n      1871.787366\n      1185.089392\n    \n    \n      P_5YMAS_M\n      4127403.0\n      1703.426744\n      1115.802146\n    \n    \n      P_60YMAS\n      1487004.0\n      615.481788\n      358.110680\n    \n    \n      P_60YMAS_F\n      850901.0\n      353.511010\n      206.712937\n    \n    \n      P_60YMAS_M\n      636074.0\n      264.040681\n      152.406790\n    \n    \n      P_6A11\n      685511.0\n      284.326421\n      213.690386\n    \n    \n      P_6A11_F\n      337113.0\n      140.055256\n      105.214351\n    \n    \n      P_6A11_M\n      348375.0\n      144.734109\n      109.164209\n    \n    \n      P_8A14\n      829786.0\n      343.739022\n      255.780534\n    \n    \n      P_8A14_F\n      408494.0\n      169.218724\n      126.379228\n    \n    \n      P_8A14_M\n      421280.0\n      174.587650\n      130.222008"
  },
  {
    "objectID": "parte_1/01_transformacion.html#para-practicar",
    "href": "parte_1/01_transformacion.html#para-practicar",
    "title": "1  Transformación de datos",
    "section": "1.9 Para Practicar",
    "text": "1.9 Para Practicar\nLa organización Wikileaks posee una Base de Datos pública en la cual se contiene, entre otras cosas, el número de casualidades existentes durante los primeros años de la Guerra de Afganistán, la cual puede ser consultada a través de la siguiente liga:\n\nhttps://docs.google.com/spreadsheets/d/1EAx8_ksSCmoWW_SlhFyq2QrRn0FNNhcg1TtDFJzZRgc/edit?hl=en#gid=1\n\n\n\n\nWikileaks\n\n\nA partir de los datos, realiza los siguientes ejercidios: * Descarga la tabla como un archivo de tipo .csv (Archivo –> Descargar como –> .csv, hoja actual). * Importa los datos a un DataFrame de Pandas. * Explora los datos generando estadísticas descriptivas y algunas gráficas. * Examina qué tanto se ajusta a los principios del Tidy Data y ajústalo según creas conveniente * Obten una cuenta total de las bajas por mes y genera una gráfica con dicho conteo."
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#exploración-del-contenido",
    "href": "parte_1/02_ejemplo_transformacion.html#exploración-del-contenido",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.1 Exploración del contenido",
    "text": "2.1 Exploración del contenido\nLo primero que vamos a hacer es explorar los datos publicados por la Secretaría de Salud para entender cómo están organizados. En la carpeta de datos del libro puedes encontrar un ejemplo de la base de datos para el 9 de enero de 2023 bajo el nombre datos_abiertos_covid19.zip.\nPara leer los datos vamos a utilizar la función read_csv(), esta función (como pueden ver) acepta que el csv venga comprimido en un zip.\n\ndf = pd.read_csv('datos/datos_abiertos_covid19.zip', dtype=object, encoding='latin-1')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      0\n      2023-01-03\n      01e27d\n      2\n      9\n      25\n      2\n      25\n      25\n      001\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      2\n      2023-01-03\n      06fce8\n      1\n      12\n      07\n      1\n      07\n      07\n      059\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      3\n      2023-01-03\n      1a4a8d\n      1\n      12\n      23\n      2\n      27\n      23\n      008\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nCada renglón en la base de datos corresponde a un caso en seguimiento, el resultado de cada caso se puede actualizar en sucesivas publicaciones de la base de datos. Las columnas describen un conjunto de variables asociadas al seguimiento de cada uno de los casos. Las dos primeras columnas corresponden a la fecha en la que se actualizó el caso y a un id único para cada caso respectivamente, en este taller no vamos a usar esas dos columnas.\nLuego vienen un conjunto de columnas que describen la unidad médica de reporte y, después, las columnas que nos interesan más, que son las que describen al paciente.\nPara entender un poco mejor los datos, conviene leer el archívo de catálogo. Lo pueden descargar del sitio de datos abiertos o bien usar el que viene en la carpeta de datos del libro bajo el nombre 201128 Catalogos.xlsx. Como el catálogo es un archivo de excel con varias hojas, lo vamos a leer usando openpyxl que nos va a devolver un diccionario de DataFrames que relacionan el nombre de la hoja con los datos que contiene.\n\ncatalogos = 'datos/201128 Catalogos.xlsx'\nnombres_catalogos = ['Catálogo de ENTIDADES', # Acá están los nombres de las hojas del excel\n                      'Catálogo MUNICIPIOS',\n                      'Catálogo SI_NO',\n                      'Catálogo TIPO_PACIENTE',\n                      'Catálogo CLASIFICACION_FINAL',\n                      'Catálogo RESULTADO_LAB'\n                     ]\n# read_excel nos regresa un diccionario que relaciona el nombre de cada hoja con \n# el contenido de la hoja como DataFrame\ndict_catalogos = pd.read_excel(catalogos,\n                          nombres_catalogos,\n                          dtype=str,\n                          engine='openpyxl')\nclasificacion_final = dict_catalogos['Catálogo CLASIFICACION_FINAL']\n# Aquí le damos nombre a las columnas porque en el excel se saltan dos líneas\nclasificacion_final.columns = [\"CLAVE\", \"CLASIFICACIÓN\", \"DESCRIPCIÓN\"] \nclasificacion_final\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n    \n      2\n      1\n      CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍ...\n      Confirmado por asociación aplica cuando el cas...\n    \n    \n      3\n      2\n      CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n      Confirmado por dictaminación solo aplica para ...\n    \n    \n      4\n      3\n      CASO DE SARS-COV-2  CONFIRMADO\n      Confirmado aplica cuando:\\nEl caso tiene muest...\n    \n    \n      5\n      4\n      INVÁLIDO POR LABORATORIO\n      Inválido aplica cuando el caso no tienen asoci...\n    \n    \n      6\n      5\n      NO REALIZADO POR LABORATORIO\n      No realizado aplica cuando el caso no tienen a...\n    \n    \n      7\n      6\n      CASO SOSPECHOSO\n      Sospechoso aplica cuando: \\nEl caso no tienen ...\n    \n    \n      8\n      7\n      NEGATIVO A SARS-COV-2\n      Negativo aplica cuando el caso:\\n1. Se le tomo...\n    \n  \n\n\n\n\nLo que estamos viendo aquí es el catálogo de datos de la columna CLASIFICACION_FINAL. Este catálogo relaciona el valor de la CLAVE con su significado. En particular, la columna CLASIFICACION_FINAL es la que nos permite identificar los casos positivos como veremos más adelante.\nEl resto de los catálogos funciona de la misma forma, en este momento sólo vamos a utilizar la clasificación de los pacientes, pero más adelante podemos utilzar algunas de las columnas restantes."
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#aplanado-de-datos",
    "href": "parte_1/02_ejemplo_transformacion.html#aplanado-de-datos",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.2 Aplanado de datos",
    "text": "2.2 Aplanado de datos\nComo acabamos de ver, de alguna forma la información viene distribuida en tres archivos, uno con los datos, otro con las categorías que usa y un tercero con sus descripciones. Para utilizar los datos más fácilmente, sobre todo para poder hablarle a las cosas por su nombre en lugar de referirnos a sus valores codificados, vamos a realizar un conjunto de operaciones para aplanar los datos.\nEn el bajo mundo del análisis de datos, aplanar una base de datos es la operación de substituir los valores codificados a partir de un diccionario. En este caso, los datos que leímos traen valores codificados, entonces la primera misión es substituir esos valores por sus equivalentes en el diccionario.\nComo la base de datos es muy grande, vamos a trabajar sólo con un estado de la república, en este caso la Ciudad de México (pero ustedes podrían elegir otro cualquiera).\nPara seleccionar un estado, tenemos que elegir las filas del DataFrame que contengan el valor que queremos en la columna ENTIDAD, para eso vamos a aprender a usar nuestro primer operador de Pandas, el operador loc que nos permite seleccionar filas a partir de los valores de una o más columnas.\n\n# el copy() nos asegura tener una copia de los datos en lugar de una referencia, \n# con eso podemos liberar la memoria más fácil\ndf = df.loc[df['ENTIDAD_RES'] == '09'].copy()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      09\n      2\n      09\n      09\n      016\n      2\n      ...\n      2\n      1\n      4\n      2\n      97\n      2\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      09\n      1\n      09\n      09\n      012\n      1\n      ...\n      99\n      2\n      97\n      1\n      1\n      3\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      09\n      1\n      18\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nFíjense que lo que hicimos fue reescribir en la variable df el resultado de nuestra selección, de forma que df ahora sólo contiene resultados para la CDMX.\nAhora ya con los datos filtrados y, por lo tanto, con un tamaño más manejable, vamos a empezar a trabajarlos. Lo primero que vamos a hacer es cambiar los valores de la columna MUNICIPIO_RES por la concatenación de las claves de estado y municipio, esto porque nos hará más adelante más fácil el trabajo de unir los datos con las geometrías de los municipios y porque además así tendremos un identificador único para estos (claro que esto sólo tiene sentido al trabajar con varios estados al mismo tiempo).\n\ndf['MUNICIPIO_RES'] = df['ENTIDAD_RES'] + df['MUNICIPIO_RES']\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      09012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      09007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      09\n      2\n      09\n      09\n      09016\n      2\n      ...\n      2\n      1\n      4\n      2\n      97\n      2\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      09\n      1\n      09\n      09\n      09012\n      1\n      ...\n      99\n      2\n      97\n      1\n      1\n      3\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      09\n      1\n      18\n      09\n      09007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nAhora vamos a corregir el nombre de una columna en la base de datos para que coincida con el nombre en el diccionario y después podamos buscar automáticamente. Pra corregir el nombre de la columna vamos a utilizar la función rename de Pandas. Esta función nos sirve para renombrar filas (el índice del DataFrame, que vamos a ver más adelante) o columnas dependiendo de qué eje seleccionemos. El eje 0 son las filas y el 1 las columnas.\n\n# Como estamos usando explícitamente el parámetro columns, \n# no necesitamos especificar el eje\ndf = df.rename(columns={'OTRA_COM': 'OTRAS_COM'})\ndf.columns\n\nIndex(['FECHA_ACTUALIZACION', 'ID_REGISTRO', 'ORIGEN', 'SECTOR', 'ENTIDAD_UM',\n       'SEXO', 'ENTIDAD_NAC', 'ENTIDAD_RES', 'MUNICIPIO_RES', 'TIPO_PACIENTE',\n       'FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF', 'INTUBADO', 'NEUMONIA',\n       'EDAD', 'NACIONALIDAD', 'EMBARAZO', 'HABLA_LENGUA_INDIG', 'INDIGENA',\n       'DIABETES', 'EPOC', 'ASMA', 'INMUSUPR', 'HIPERTENSION', 'OTRAS_COM',\n       'CARDIOVASCULAR', 'OBESIDAD', 'RENAL_CRONICA', 'TABAQUISMO',\n       'OTRO_CASO', 'TOMA_MUESTRA_LAB', 'RESULTADO_LAB',\n       'TOMA_MUESTRA_ANTIGENO', 'RESULTADO_ANTIGENO', 'CLASIFICACION_FINAL',\n       'MIGRANTE', 'PAIS_NACIONALIDAD', 'PAIS_ORIGEN', 'UCI'],\n      dtype='object')\n\n\nFíjense cómo otra vez reescribimos la variable df. La mayor parte de las operaciones en Pandas regresan un DataFrame con el resultado de la operación y no modifican el DataFrame original, entonces para guardar los resultados, necesitamos reescribir la variable (o guardarla con otro nombre)\nAhora sí podemos empezar a aplanar los datos. Vamos a empezar por resolver las claves de resultado de las pruebas COVID. En los datos originales estos vienen codificados en la columna RESULTADO_LAB, pero en el diccionario ese velor se llama RESULTADO, entonces otra vez vamos a empezar por renombrar una columna.\n\ndf = df.rename(columns={'RESULTADO_LAB': 'RESULTADO'})\n\nPara sustituir los valores en nuestros datos originales vamos a usar la función map que toma una serie (una serie es una columna de un dataframe) y mapea sus valores de acuerdo a una correspondencia que podemos pasar como un diccionario. Veamos poco a poco cómo hacer lo que queremos.\nLo primero que necesitamos es un diccionario que relacione los valores en nuestros datos con los nombres en el diccionario. Recordemos cómo se ve el diccionario:\n\nclasificacion_final\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n    \n      2\n      1\n      CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍ...\n      Confirmado por asociación aplica cuando el cas...\n    \n    \n      3\n      2\n      CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n      Confirmado por dictaminación solo aplica para ...\n    \n    \n      4\n      3\n      CASO DE SARS-COV-2  CONFIRMADO\n      Confirmado aplica cuando:\\nEl caso tiene muest...\n    \n    \n      5\n      4\n      INVÁLIDO POR LABORATORIO\n      Inválido aplica cuando el caso no tienen asoci...\n    \n    \n      6\n      5\n      NO REALIZADO POR LABORATORIO\n      No realizado aplica cuando el caso no tienen a...\n    \n    \n      7\n      6\n      CASO SOSPECHOSO\n      Sospechoso aplica cuando: \\nEl caso no tienen ...\n    \n    \n      8\n      7\n      NEGATIVO A SARS-COV-2\n      Negativo aplica cuando el caso:\\n1. Se le tomo...\n    \n  \n\n\n\n\nNecesitamos un diccionario {CLASIFICACION:CLAVE} (ya sé que hay unos valores espurios, pero no nos importan porque simplemente esos no los va a encontrar en nuestra base de datos).\nPara construir este diccionario, vamos a empezar por construir la tupla que mantiene la relación que buscamos, para eso vamos a utilizar la función zip que toma dos iteradores como entrada y regresa un iterador que tiene por elementos las tuplas hechas elemento a elemento entre los dos iteradores de inicio. Veamoslo con calma:\n\nl1 = ['a', 'b', 'c']\nl2 = [1, 2, 3]\nl3 = list(zip(l1,l2))\nl3\n\n[('a', 1), ('b', 2), ('c', 3)]\n\n\nLo que nos regresa zip es un iterado con las tuplas formadas por los pares ordenados de los iteradores de entrada. En Python un iterador es cualquier cosa que se pueda recorrer en orden, a veces estos iteradores, como en el caso de zip no regresan todas las entradas sino, para ahorrar memoria, las generan conforme se recorren, por eso hay que hacer list(zip) para que se generen las entradas.\nAhora sí podemos entonces crear el diccionario con el que vamos a actualizar los datos:\n\nclasificacion_final = dict(zip(clasificacion_final['CLAVE'], clasificacion_final['CLASIFICACIÓN']))\nclasificacion_final\n\n{nan: nan,\n 'CLAVE': 'CLASIFICACIÓN',\n '1': 'CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n '2': 'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n '3': 'CASO DE SARS-COV-2  CONFIRMADO',\n '4': 'INVÁLIDO POR LABORATORIO',\n '5': 'NO REALIZADO POR LABORATORIO',\n '6': 'CASO SOSPECHOSO',\n '7': 'NEGATIVO A SARS-COV-2'}\n\n\nY entonces pasarlo como argumento a la función map. Hay un truco aquí, map toma como argumento una función que, para cada llave, regresa el valor correspondiente, entonces no es propiamente el diccionario lo que vamos a pasar, sino la función get del diccionario que hace justo lo que queremos. Esto nos revela una prpiedad curiosa de Python, los argumentos de una función pueden ser funciones.\n\ndf['CLASIFICACION_FINAL'] = df['CLASIFICACION_FINAL'].map(clasificacion_final.get)\ndf['CLASIFICACION_FINAL'].head()\n\n1                                 NEGATIVO A SARS-COV-2\n4                                 NEGATIVO A SARS-COV-2\n8     CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n13                       CASO DE SARS-COV-2  CONFIRMADO\n15                                NEGATIVO A SARS-COV-2\nName: CLASIFICACION_FINAL, dtype: object\n\n\nAhora vamos a hacer una sustitución un poco más compleja, tenemos que encontrar todos los campos de tipo “SI - NO” y resolverlos (sustituir por valores que podamos manejar más fácil). Los campos que tienen este tipo de datos vienen en el excel de descriptores:\n\ndescriptores = pd.read_excel('datos/201128 Descriptores_.xlsx',\n                             index_col='Nº',\n                             engine='openpyxl')\ndescriptores\n\n\n\n\n\n  \n    \n      \n      NOMBRE DE VARIABLE\n      DESCRIPCIÓN DE VARIABLE\n      FORMATO O FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      1\n      FECHA_ACTUALIZACION\n      La base de datos se alimenta diariamente, esta...\n      AAAA-MM-DD\n    \n    \n      2\n      ID_REGISTRO\n      Número identificador del caso\n      TEXTO\n    \n    \n      3\n      ORIGEN\n      La vigilancia centinela se realiza a través de...\n      CATÁLOGO: ORIGEN                              ...\n    \n    \n      4\n      SECTOR\n      Identifica el tipo de institución del Sistema ...\n      CATÁLOGO: SECTOR                              ...\n    \n    \n      5\n      ENTIDAD_UM\n      Identifica la entidad donde se ubica la unidad...\n      CATALÓGO: ENTIDADES\n    \n    \n      6\n      SEXO\n      Identifica al sexo del paciente.\n      CATÁLOGO: SEXO\n    \n    \n      7\n      ENTIDAD_NAC\n      Identifica la entidad de nacimiento del paciente.\n      CATALÓGO: ENTIDADES\n    \n    \n      8\n      ENTIDAD_RES\n      Identifica la entidad de residencia del paciente.\n      CATALÓGO: ENTIDADES\n    \n    \n      9\n      MUNICIPIO_RES\n      Identifica el municipio de residencia del paci...\n      CATALÓGO: MUNICIPIOS\n    \n    \n      10\n      TIPO_PACIENTE\n      Identifica el tipo de atención que recibió el ...\n      CATÁLOGO: TIPO_PACIENTE\n    \n    \n      11\n      FECHA_INGRESO\n      Identifica la fecha de ingreso del paciente a ...\n      AAAA-MM-DD\n    \n    \n      12\n      FECHA_SINTOMAS\n      Idenitifica la fecha en que inició la sintomat...\n      AAAA-MM-DD\n    \n    \n      13\n      FECHA_DEF\n      Identifica la fecha en que el paciente falleció.\n      AAAA-MM-DD\n    \n    \n      14\n      INTUBADO\n      Identifica si el paciente requirió de intubación.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      15\n      NEUMONIA\n      Identifica si al paciente se le diagnosticó co...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      16\n      EDAD\n      Identifica la edad del paciente.\n      NÚMERICA EN AÑOS\n    \n    \n      17\n      NACIONALIDAD\n      Identifica si el paciente es mexicano o extran...\n      CATÁLOGO: NACIONALIDAD\n    \n    \n      18\n      EMBARAZO\n      Identifica si la paciente está embarazada.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      19\n      HABLA_LENGUA_INDIG\n      Identifica si el paciente habla lengua índigena.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      20\n      INDIGENA\n      Identifica si el paciente se autoidentifica co...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      21\n      DIABETES\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      22\n      EPOC\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      23\n      ASMA\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      24\n      INMUSUPR\n      Identifica si el paciente presenta inmunosupre...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      25\n      HIPERTENSION\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      26\n      OTRAS_COM\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      27\n      CARDIOVASCULAR\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      28\n      OBESIDAD\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      29\n      RENAL_CRONICA\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      30\n      TABAQUISMO\n      Identifica si el paciente tiene hábito de taba...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      31\n      OTRO_CASO\n      Identifica si el paciente tuvo contacto con al...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      32\n      TOMA_MUESTRA_LAB\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      33\n      RESULTADO_LAB\n      Identifica el resultado del análisis de la mue...\n      CATÁLOGO: RESULTADO_LAB\n    \n    \n      34\n      TOMA_MUESTRA_ANTIGENO\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      35\n      RESULTADO_ANTIGENO\n      Identifica el resultado del análisis de la mue...\n      CATÁLOGO: RESULTADO_ANTIGENO\n    \n    \n      36\n      CLASIFICACION_FINAL\n      Identifica si el paciente es un caso de COVID-...\n      CATÁLOGO: CLASIFICACION_FINAL\n    \n    \n      37\n      MIGRANTE\n      Identifica si el paciente es una persona migra...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      38\n      PAIS_NACIONALIDAD\n      Identifica la nacionalidad del paciente.\n      TEXTO, 99= SE IGNORA\n    \n    \n      39\n      PAIS_ORIGEN\n      Identifica el país del que partió el paciente ...\n      TEXTO, 97= NO APLICA\n    \n    \n      40\n      UCI\n      Identifica si el paciente requirió ingresar a ...\n      CATÁLOGO: SI_ NO                              ...\n    \n  \n\n\n\n\nFíjense en alguno de estos campos en los datos:\n\ndf['OBESIDAD'].unique()\n\narray(['2', '98', '1'], dtype=object)\n\n\nTenemos tres valores diferentes que corresponden (vean el diccionario) a SI, NO y NO ESPECIFICADO. Para todos los análisis que vamos a hacer en general sólo nos van a interesar los casos que sabemos que son SI, entonces lo que más nos conviene es codificar todos estos como binarios, es decir, sólo SI o NO. Además, podemos mejor decirles 1,0 respectivamente y así vamos a poder hacer cuentas mucho más fácil\nDe estos descriptores nos interesan los que tienen CATÁLOGO: SI_ NO en el campo FORMATO O FUENTE. Para poder encontrar y sustituir de forma más sencilla y automática vamos a hacer un par de modificaciones a los datos:\n\nReemplazar los espacios en los nombres de columnas por guiones bajos (para poder “hablarles” más fácil a las columnas)\nQuitar espacios al principio o al final de los valores de los campos (para asegurarnos de que siempre van a ser los mismos)\n\n\ndescriptores.columns = list(map(lambda col: col.replace(' ', '_'), descriptores.columns))\ndescriptores.head()\n\n\n\n\n\n  \n    \n      \n      NOMBRE_DE_VARIABLE\n      DESCRIPCIÓN_DE_VARIABLE\n      FORMATO_O_FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      1\n      FECHA_ACTUALIZACION\n      La base de datos se alimenta diariamente, esta...\n      AAAA-MM-DD\n    \n    \n      2\n      ID_REGISTRO\n      Número identificador del caso\n      TEXTO\n    \n    \n      3\n      ORIGEN\n      La vigilancia centinela se realiza a través de...\n      CATÁLOGO: ORIGEN                              ...\n    \n    \n      4\n      SECTOR\n      Identifica el tipo de institución del Sistema ...\n      CATÁLOGO: SECTOR                              ...\n    \n    \n      5\n      ENTIDAD_UM\n      Identifica la entidad donde se ubica la unidad...\n      CATALÓGO: ENTIDADES\n    \n  \n\n\n\n\nPoco a poco:\n\ndescriptores.columns nos regresa (o les da valor, cuando está del lado izquierdo de un =) los nombres de las columnas del DataFrame\nmap(lambda col: col.replace(' ', '_'), descriptores.columns) la función map regresa una asosiación, como ya vimos. En este caso esta asosiación se hace a través de una función anónima lambda que toma como argumento el nombre de una columna y regresa el mismo nombre pero con los espacios sustituidos por guines bajos\n\nAl final, lo que hacemos es sustituir los nombres de las columnas por una lista hecha por nosotros, para que esto funcione la lista que pasamos debe ser de igual tamaño que la lista original de columnas.\nAhora vamos a hacer lo mismo pero con los valores de los campos:\n\ndescriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\ndescriptores['FORMATO_O_FUENTE'].head()\n\nNº\n1             AAAA-MM-DD\n2                  TEXTO\n3       CATÁLOGO: ORIGEN\n4       CATÁLOGO: SECTOR\n5    CATALÓGO: ENTIDADES\nName: FORMATO_O_FUENTE, dtype: object\n\n\nEste fué más fácil. Fíjense cómo pedimos el campo del lado derecho: descriptores.FORMATO_O_FUENTE, esto es equivalente a descriptores['FORMATO_O_FUENTE'] y los pueden usar indistintamente (claro, el primero sólo funciona si el nombre del campo no tiene espacios).\nFiltremos ahora los descriptores para quedarnos sólo con los que nos interesan, para eso vamos a usar la función query de Pandas, que nos permite filtrar un DataFrame de forma conveniente usando una expresión booleana:\n\ndatos_si_no = descriptores.query('FORMATO_O_FUENTE == \"CATÁLOGO: SI_ NO\"')\ndatos_si_no\n\n\n\n\n\n  \n    \n      \n      NOMBRE_DE_VARIABLE\n      DESCRIPCIÓN_DE_VARIABLE\n      FORMATO_O_FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      14\n      INTUBADO\n      Identifica si el paciente requirió de intubación.\n      CATÁLOGO: SI_ NO\n    \n    \n      15\n      NEUMONIA\n      Identifica si al paciente se le diagnosticó co...\n      CATÁLOGO: SI_ NO\n    \n    \n      18\n      EMBARAZO\n      Identifica si la paciente está embarazada.\n      CATÁLOGO: SI_ NO\n    \n    \n      19\n      HABLA_LENGUA_INDIG\n      Identifica si el paciente habla lengua índigena.\n      CATÁLOGO: SI_ NO\n    \n    \n      20\n      INDIGENA\n      Identifica si el paciente se autoidentifica co...\n      CATÁLOGO: SI_ NO\n    \n    \n      21\n      DIABETES\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      22\n      EPOC\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      23\n      ASMA\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      24\n      INMUSUPR\n      Identifica si el paciente presenta inmunosupre...\n      CATÁLOGO: SI_ NO\n    \n    \n      25\n      HIPERTENSION\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      26\n      OTRAS_COM\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      27\n      CARDIOVASCULAR\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      28\n      OBESIDAD\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      29\n      RENAL_CRONICA\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      30\n      TABAQUISMO\n      Identifica si el paciente tiene hábito de taba...\n      CATÁLOGO: SI_ NO\n    \n    \n      31\n      OTRO_CASO\n      Identifica si el paciente tuvo contacto con al...\n      CATÁLOGO: SI_ NO\n    \n    \n      32\n      TOMA_MUESTRA_LAB\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      34\n      TOMA_MUESTRA_ANTIGENO\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      37\n      MIGRANTE\n      Identifica si el paciente es una persona migra...\n      CATÁLOGO: SI_ NO\n    \n    \n      40\n      UCI\n      Identifica si el paciente requirió ingresar a ...\n      CATÁLOGO: SI_ NO\n    \n  \n\n\n\n\nPor si acaso, quitémosle también los espacios al campo FORMATO_O_FUENTE\n\ndescriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\n\nAhora sí, vamos a sustituir los valores como queremos en los datos originales. Para eso, lo primero que tenemos que hacer es fijarnos en el catálogo de estos campos:\n\ncat_si_no = dict_catalogos['Catálogo SI_NO']\ncat_si_no\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      1\n      SI\n    \n    \n      1\n      2\n      NO\n    \n    \n      2\n      97\n      NO APLICA\n    \n    \n      3\n      98\n      SE IGNORA\n    \n    \n      4\n      99\n      NO ESPECIFICADO\n    \n  \n\n\n\n\nJusto estos valores los queremos cambiar por claves binarias (acuérdense, para distinguirlos fácilmente). Entonces lo que necesitamos ahora es:\n\nUna lista de los nombres de los campos en donde vamos a hacer la sustitución\nUn mapeo de los valores con los que vamos a sustituir\nHacer la sustitución primero en el diccionario y a partir de eso en los datos originales\n\n\n# lista de los nombres de los campos\ncampos_si_no = datos_si_no.NOMBRE_DE_VARIABLE\n# sustituimos en el catálogo de acuerdo a lo que nos interesa\ncat_si_no['DESCRIPCIÓN'] = list(map(lambda val: 1 if val == 'SI' else 0, cat_si_no['DESCRIPCIÓN']))\n# sustituimos en los datos originales\ndf[campos_si_no] = df[datos_si_no.NOMBRE_DE_VARIABLE].replace(\n                                            to_replace=cat_si_no['CLAVE'].values,\n                                            value=cat_si_no['DESCRIPCIÓN'].values)\ndf[campos_si_no]\n\n\n\n\n\n  \n    \n      \n      INTUBADO\n      NEUMONIA\n      EMBARAZO\n      HABLA_LENGUA_INDIG\n      INDIGENA\n      DIABETES\n      EPOC\n      ASMA\n      INMUSUPR\n      HIPERTENSION\n      OTRAS_COM\n      CARDIOVASCULAR\n      OBESIDAD\n      RENAL_CRONICA\n      TABAQUISMO\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      TOMA_MUESTRA_ANTIGENO\n      MIGRANTE\n      UCI\n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      13\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6393642\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394417\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394626\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394988\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6395781\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n1896084 rows × 20 columns\n\n\n\nAcá utilizamos para la última sustitución la función replace de Pandas que toma dos parámetros: la lista de valores a reemplazar y la lista de los valoresa de reemplazo. El reemplazo sucede elemento a elemento, es decir, se sustituye el primer elemento de la lista to_replace por el primer elemento de la lista value y así sucesivamente.\nHay más campos que podemos aplanar en la base de datos, como ejercicio pueden explorar algunos de ellos y sustituir como le hemos hecho aquí. Regresaremos a esto más adelante en el taller, pero por lo pronto nos vamos a mover a otra etapa del pre-procesamiento: el manejo de las fechas"
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#manejo-de-fechas",
    "href": "parte_1/02_ejemplo_transformacion.html#manejo-de-fechas",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.3 Manejo de fechas",
    "text": "2.3 Manejo de fechas\nEn Python las fechas son un tipo especial de datos, nosotros estamos acostumbrados a verlas como cadenas de caractéres: 20 de febrero de 2010, por ejemplo. Python puede hacer muchas cosas con las fechas, pero para eso tienen que estar codificados de la forma correcta.\nEn general el módulo datetime de Python provee las utilerías necesarias para manejar/transformar objetos del tipo fecha. Una de las cosas más útiles es transformar strings en objetos datetime:\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime_object\n\ndatetime.datetime(2005, 6, 1, 13, 33)\n\n\nAcá usamos un formato de fecha, '%b %d %Y %I:%M%p', para convertir el string 'Jun 1 2005  1:33PM'. De esa misma forma podemos especificar formatos diferentes:\n\ndatetime_object = datetime.strptime('06-01-2005  1:33PM', '%m-%d-%Y %I:%M%p')\ndatetime_object\n\ndatetime.datetime(2005, 6, 1, 13, 33)\n\n\nPandas tiene la interfase to_datetime para este tipo de operaciones que nos permite transformar campos de forma muy sencilla, por ejemplo, para transformar la columna FECHA_INGRESO de los datos originales en objetos de tipo datetime podemos hacer:\n\npd.to_datetime(df['FECHA_INGRESO'].head())\n\n1    2022-01-19\n4    2022-03-09\n8    2022-02-20\n13   2022-01-01\n15   2022-06-28\nName: FECHA_INGRESO, dtype: datetime64[ns]\n\n\nVean la diferencia con el tipo de datos original:\n\ndf['FECHA_INGRESO'].head()\n\n1     2022-01-19\n4     2022-03-09\n8     2022-02-20\n13    2022-01-01\n15    2022-06-28\nName: FECHA_INGRESO, dtype: object\n\n\nEsto va a funcionar perfecto cuando el campo tiene sólo datos válidos (todos se ajustan a algún formato de fecha), pero ¿qué pasa en campos en donde no todos los datos son fechas válidas?\n\npd.to_datetime(df['FECHA_DEF'].head())\n\nParserError: month must be in 1..12: 9999-99-99 present at position 0\n\n\nERROR!!!!! Efectívamente, pandas no puede transformar algunos datos en fechas verdaderas (porque no todos los registros tienen una fecha de defunción válida). Entonces hay que decirle a Pandas qué hacer con estos registros, lo que queremos en este caso es que los registros inválidos los regrese como nulos, para eso usamos la opción coerce\n\npd.to_datetime(df['FECHA_DEF'].head(), 'coerce')\n\n1           NaT\n4           NaT\n8    2022-02-21\n13          NaT\n15          NaT\nName: FECHA_DEF, dtype: datetime64[ns]\n\n\nListo, los registros que no se pueden convertir en fechas ahopra regresan NaT (Not a Time) en lugar de error.\nCon esto en realidad ya es muy simple convertir toda una columna en tipo fecha:\n\ndf['FECHA_INGRESO'] = pd.to_datetime(df['FECHA_INGRESO'])\ndf['FECHA_INGRESO'].head()\n\n1    2022-01-19\n4    2022-03-09\n8    2022-02-20\n13   2022-01-01\n15   2022-06-28\nName: FECHA_INGRESO, dtype: datetime64[ns]"
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#exportar-datos",
    "href": "parte_1/02_ejemplo_transformacion.html#exportar-datos",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.4 Exportar datos",
    "text": "2.4 Exportar datos\nYa que tenemos procesados los datos, es muy posible que los querramos guardar para usarlos más adelante. La forma más sencilla de exportar los datos es guardarlos como un csv. Para esto Pandas tiene el método to_csv\n\ndf.to_csv(\"datos/covid_enero_2023_procesados.csv\")\n\nHasta aquí hemos cubierto más o menos todo el pre-proceso de los datos. Claro no vimos todas las columnas, sólo nos fijamos en algunas, pero eso basta para darnos una buena idea de cómo se hacen las demás.\n\n2.4.1 Tarea\nSustituyan los valores de la columna TIPO_PACIENTE por sus valores en el catálogo correspondiente"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#bajar-y-guardar-datos",
    "href": "parte_1/03_automatizacion_transformacion.html#bajar-y-guardar-datos",
    "title": "3  Automatización",
    "section": "3.1 Bajar y guardar datos",
    "text": "3.1 Bajar y guardar datos\nEn el taller anterior bajamos los datos directamente del sitio de la Secretaría de Salud, ahora vamos a automatizar el proceso de descarga de datos de forma que, desde Python, podamos descargar los datos y asegurarnos de que tenemos la última versión disponible.\nDescargar y guardar archivos en Python es relativamente sencillo, vamos a usar tres módulos de la distribución base de Python:\n\nos. Este módulo provee herramientas para interactuar con el sistema operativo. La vamos a usar para construir los paths en donde vamos a guardar los datos y preguntar si el archivo ya existe.\nrequests. Esta librería provee diferentes formas de interactuar con el protocolo HTTP. La vamos a usar para hacer las peticiones a la página y procesar la respuesta.\nzipfile. Esta librería sirve para trabajar con archivos comprimidos en formato zip. En nuestro caso la usaremos para descomprimir los diccionarios.\n\nLa parte complicada de entender es el uso de requests pra comunicarse con la página en donde están los datos.\n\nr = requests.get(\"https://www.centrogeo.org.mx/\")\nr.content[0:500]\n\nb'\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"es-es\" dir=\"ltr\" class=\\'com_content view-featured itemid-101 home j31 mm-hover\\'>\\r\\n<head>\\r\\n<base href=\"https://www.centrogeo.org.mx/\" />\\n\\t<meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\\n\\t<meta name=\"keywords\" content=\"M\\xc3\\xa9xico, CONACYT, CentroGeo, Ciencias de Informaci\\xc3\\xb3n Geoespacial, Centro de Investigaci\\xc3\\xb3n, Ciencias de Informaci\\xc3\\xb3n, Investigaci\\xc3\\xb3n, Geoespacial\" />\\n\\t<meta name=\"rights\" content=\"Esta obra est\\xc3\\xa1 bajo una licencia d'\n\n\nComo ven, una petición de tipo get simplemente nos regresa, a través de la propiedad content, el contenido de la respuesta del servidor. En el caso de la página de CentroGeo, el contenido es el HTML de la página (que podríamos ver mejor con un browser), pero en el caso de que la dirección apunte a un archivo de descarga, el contenido es el stream de datos del archivo. Este stream de datos lo podemos usar como entrada para escribir un archivo utilizando la función open.\nLa función open va a tomar como entrada el path en donde queremos guardar el archivo, este path puede ser simplemente una cadena de caracteres, sin embargo esto haría que nuestro código no fuera interoperable entre sistemas operativos, entonces, en lugar de escribir el path como cadena de caracteres, vamos a escribirlo como un objeto de os:\n\nos.path.join(\"datos\", \"datos_covid.zip\")\n\n'datos/datos_covid.zip'\n\n\nEsta forma de construir el path nos asegura que va a funcionar en cualquier sistema operativo.\nYa con estas explicaciones, podemos escribir la función que descarga los datos:\n\ndef bajar_datos_salud(directorio_datos='data/'):\n    '''\n        Descarga el ultimo archivo disponible en datos abiertos y los diccionarios correspondientes.\n    '''\n    fecha_descarga = datetime.now().date()\n    url_datos = 'https://datosabiertos.salud.gob.mx/gobmx/salud/datos_abiertos/datos_abiertos_covid19.zip'    \n    archivo_nombre = f'{fecha_descarga.strftime(\"%y%m%d\")}COVID19MEXICO.csv.zip'\n    archivo_ruta = os.path.join(directorio_datos, archivo_nombre)\n    url_diccionario = 'https://datosabiertos.salud.gob.mx/gobmx/salud/datos_abiertos/diccionario_datos_covid19.zip'\n    diccionario_ruta = os.path.join(directorio_datos, 'diccionario.zip')\n    if os.path.exists(archivo_ruta):\n        logging.debug(f'Ya existe {archivo_nombre}')\n    else:\n        print(f'Bajando datos...')\n        r = requests.get(url_datos, allow_redirects=True)\n        open(archivo_ruta, 'wb').write(r.content)\n        r = requests.get(url_diccionario, allow_redirects=True)\n        open(diccionario_ruta, 'wb').write(r.content)\n        with zipfile.ZipFile(diccionario_ruta, 'r') as zip_ref:\n          zip_ref.extractall(directorio_datos)\n\nPara utilizar la función hacemos:\n\nbajar_datos_salud('datos/')\n\nBajando datos..."
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#preproceso",
    "href": "parte_1/03_automatizacion_transformacion.html#preproceso",
    "title": "3  Automatización",
    "section": "3.2 Preproceso",
    "text": "3.2 Preproceso\nAhora, ya que tenemos los datos descargados, vamos a empaquetar en una función el flujo de preproceso que trabajamos en el taller anterior. Esta función va a tomar como entrada la carpeta en donde se encuentran los datos y dicionariosy el nombre del archivo de datos que queremos procesar. Toma dos parámetros adicionales, uno para decidir si queremos resolver o no las claves binarias y otro para definir la entidad que queremos procesar.\n\ndef carga_datos_covid19_MX(data_dir='datos/', archivo='datos_abiertos_covid19.zip', resolver_claves='si_no_binarias', entidad='09'):\n    \"\"\"\n        Lee en un DataFrame el CSV con el reporte de casos de la Secretaría de Salud de México publicado en una fecha dada. Esta función\n        también lee el diccionario de datos que acompaña a estas publicaciones para preparar algunos campos, en particular permite la funcionalidad\n        de generar columnas binarias para datos con valores 'SI', 'No'.\n\n        **Nota 2**: Por las actualizaciones a los formatos de datos, esta función sólo va a servir para archivos posteriores a 20-11-28\n\n        resolver_claves: 'sustitucion', 'agregar', 'si_no_binarias', 'solo_localidades'. Resuelve los valores del conjunto de datos usando el\n        diccionario de datos y los catálogos. 'sustitucion' remplaza los valores en las columnas, 'agregar'\n        crea nuevas columnas. 'si_no_binarias' cambia valores SI, NO, No Aplica, SE IGNORA, NO ESPECIFICADO por 1, 0, 0, 0, 0 respectivamente.\n\n    \"\"\"\n    catalogo_nombre ='201128 Catalogos.xlsx'\n    catalogo_path = os.path.join(data_dir, catalogo_nombre)\n    descriptores_nombre = '201128 Descriptores.xlsx'\n    descriptores_path = os.path.join(data_dir, descriptores_nombre)\n    data_file = os.path.join(data_dir, archivo)\n    print(data_file)\n    df = pd.read_csv(data_file, dtype=object, encoding='latin-1')\n    if entidad is not None:\n      df = df[df['ENTIDAD_RES'] == entidad]\n    # Hay un error y el campo OTRA_COMP es OTRAS_COMP según los descriptores\n    df.rename(columns={'OTRA_COM': 'OTRAS_COM'}, inplace=True)\n    # Asignar clave única a municipios\n    df['MUNICIPIO_RES'] = df['ENTIDAD_RES'] + df['MUNICIPIO_RES']\n    df['CLAVE_MUNICIPIO_RES'] = df['MUNICIPIO_RES']\n    # Leer catalogos\n    nombres_catalogos = ['Catálogo de ENTIDADES',\n                         'Catálogo MUNICIPIOS',\n                         'Catálogo RESULTADO',\n                         'Catálogo SI_NO',\n                         'Catálogo TIPO_PACIENTE']\n    nombres_catalogos.append('Catálogo CLASIFICACION_FINAL')\n    nombres_catalogos[2] = 'Catálogo RESULTADO_LAB'\n\n    dict_catalogos = pd.read_excel(catalogo_path,\n                              nombres_catalogos,\n                              dtype=str,\n                              engine='openpyxl')\n\n    entidades = dict_catalogos[nombres_catalogos[0]]\n    municipios = dict_catalogos[nombres_catalogos[1]]\n    tipo_resultado = dict_catalogos[nombres_catalogos[2]]\n    cat_si_no = dict_catalogos[nombres_catalogos[3]]\n    cat_tipo_pac = dict_catalogos[nombres_catalogos[4]]\n    # Arreglar los catálogos que tienen mal las primeras líneas\n    dict_catalogos[nombres_catalogos[2]].columns = [\"CLAVE\", \"DESCRIPCIÓN\"]\n    dict_catalogos[nombres_catalogos[5]].columns = [\"CLAVE\", \"CLASIFICACIÓN\", \"DESCRIPCIÓN\"]\n\n\n    clasificacion_final = dict_catalogos[nombres_catalogos[5]]\n\n\n    # Resolver códigos de entidad federal\n    cols_entidad = ['ENTIDAD_RES', 'ENTIDAD_UM', 'ENTIDAD_NAC']\n    df['CLAVE_ENTIDAD_RES'] = df['ENTIDAD_RES']\n    df[cols_entidad] = df[cols_entidad].replace(to_replace=entidades['CLAVE_ENTIDAD'].values,\n                                               value=entidades['ENTIDAD_FEDERATIVA'].values)\n\n    # Construye clave unica de municipios de catálogo para resolver nombres de municipio\n    municipios['CLAVE_MUNICIPIO'] = municipios['CLAVE_ENTIDAD'] + municipios['CLAVE_MUNICIPIO']\n\n    # Resolver códigos de municipio\n    municipios_dict = dict(zip(municipios['CLAVE_MUNICIPIO'], municipios['MUNICIPIO']))\n    df['MUNICIPIO_RES'] = df['MUNICIPIO_RES'].map(municipios_dict.get)\n\n    # Resolver resultados\n\n    df.rename(columns={'RESULTADO_LAB': 'RESULTADO'}, inplace=True)\n    tipo_resultado['DESCRIPCIÓN'].replace({'POSITIVO A SARS-COV-2': 'Positivo SARS-CoV-2'}, inplace=True)\n\n    tipo_resultado = dict(zip(tipo_resultado['CLAVE'], tipo_resultado['DESCRIPCIÓN']))\n    df['RESULTADO'] = df['RESULTADO'].map(tipo_resultado.get)\n    clasificacion_final = dict(zip(clasificacion_final['CLAVE'], clasificacion_final['CLASIFICACIÓN']))\n    df['CLASIFICACION_FINAL'] = df['CLASIFICACION_FINAL'].map(clasificacion_final.get)\n    # Resolver datos SI - NO\n\n    # Necesitamos encontrar todos los campos que tienen este tipo de dato y eso\n    # viene en los descriptores, en el campo FORMATO_O_FUENTE\n    descriptores = pd.read_excel(f'{data_dir}201128 Descriptores_.xlsx',\n                                 index_col='Nº',\n                                 engine='openpyxl')\n    descriptores.columns = list(map(lambda col: col.replace(' ', '_'), descriptores.columns))\n    descriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\n\n    datos_si_no = descriptores.query('FORMATO_O_FUENTE == \"CATÁLOGO: SI_ NO\"')\n    cat_si_no['DESCRIPCIÓN'] = cat_si_no['DESCRIPCIÓN'].str.strip()\n\n    campos_si_no = datos_si_no.NOMBRE_DE_VARIABLE\n    nuevos_campos_si_no = campos_si_no\n\n    if resolver_claves == 'agregar':\n        nuevos_campos_si_no = [nombre_var + '_NOM' for nombre_var in campos_si_no]\n    elif resolver_claves == 'si_no_binarias':\n        nuevos_campos_si_no = [nombre_var + '_BIN' for nombre_var in campos_si_no]\n        cat_si_no['DESCRIPCIÓN'] = list(map(lambda val: 1 if val == 'SI' else 0, cat_si_no['DESCRIPCIÓN']))\n\n    df[nuevos_campos_si_no] = df[datos_si_no.NOMBRE_DE_VARIABLE].replace(\n                                                to_replace=cat_si_no['CLAVE'].values,\n                                                value=cat_si_no['DESCRIPCIÓN'].values)\n\n    # Resolver tipos de paciente\n    cat_tipo_pac = dict(zip(cat_tipo_pac['CLAVE'], cat_tipo_pac['DESCRIPCIÓN']))\n    df['TIPO_PACIENTE'] = df['TIPO_PACIENTE'].map(cat_tipo_pac.get)\n\n    df = procesa_fechas(df)\n\n    return df\n\ndef procesa_fechas(covid_df):\n    df = covid_df.copy()\n\n    df['FECHA_INGRESO'] = pd.to_datetime(df['FECHA_INGRESO'])\n    df['FECHA_SINTOMAS'] = pd.to_datetime(df['FECHA_SINTOMAS'])\n    df['FECHA_DEF'] = pd.to_datetime(df['FECHA_DEF'], 'coerce')\n    df['DEFUNCION'] = (df['FECHA_DEF'].notna()).astype(int)\n    df['EDAD'] = df['EDAD'].astype(int)\n\n    df.set_index('FECHA_INGRESO', drop=False, inplace=True)\n    df['AÑO_INGRESO'] = df.index.year\n    df['MES_INGRESO'] = df.index.month\n    df['DIA_SEMANA_INGRESO'] = df.index.weekday\n    df['SEMANA_AÑO_INGRESO'] = df.index.week\n    df['DIA_MES_INGRESO'] = df.index.day\n    df['DIA_AÑO_INGRESO'] = df.index.dayofyear\n\n    return df"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#preprocesar-usando-nuestras-funciones",
    "href": "parte_1/03_automatizacion_transformacion.html#preprocesar-usando-nuestras-funciones",
    "title": "3  Automatización",
    "section": "3.3 Preprocesar usando nuestras funciones",
    "text": "3.3 Preprocesar usando nuestras funciones\n\nbajar_datos_salud('datos/')\ndf = carga_datos_covid19_MX(entidad='09')\ndf\n\ndatos/datos_abiertos_covid19.zip\n\n\n/tmp/ipykernel_138343/243715272.py:124: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n  df['SEMANA_AÑO_INGRESO'] = df.index.week\n\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      TOMA_MUESTRA_ANTIGENO_BIN\n      MIGRANTE_BIN\n      UCI_BIN\n      DEFUNCION\n      AÑO_INGRESO\n      MES_INGRESO\n      DIA_SEMANA_INGRESO\n      SEMANA_AÑO_INGRESO\n      DIA_MES_INGRESO\n      DIA_AÑO_INGRESO\n    \n    \n      FECHA_INGRESO\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-01-19\n      2023-01-03\n      180725\n      2\n      9\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      HOSPITALIZADO\n      ...\n      1\n      0\n      0\n      0\n      2022\n      1\n      2\n      3\n      19\n      19\n    \n    \n      2022-03-09\n      2023-01-03\n      1933c0\n      1\n      12\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      IZTAPALAPA\n      AMBULATORIO\n      ...\n      1\n      0\n      0\n      0\n      2022\n      3\n      2\n      10\n      9\n      68\n    \n    \n      2022-02-20\n      2023-01-03\n      0741e4\n      2\n      6\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      MIGUEL HIDALGO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      0\n      1\n      2022\n      2\n      6\n      7\n      20\n      51\n    \n    \n      2022-01-01\n      2023-01-03\n      1c4d2e\n      2\n      9\n      CIUDAD DE MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      1\n      0\n      0\n      0\n      2022\n      1\n      5\n      52\n      1\n      1\n    \n    \n      2022-06-28\n      2023-01-03\n      0a6cd6\n      2\n      6\n      CIUDAD DE MÉXICO\n      1\n      NAYARIT\n      CIUDAD DE MÉXICO\n      IZTAPALAPA\n      AMBULATORIO\n      ...\n      1\n      0\n      0\n      0\n      2022\n      6\n      1\n      26\n      28\n      179\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-01-23\n      2023-01-03\n      m1cd235\n      2\n      12\n      MÉXICO\n      1\n      MÉXICO\n      CIUDAD DE MÉXICO\n      GUSTAVO A. MADERO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      0\n      0\n      2022\n      1\n      6\n      3\n      23\n      23\n    \n    \n      2022-11-17\n      2023-01-03\n      m0dbc4c\n      2\n      12\n      MÉXICO\n      1\n      AGUASCALIENTES\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      2022\n      11\n      3\n      46\n      17\n      321\n    \n    \n      2022-11-16\n      2023-01-03\n      m13431e\n      2\n      12\n      MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      1\n      0\n      0\n      0\n      2022\n      11\n      2\n      46\n      16\n      320\n    \n    \n      2022-12-01\n      2023-01-03\n      m1493ea\n      2\n      12\n      MÉXICO\n      1\n      MÉXICO\n      CIUDAD DE MÉXICO\n      GUSTAVO A. MADERO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      2022\n      12\n      3\n      48\n      1\n      335\n    \n    \n      2022-12-16\n      2023-01-03\n      m0a22b8\n      2\n      12\n      MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      2022\n      12\n      4\n      50\n      16\n      350\n    \n  \n\n1896084 rows × 69 columns"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#guardando-el-resultado",
    "href": "parte_1/03_automatizacion_transformacion.html#guardando-el-resultado",
    "title": "3  Automatización",
    "section": "3.4 Guardando el resultado",
    "text": "3.4 Guardando el resultado\nListo, con nuestras funciones tenemos ya nuestros datos preprocesados, ahora vamos a guardarlos para poder utlizarlos rápidamente en otros notebooks. En general tenemos muchas opciones para guardar los datos, csv, por ejemplo. En esta ocasión vamos a usar un formato nativo de Python el pickle, que es una forma de serializar un objeto de Python. Pandas nos provee una función para guardar directamente un dataframe como pickle:\n\ndf.to_pickle(\"data/datos_covid_ene19.pkl\")\n\nEn la documentación de to_pickle pueden ver las opcioones completas."
  },
  {
    "objectID": "parte_1/04_visualizacion_covid_1.html#curvas-epidémicas",
    "href": "parte_1/04_visualizacion_covid_1.html#curvas-epidémicas",
    "title": "4  Curvas epidémicas",
    "section": "4.1 Curvas epidémicas",
    "text": "4.1 Curvas epidémicas\nLo primero que haremos será el desarrollo de Curvas Epidémicas es decir, la evolución temporal de los casos confirmados y las defunciones. Si consultamos los diccionarios de datos, podemos ver que los casos confirmados para COVID-19 corresponden a 3 categorías de la columna clasificación final:\n\nCASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA\nCASO DE COVID-19 CONFIRMADO POR COMITÉ DE DICTAMINACIÓN\nCASO DE SARS-COV-2 CONFIRMADO\n\nmientras que las defunciones corresponden a todos aquellos registros que tengan una fecha de defunción válida, es decir, en nuestros datos preprocesados, todas las fechas válidas.\n\n4.1.1 Curva de casos confirmados\nEl primer paso es extraer las filas que corresponden a casos confirmados\n\ndf.CLASIFICACION_FINAL.unique()\n\narray(['NEGATIVO A SARS-COV-2',\n       'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n       'CASO DE SARS-COV-2  CONFIRMADO', 'CASO SOSPECHOSO',\n       'NO REALIZADO POR LABORATORIO',\n       'CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n       'INVÁLIDO POR LABORATORIO'], dtype=object)\n\n\nA partir de estos valores podemos seleccionar las filas que queremos\n\nvalores_confirmados = ['CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n                       'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n                       'CASO DE SARS-COV-2  CONFIRMADO']\nconfirmados = df.loc[df['CLASIFICACION_FINAL'].isin(valores_confirmados)]\nconfirmados.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      9\n      2\n      9\n      9\n      9016\n      2\n      ...\n      0\n      0\n      4\n      0\n      97\n      CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n      0\n      MÃ©xico\n      97\n      0\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      9\n      1\n      9\n      9\n      9012\n      1\n      ...\n      0\n      0\n      97\n      0\n      1\n      CASO DE SARS-COV-2  CONFIRMADO\n      0\n      MÃ©xico\n      97\n      0\n    \n    \n      25\n      2023-01-03\n      0a98b4\n      2\n      12\n      9\n      1\n      16\n      9\n      9009\n      1\n      ...\n      0\n      0\n      97\n      0\n      1\n      CASO DE SARS-COV-2  CONFIRMADO\n      0\n      MÃ©xico\n      97\n      0\n    \n    \n      42\n      2023-01-03\n      13cf10\n      2\n      9\n      9\n      1\n      9\n      9\n      9014\n      1\n      ...\n      0\n      0\n      1\n      0\n      97\n      CASO DE SARS-COV-2  CONFIRMADO\n      0\n      MÃ©xico\n      97\n      0\n    \n    \n      54\n      2023-01-03\n      0fef08\n      1\n      12\n      9\n      2\n      9\n      9\n      9002\n      1\n      ...\n      0\n      0\n      1\n      0\n      97\n      CASO DE SARS-COV-2  CONFIRMADO\n      0\n      MÃ©xico\n      97\n      0\n    \n  \n\n5 rows × 40 columns\n\n\n\nAhora tenemos una tabla con todos los casos confirmados, para hacer una curva epidémica, tenemos que agregar en una escala temporal. Lo más sencillo es primero agregar por día y a partir de ahí podemos construir agregados para cualquier intervalo que queramos.\nNecesitamos decidir cuál fecha de todas las disponibles vamos a utilizar para agregar los casos. En este caso, la DGE sugiere utilizar la fecha de inicio de síntomas (FECHA_SINTOMAS) para construir la curva de casos confirmados y la de defunción (FECHA_DEF) para la curva de defunciones.\nEntonces, para construir la curva de confirmados lo primero que tenemos que hacer es indexar el DataFrame por la fecha de inicio de síntomas\n\nconfirmados = confirmados.set_index('FECHA_SINTOMAS')\nconfirmados.index\n\nDatetimeIndex(['2022-02-13', '2022-01-01', '2022-04-22', '2022-08-07',\n               '2022-01-10', '2022-01-14', '2022-12-01', '2022-05-25',\n               '2022-12-23', '2022-08-02',\n               ...\n               '2022-06-23', '2022-08-16', '2022-08-19', '2022-08-01',\n               '2022-07-05', '2022-07-08', '2022-09-05', '2022-06-19',\n               '2022-06-20', '2022-09-23'],\n              dtype='datetime64[ns]', name='FECHA_SINTOMAS', length=769894, freq=None)\n\n\nYa con los datos indexados es fácil construir agregados diarios, sólo tenemos que seleccionar qué columnas queremos agregar. Por lo pronto hagamos un conteo sólo de casos confirmados. Para eso sólo tenemos que agrupár el índice usando una frecuencia diaría y tomar el tamaño de los grupos (de alguna columna, realmente no importa cual).\n\nconfirmados_diarios = (confirmados\n                       .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                       .size() # Calculamos el tamaño de cada grupo\n                       .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                       .rename({0:'Confirmados'}, axis=1) # Le damos nombre a la columna que obtenemos\n                       )\nconfirmados_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      Confirmados\n    \n  \n  \n    \n      0\n      2022-01-01\n      6748\n    \n    \n      1\n      2022-01-02\n      6585\n    \n    \n      2\n      2022-01-03\n      10398\n    \n    \n      3\n      2022-01-04\n      9729\n    \n    \n      4\n      2022-01-05\n      10924\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      362\n      2022-12-29\n      715\n    \n    \n      363\n      2022-12-30\n      399\n    \n    \n      364\n      2022-12-31\n      258\n    \n    \n      365\n      2023-01-01\n      187\n    \n    \n      366\n      2023-01-02\n      80\n    \n  \n\n367 rows × 2 columns\n\n\n\nHay muchas formas de visualizar estos datos, la primera y más sencilla es utilizar los métodos que provee Pandas, por ejemplo:\n\nconfirmados_diarios.set_index('FECHA_SINTOMAS').plot()\n\n<AxesSubplot: xlabel='FECHA_SINTOMAS'>\n\n\n\n\n\nUna alternativa que nos provee herramientas interactivas para visualizar los datos y que es muy fácil de usar es Plotly. A través del módulo Plotly express podemos crear de forma muy simple gráficas que nos permitan interactuar con ellas.\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y=\"Confirmados\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nComo pueden ver, fue muy simple hacer una gráfica con herramientas para pan y zoom. Estas herramientas hacen más fácil ver que los datos de casos confirmados contienen la mezcla de dos señales: una de alta frecuancia que representa la variación diaria, con una especie de periodicidad semanal y una seññal de baja frecuencia que contiene las olas epidémicas.\nLa señal de alta frecuencia contiene mucho ruido que corresponde a los ciclos de actualización de la información y que realmente nos dice poco de la tendencia de los datos. Una forma sencilla de filtrar este ruido es utilizando la media móvil. Para calcular este promedio, Pandas provee la función rolling\n\nconfirmados_diarios['Media Móvil'] = (confirmados_diarios\n                                      .rolling(window=7)\n                                      .mean())\nconfirmados_diarios.head(10)\n\n/tmp/ipykernel_10963/2896601829.py:3: FutureWarning:\n\nDropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['FECHA_SINTOMAS'], dtype='object')\n\n\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      Confirmados\n      Media Móvil\n    \n  \n  \n    \n      0\n      2022-01-01\n      6748\n      NaN\n    \n    \n      1\n      2022-01-02\n      6585\n      NaN\n    \n    \n      2\n      2022-01-03\n      10398\n      NaN\n    \n    \n      3\n      2022-01-04\n      9729\n      NaN\n    \n    \n      4\n      2022-01-05\n      10924\n      NaN\n    \n    \n      5\n      2022-01-06\n      9816\n      NaN\n    \n    \n      6\n      2022-01-07\n      11910\n      9444.285714\n    \n    \n      7\n      2022-01-08\n      11229\n      10084.428571\n    \n    \n      8\n      2022-01-09\n      11794\n      10828.571429\n    \n    \n      9\n      2022-01-10\n      19673\n      12153.571429\n    \n  \n\n\n\n\nY ahora la podemos graficar\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y='Media Móvil')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nPara graficar las dos series en la misma gráfica lo más sencillo es pasar los datos del formato ancho (en columnas) al formato largo (en filas con una columna que los distinga). Para esto vamos a usar la función melt de Pandas\n\n confirmados_diarios = confirmados_diarios.melt(id_vars=['FECHA_SINTOMAS'], value_vars=['Confirmados', 'Media Móvil'])\n confirmados_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      variable\n      value\n    \n  \n  \n    \n      0\n      2022-01-01\n      Confirmados\n      6748.000000\n    \n    \n      1\n      2022-01-02\n      Confirmados\n      6585.000000\n    \n    \n      2\n      2022-01-03\n      Confirmados\n      10398.000000\n    \n    \n      3\n      2022-01-04\n      Confirmados\n      9729.000000\n    \n    \n      4\n      2022-01-05\n      Confirmados\n      10924.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      2022-12-29\n      Media Móvil\n      1137.142857\n    \n    \n      730\n      2022-12-30\n      Media Móvil\n      1045.571429\n    \n    \n      731\n      2022-12-31\n      Media Móvil\n      934.571429\n    \n    \n      732\n      2023-01-01\n      Media Móvil\n      796.714286\n    \n    \n      733\n      2023-01-02\n      Media Móvil\n      573.714286\n    \n  \n\n734 rows × 3 columns\n\n\n\nCon los datos de esta forma, ahora podemos usar Plotly para graficar ambas variables utilizando como color la columna variable\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y='value', color='variable')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n4.1.2 Curva de defunciones\nYa que construimos la curva de casos confirmados, la de defunciones es exáctamente igual, sólo necesitamos seleccionar al inicio del proceso los renglones que tengan una fecha de defunción válida e indexar por fecha de defunción\n\ndefunciones = confirmados.loc[confirmados['FECHA_DEF'].notnull()] # Seleccionamos los casos con fecha de defunción\ndefunciones = defunciones.set_index('FECHA_DEF') # indexamos por fecha de defuncióón\ndefunciones_diarios = (defunciones\n                       .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                       .size() # Calculamos el tamaño de cada grupo\n                       .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                       .rename({0:'Defunciones'}, axis=1) # Le damos nombre a la columna que obtenemos\n                       )\ndefunciones_diarios['Media Móvil'] = defunciones_diarios.rolling(window=7).mean()\ndefunciones_diarios = defunciones_diarios.melt(id_vars=['FECHA_DEF'], value_vars=['Defunciones', 'Media Móvil'])\nfig = px.line(defunciones_diarios, x='FECHA_DEF', y='value', color='variable')\nfig.show()\n\n\n                                                \n\n\n\n\n4.1.3 Combinando las dos\nLa forma más sencilla de combinar ambas gráficas es hacer un Facet Plot, es decir, prodcir dos gráficas ligadas a partir de una sóla base de datos. Para lograr esto necesitamos una estructura un poco diferente, seguimos necesitando una columna que nos distinga los conteos de sus medias móviles, pero además vamos a necesitar otra columna que nos distina el tipo de caso: casos confirmados o defunciones.\nPodemos partir de las bases que ya tenemos y simplemente cambiar algunas cosas:\n\nAgregar una columna que distinga si es Caso o defunción\nCambiar los valores en las columnas variable para que coincidan en ambas series\nCambiar los nombres de las fechas para que coincidan\nHacer una base con las dos fuentes\n\n\ndefunciones_diarios['Tipo'] = 'Defunciones'\ndefunciones_diarios.loc[defunciones_diarios['variable'] == 'Defunciones', 'variable'] = 'Conteo'\ndefunciones_diarios = defunciones_diarios.rename({'FECHA_DEF': 'Fecha'}, axis=1)\nconfirmados_diarios['Tipo'] = 'Casos Confirmados'\nconfirmados_diarios.loc[confirmados_diarios['variable'] == 'Confirmados', 'variable'] = 'Conteo'\nconfirmados_diarios = confirmados_diarios.rename({'FECHA_SINTOMAS': 'Fecha'}, axis=1)\ncasos_defunciones = defunciones_diarios.append(confirmados_diarios)\ncasos_defunciones\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2020-03-22\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      1\n      2020-03-23\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      2\n      2020-03-24\n      Conteo\n      0.000000\n      Defunciones\n    \n    \n      3\n      2020-03-25\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      4\n      2020-03-26\n      Conteo\n      3.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1387\n      2022-01-13\n      Media Móvil\n      6428.142857\n      Casos Confirmados\n    \n    \n      1388\n      2022-01-14\n      Media Móvil\n      5812.142857\n      Casos Confirmados\n    \n    \n      1389\n      2022-01-15\n      Media Móvil\n      5169.142857\n      Casos Confirmados\n    \n    \n      1390\n      2022-01-16\n      Media Móvil\n      4368.000000\n      Casos Confirmados\n    \n    \n      1391\n      2022-01-17\n      Media Móvil\n      3189.428571\n      Casos Confirmados\n    \n  \n\n2726 rows × 4 columns\n\n\n\nYa con la nueva serie como la queremos, podemos hacer un Facet, la parte importante es decirle que no queremos que compartan el eje \\(y\\) porque las escalas son muy diferentes\n\nfig = px.line(casos_defunciones, x='Fecha', y='value', color='variable', facet_col='Tipo', facet_col_wrap=1)\nfig.update_yaxes(matches=None)\nfig.show()\n\n\n                                                \n\n\n\n\n4.1.4 Hospitalizaciones\nOtra grááfica muy interesante para comprender la evolucióón de la epidemia es la de hospitalizaciones. Para obtener esta grááfica primero tenemos que seleccionar los pacientes confirmados como positivos a COVID-19 y que además fueron hospitalizados.\nLos casos confirmados ya los tenemos calculados en la variable confirmados, entonces falta ver cómo obtener los pacientes hospitalizados\n\nconfirmados.TIPO_PACIENTE.unique()\n\narray(['AMBULATORIO', 'HOSPITALIZADO'], dtype=object)\n\n\nGracias a nuentra base aplanada es muy fácil distinguirlos, entonces sólo los tenemos que seleccionar, agregar por día y podemos hacer una gráfica como las anteriores (incluyendo la media móvil). Recordemos que confirmados estáá indexado por fecha de inicio de síntomas, entonces nuestra curva de hospitalización estará indexada por la misma fecha\n\nhospitalizados = confirmados[confirmados.TIPO_PACIENTE == 'HOSPITALIZADO']\nhospitalizados_diarios = (hospitalizados\n                          .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                          .size() # Calculamos el tamaño de cada grupo\n                          .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                          .rename({0:'Hospitalizaciones'}, axis=1) # Le damos nombre a la columna que obtenemos\n                        )\nhospitalizados_diarios['Media Móvil'] = hospitalizados_diarios.rolling(window=7).mean()\nhospitalizados_diarios = hospitalizados_diarios.melt(id_vars=['FECHA_SINTOMAS'], value_vars=['Hospitalizaciones', 'Media Móvil'])\nfig = px.line(hospitalizados_diarios, x='FECHA_SINTOMAS', y='value', color='variable')\nfig.show()\n\n\n                                                \n\n\nY, una vez más, para comparar vamos a poner las tres gráficas (casos confirmados, defunciones y hospitalizacones) en un Facet\n\nhospitalizados_diarios['Tipo'] = 'Hospitalizaciones'\nhospitalizados_diarios.loc[hospitalizados_diarios['variable'] == 'Hospitalizaciones', 'variable'] = 'Conteo'\nhospitalizados_diarios = hospitalizados_diarios.rename({'FECHA_SINTOMAS': 'Fecha'}, axis=1)\ncasos_defunciones_hospitalizaciones = casos_defunciones.append(hospitalizados_diarios)\ncasos_defunciones_hospitalizaciones\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2020-03-22\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      1\n      2020-03-23\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      2\n      2020-03-24\n      Conteo\n      0.000000\n      Defunciones\n    \n    \n      3\n      2020-03-25\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      4\n      2020-03-26\n      Conteo\n      3.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1387\n      2022-01-13\n      Media Móvil\n      102.000000\n      Hospitalizaciones\n    \n    \n      1388\n      2022-01-14\n      Media Móvil\n      96.000000\n      Hospitalizaciones\n    \n    \n      1389\n      2022-01-15\n      Media Móvil\n      88.285714\n      Hospitalizaciones\n    \n    \n      1390\n      2022-01-16\n      Media Móvil\n      79.571429\n      Hospitalizaciones\n    \n    \n      1391\n      2022-01-17\n      Media Móvil\n      58.285714\n      Hospitalizaciones\n    \n  \n\n4118 rows × 4 columns\n\n\n\n\nfig = px.line(casos_defunciones_hospitalizaciones, x='Fecha', y='value', color='variable', facet_col='Tipo', facet_col_wrap=1)\nfig.update_yaxes(matches=None)\nfig.show()"
  },
  {
    "objectID": "parte_2.html",
    "href": "parte_2.html",
    "title": "Geoinformática en R",
    "section": "",
    "text": "Esta parte del libro cubre el manejo de datos espaciales utilizando R\nEN CONSTRUCCIÓN"
  },
  {
    "objectID": "parte_2/intro_R.html",
    "href": "parte_2/intro_R.html",
    "title": "5  Introducción a R",
    "section": "",
    "text": "EN CONSTRUCCIÓN"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]