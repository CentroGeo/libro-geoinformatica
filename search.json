[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geoinformática",
    "section": "",
    "text": "Prefacio\nPara nosotros en CentroGeo, la computación no es sólo una herramienta para ayudarnos a resolver diferentes problemas geoespaciales; la computación es una parte integral del proceso de análisis y una forma de pensar en geografía. Ser capaz de programar nos permite liberarnos de los algoritmos, técnicas y configuraciones que se incluyen en el software (comercial o abierto) para el análisis de datos geográficos y pensar los problemas de forma diferente. Abrir la posibilidad de automatizar los procesos de análisis no sólo hace más eficiente nuestro trabajo, sino que también nos permite pensar en los problemas de forma diferente, de forma computacional. Con el fin de ayudar a nuestros estudiantes (y a estudiantes o profesionales de otras instituciones) a adquirir las herramientas técnicas básicas para poder llevar a cabo tareras de análisis de datos geoespaciales en Python o en R hemos creado este libro.\nEste libro es (por lo pronto, pretende ser) una compilación de materiales educativos sobre Geoinformática. Busca funcionar como un apoyo para profesores interesados en impartir cursos relacionados con el uso de herramientas de programación para el análisis de datos geográficos o bien, para estudiantes independientes que busquen complementar su formación de manera autodidacta.\nEl libro y el material incluido se distribuye bajo una licencia Creative Commons, de forma que todo mundo es libre de utilizarlo y modificarlo de acuerdo a sus propiuas necesidades, siempre citando la fuente original.\nEste libro fue creado con Quarto.\nPara aprender más sobre quarto, visita: https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#organización-del-libro",
    "href": "intro.html#organización-del-libro",
    "title": "Introducción",
    "section": "Organización del libro",
    "text": "Organización del libro\nEl libro está (estará) organizado en dos grandes secciones: una dedicada a Python y otra a R. Está organizado como un conjunto de talleres. En cada taller se revisarán algunas ideas detrás del análisis de datos geoespaciales con énfasis en las herramientas y técnicas computacionales. Cada taller contiene todo el código necesario y las explicaciones básicas."
  },
  {
    "objectID": "parte_1.html",
    "href": "parte_1.html",
    "title": "Geoinformática: las herramientas básicas",
    "section": "",
    "text": "En esta parte del libro vamos a tratar de cubrir los fundamentos técnicos del procesaminto, análisis y visualización de datos geoespaciales con Python.\nPara seguir los talleres vas a necesitar varios conjuntos de datos que puedes descargar de aqui.\nEl libro está desarrollado a partir de Notebooks de Jupyter, de forma que lo más natural es que vayas siguiendo el desarrollo del libro utilizando estos notebooks.\nLa forma más sencilla de instalar Jupyter y las librerías que estaremos utilizando es utilizando el gestor de paquetes conda, que nos permite instalar fácilmente paquetes de Python sin preocuparnos por dependencias del sistema.\nExisten varis formas de instalar y trabajar con conda. Para usuarios de Windows quizá lo más sencillo sea instalar el paquete de cómputo científico Anaconda. Anaconda contiene, además del gestor de paquetes conda, muchas librerías ya preinstaladas por lo que puede resultar un poco excesivo en tamaño.\nPara trabajar de mejor forma en Python es recomendable crear environments de trabajo. Un environment es algo así como una instalación independiente de Python que contiene todo lo necesario para el desarrollo de un proyecto específico. A continuación les dejo un par de tutoriales en video para aprender a trabajar con environments de conda:\nAnaconda Beginners Guide for Linux and Windows - Python Working Environments Tutorial\nMaster the basics of Conda environments in Python\nFinalmente, conda viene configurado por defecto para utilizar los repositorios de Anaconda, Inc.. La epmpresa provee acceso a sus repositorios sin ningún costo, sin embargo en este repositorio no siempre se encuantran las versiones más actuañlizadoas y completas que vamos a necesitar. Para evitar dificultades les recomiendo utilizar los repositorios de conda-forge, acá les dejo un tutorial:\nTutorial conda-forge"
  },
  {
    "objectID": "parte_1/01_transformacion.html#conjunto-de-datos",
    "href": "parte_1/01_transformacion.html#conjunto-de-datos",
    "title": "1  Transformación de datos",
    "section": "1.1 Conjunto de Datos",
    "text": "1.1 Conjunto de Datos\nVamos a utlizar los datos del Censo de Poblacioń y Vivienda 2020 de INEGI. Trabajaremos con los datos a nivel AGEB para la Ciudad de México. Una AGEB se define como un Área Geográfica ocupada por un conjunto de manzanas perfectamente delimitadas por calles, avenidas, andadores o cualquier otro rasgo de fácil identificación en el terreno y cuyo uso de suelo es principamete habitacional, industrial, de servicios, etc.. Las AGEB’s son la unidad básica de representatividad del Marco Geoestadístico Nacional, son lo suficientemente pequeñas para representar la variabilidad espacial, pero lo suficientemente grandes para mantener la privacidad de la población y disminuir efectos de ruido estadístico.\nLos datos son publicados por INEGI en un archivo en formato csv que contiene diferentes agregaciones geográficas en el mismo archivo. Para entenderlo bien, vamos a abrirlo:\n\n\n\n\n\n\nNote\n\n\n\nEl archivo con los datos lo encuentras en la caropeta de datos del libro con el nombre conjunto_de_datos_ageb_urbana_09_cpv2020.zip\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDentro de este libro, la convención es que los datos están guardados en la carpeta datos/ relativa al notebook que se esté ejecutando.\n\n\n\ndb = pd.read_csv('datos/conjunto_de_datos_ageb_urbana_09_cpv2020.zip',\n                 dtype={'ENTIDAD': object,\n                        'MUN':object,\n                        'LOC':object,\n                        'AGEB':object})\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      0\n      09\n      Ciudad de México\n      000\n      Total de la entidad Ciudad de México\n      0000\n      Total de la entidad\n      0000\n      0\n      9209944\n      4805017\n      ...\n      1898265\n      2536523\n      2084156\n      1290811\n      957162\n      568827\n      46172\n      77272\n      561128\n      10528\n    \n    \n      1\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0000\n      Total del municipio\n      0000\n      0\n      432205\n      227255\n      ...\n      96128\n      123961\n      105899\n      66399\n      50965\n      31801\n      1661\n      2869\n      22687\n      322\n    \n    \n      2\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total de la localidad urbana\n      0000\n      0\n      432205\n      227255\n      ...\n      96128\n      123961\n      105899\n      66399\n      50965\n      31801\n      1661\n      2869\n      22687\n      322\n    \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183\n      1695\n      ...\n      741\n      772\n      692\n      313\n      221\n      145\n      8\n      14\n      148\n      5\n    \n    \n      4\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Azcapotzalco\n      0010\n      1\n      159\n      86\n      ...\n      45\n      42\n      39\n      18\n      13\n      6\n      *\n      0\n      9\n      0\n    \n  \n\n5 rows × 230 columns\n\n\n\nLa librería Pandas es la que provee la funcionalidad para trabajar con datos tabulares en Python. La estructura fundamental de Pandas es el DataFrame, podemos pensar en los DataFrames como hojas de Excel, con columnas nombradas que funcionan como indices para las variables y filas para las observaciones.\nPara leer el archivo utilizamos el método read_csv() de los DataFrames de Pandas. El parámetro dtype que le pasamos a la función nos asegura que ciertas columnas se lean con un tipo de datos especial, en este caso como object, para asegurarnos que no se lean como números y perdamos identificadores, vamos a regresar a esto más adelante.\nLa columna que nos interesa ahorita es NOM_LOC, esta nos ayuda a distinguiir los datos que vienen en cada fila: las filas etiquetadas con Total AGEB urbana contienen los conteos para cada AGEB de todas las variables, entonces, nuestra primera tarea es filtrar la base y quedarnos sólo con las columnas que en la columna NOM_LOC dice Total AGEB urbana.\n\ndb = db.loc[db['NOM_LOC'] == 'Total AGEB urbana']\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183\n      1695\n      ...\n      741\n      772\n      692\n      313\n      221\n      145\n      8\n      14\n      148\n      5\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593\n      2915\n      ...\n      1373\n      1510\n      1203\n      478\n      349\n      238\n      28\n      68\n      393\n      14\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235\n      2232\n      ...\n      965\n      1049\n      878\n      361\n      339\n      247\n      5\n      12\n      250\n      *\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768\n      2551\n      ...\n      1124\n      1237\n      1076\n      481\n      452\n      294\n      10\n      17\n      254\n      *\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176\n      1115\n      ...\n      517\n      562\n      507\n      276\n      260\n      153\n      4\n      3\n      70\n      0\n    \n  \n\n5 rows × 230 columns\n\n\n\nLo que hicimos aquí fue utlizar el selector loc de pandas para seleccionar las filas que queremos, pasándole el filtro que nos interesa, en este caso db['NOM_LOC'] == 'Total AGEB urbana'"
  },
  {
    "objectID": "parte_1/01_transformacion.html#limpieza-de-los-datos",
    "href": "parte_1/01_transformacion.html#limpieza-de-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.2 Limpieza de los datos",
    "text": "1.2 Limpieza de los datos\nHasta aquí lo que tenemos es un DataFrame con todas las variables del censo agregadas por AGEB. Ahora, para poder realizar análisis a partir de esta base de datos, necesitamos asegurarnos de que los datos son del tipo correcto, es decir, si vamos a hacer cuentas, los datos deben ser de tipo float o int. Utlicemos entonces la propiedad db.dtypes para preguntar los tipos de datos.\n\ndb.dtypes\n\nENTIDAD        object\nNOM_ENT        object\nMUN            object\nNOM_MUN        object\nLOC            object\n                ...  \nVPH_CVJ        object\nVPH_SINRTV     object\nVPH_SINLTC     object\nVPH_SINCINT    object\nVPH_SINTIC     object\nLength: 230, dtype: object\n\n\nComo podemos ver, no sólo las columnas que pedimos que leyera como object las leyó así, también las demás columnas. Esto se puede deber a que tienen codificados valores faltantes con caracteres especiales, por lo que pandas no pudo convertirlos automáticamente en números.\nPara entender esto un poco mejor, vamos a leer el diccionario de datos del censo.\n\n\n\n\n\n\nNote\n\n\n\nTambién pueden explorar el archivo en excel, para verlo con más calma\n\n\n\ndiccionario = pd.read_csv('datos/diccionario_datos_ageb_urbana_09_cpv2020.csv', skiprows=3)\ndiccionario\n\n\n\n\n\n  \n    \n      \n      Núm.\n      Indicador\n      Descripción\n      Mnemónico\n      Rangos\n      Longitud\n    \n  \n  \n    \n      0\n      1\n      Clave de entidad federativa\n      Código que identifica a la entidad federativa....\n      ENTIDAD\n      00…32\n      2\n    \n    \n      1\n      2\n      Entidad federativa\n      Nombre oficial de la entidad federativa.\n      NOM_ENT\n      Alfanumérico\n      50\n    \n    \n      2\n      3\n      Clave de municipio o demarcación territorial\n      Código que identifica al municipio o demarcaci...\n      MUN\n      000…570\n      3\n    \n    \n      3\n      4\n      Municipio o demarcación territorial\n      Nombre oficial del municipio o demarcación ter...\n      NOM_MUN\n      Alfanumérico\n      50\n    \n    \n      4\n      5\n      Clave de localidad\n      Código que identifica a la localidad al interi...\n      LOC\n      0000…9999\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      225\n      218\n      Viviendas particulares habitadas que disponen ...\n      Viviendas particulares habitadas que tienen co...\n      VPH_CVJ\n      0…999999999\n      9\n    \n    \n      226\n      219\n      Viviendas particulares habitadas sin radio ni ...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINRTV\n      0…999999999\n      9\n    \n    \n      227\n      220\n      Viviendas particulares habitadas sin línea tel...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINLTC\n      0…999999999\n      9\n    \n    \n      228\n      221\n      Viviendas particulares habitadas sin computado...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINCINT\n      0…999999999\n      9\n    \n    \n      229\n      222\n      Viviendas particulares habitadas sin tecnologí...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINTIC\n      0…999999999\n      9\n    \n  \n\n230 rows × 6 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFíjense como pasamos skiprows=3 para leer el diccionario del censo. Esto le dice a pandas que el header (los nombres de las columnas), vienen en el cuarto renglón.\n\n\nA partir de este diccionario podemos ver que hay varias formas de codificar valores faltantes: ‘999999999’, ‘99999999’, ’*’ y ‘N/D’.\nPara poder convertir todas estas columnas en numéricas tenemos que reemplazar todos esos valores por la forma en la que se expresan los datos faltantes en Pandas, utilizando el valor Not a Number de numpy. Para hacer este reemplazo vamos a usar la función replace de Pandas, que toma como argumento el valor que queremos reemplazar y el valor por el cual lo queremos reemplazar:\n\ndb = (db  \n      .replace('999999999', np.nan)\n      .replace('99999999', np.nan)\n      .replace('*', np.nan)\n      .replace('N/D', np.nan))\n\n¡Esta fue una instrucción complicada!\nPero no es realmente difícil. Como hemos visto hasta aquí, los métodos de los DataFrames en general regresan otros DataFrames con el resultado de la operación, esto nos permite encadenar métodos, de forma que cuando hacemos db..replace('999999999', np.nan)..replace('99999999', np.nan), el segundo replace opera sobre el resultado del primero y así sucesivamente. Este encadenamiento de métodos nos ayuda a escribir código más fácil de leer.\nAhora ya tenemos todos los valores faltantes codificados adecuadamente, sin embargo aún nos falta convertirlos a números ¿verdad?\n\ndb.dtypes\n\nENTIDAD        object\nNOM_ENT        object\nMUN            object\nNOM_MUN        object\nLOC            object\n                ...  \nVPH_CVJ        object\nVPH_SINRTV     object\nVPH_SINLTC     object\nVPH_SINCINT    object\nVPH_SINTIC     object\nLength: 230, dtype: object\n\n\nLa forma normal de cambiar el tipo de datos de una columna es utilizar el método astype\n\ndb['VPH_CVJ'].astype('float').dtypes\n\ndtype('float64')\n\n\n\n\n\n\n\n\nNote\n\n\n\nAquí no estamos asignando el resultado de la operación a ninguna variable, el resultado de esta operación no modifica el valor de los datos.\n\n\nAsí podríamos ir cambiando columna por columna, pero como estamos programando ¡nos gusta hacer las cosas en bruto!\nEn el diccionario de datos tenemos los nombres de todas las variables, entonces podemos utilizar estos nombres para seleccionar todas las columnas que contienen datos numéricos y cambiar su tipo en el DataFrame. Fíjense que las primeras 8 filas del diccionario contienen los identificadores geográficos:\n\ndiccionario.head(8)\n\n\n\n\n\n  \n    \n      \n      Núm.\n      Indicador\n      Descripción\n      Mnemónico\n      Rangos\n      Longitud\n    \n  \n  \n    \n      0\n      1\n      Clave de entidad federativa\n      Código que identifica a la entidad federativa....\n      ENTIDAD\n      00…32\n      2\n    \n    \n      1\n      2\n      Entidad federativa\n      Nombre oficial de la entidad federativa.\n      NOM_ENT\n      Alfanumérico\n      50\n    \n    \n      2\n      3\n      Clave de municipio o demarcación territorial\n      Código que identifica al municipio o demarcaci...\n      MUN\n      000…570\n      3\n    \n    \n      3\n      4\n      Municipio o demarcación territorial\n      Nombre oficial del municipio o demarcación ter...\n      NOM_MUN\n      Alfanumérico\n      50\n    \n    \n      4\n      5\n      Clave de localidad\n      Código que identifica a la localidad al interi...\n      LOC\n      0000…9999\n      4\n    \n    \n      5\n      6\n      Localidad\n      Nombre con el que se reconoce a la localidad d...\n      NOM_LOC\n      Alfanumérico\n      70\n    \n    \n      6\n      7\n      Clave del AGEB\n      Clave que identifica al AGEB urbana, al interi...\n      AGEB\n      001...999; 0...9 o A-P\n      4\n    \n    \n      7\n      8\n      Clave de manzana\n      Clave que identifica a la manzana, al interior...\n      MZA\n      001...999\n      3\n    \n  \n\n\n\n\nLas demás filas contienen los nombres (y descripciones) de las variables del Censo.\n\ncampos_datos = diccionario.loc[8:,]['Mnemónico']\ncampos_datos\n\n8           POBTOT\n9           POBFEM\n10          POBMAS\n11           P_0A2\n12         P_0A2_F\n          ...     \n225        VPH_CVJ\n226     VPH_SINRTV\n227     VPH_SINLTC\n228    VPH_SINCINT\n229     VPH_SINTIC\nName: Mnemónico, Length: 222, dtype: object\n\n\nAquí utilizamos una vez más el método loc para seleccionar filas en nuestros datos. En esta ocasión seleccionamos las filas por índice (en este momento nuestro índice es simplemente el número de fila, más adelante usaremos índices diferentes), la selección loc[8:,] simplemente quiere decir todas las columnas para las filas de la 9 en adelante.\nTambién estamos seleccionando una única columna al hacer ['Mnemónico'], el resultado de esta selección ya no es un DataFrame, es una Serie. Las series son las estructuras que usa Pandas para guardar una sóla columna (o fila).\nLas Series se pueden utilizar (igual que las listas) para seleccionar columnas de un DataFrame, entoinces, ahora sí podemos cambiar todos los tipos de datos de una sola vez.\n\ndb[campos_datos] = db[campos_datos].astype('float')\ndb.dtypes\n\nENTIDAD         object\nNOM_ENT         object\nMUN             object\nNOM_MUN         object\nLOC             object\n                ...   \nVPH_CVJ        float64\nVPH_SINRTV     float64\nVPH_SINLTC     float64\nVPH_SINCINT    float64\nVPH_SINTIC     float64\nLength: 230, dtype: object"
  },
  {
    "objectID": "parte_1/01_transformacion.html#descripciones-de-los-datos",
    "href": "parte_1/01_transformacion.html#descripciones-de-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.3 Descripciones de los datos",
    "text": "1.3 Descripciones de los datos\nPandas nos provee una serie de métodos para obtener descripciones generales de la tabla. Podemos usar el método info para obtener una descripción general de la estructura de la tabla y el espacio que ocupa en la memoria:\n\ndb.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2433 entries, 3 to 68915\nColumns: 230 entries, ENTIDAD to VPH_SINTIC\ndtypes: float64(222), int64(1), object(7)\nmemory usage: 4.3+ MB\n\n\nPara obtener las estadísticas descriptivas podemos usar el método describe:\n\ndb.describe()\n\n\n\n\n\n  \n    \n      \n      MZA\n      POBTOT\n      POBFEM\n      POBMAS\n      P_0A2\n      P_0A2_F\n      P_0A2_M\n      P_3YMAS\n      P_3YMAS_F\n      P_3YMAS_M\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      count\n      2433.0\n      2433.000000\n      2422.000000\n      2423.00000\n      2406.000000\n      2392.000000\n      2390.000000\n      2423.000000\n      2422.000000\n      2423.000000\n      ...\n      2416.000000\n      2420.000000\n      2418.000000\n      2415.000000\n      2415.000000\n      2410.000000\n      2251.000000\n      2235.000000\n      2405.000000\n      1801.000000\n    \n    \n      mean\n      0.0\n      3758.993835\n      1970.647812\n      1804.64837\n      109.901912\n      54.471990\n      56.089121\n      3661.372678\n      1914.832370\n      1747.329757\n      ...\n      783.982616\n      1041.995455\n      859.506617\n      533.200000\n      395.840580\n      235.558506\n      20.068858\n      33.803579\n      229.281081\n      5.181011\n    \n    \n      std\n      0.0\n      2433.068753\n      1254.533102\n      1186.95856\n      85.636899\n      42.286817\n      43.908616\n      2347.050678\n      1215.700184\n      1147.281855\n      ...\n      525.413812\n      690.331581\n      601.110222\n      426.577764\n      390.905691\n      204.624708\n      16.611861\n      30.598161\n      191.422212\n      6.154989\n    \n    \n      min\n      0.0\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.0\n      2045.000000\n      1083.500000\n      974.00000\n      46.250000\n      23.000000\n      24.000000\n      2018.000000\n      1053.000000\n      942.500000\n      ...\n      456.750000\n      590.000000\n      488.000000\n      271.000000\n      173.000000\n      118.250000\n      8.000000\n      10.000000\n      79.000000\n      0.000000\n    \n    \n      50%\n      0.0\n      3396.000000\n      1783.500000\n      1616.00000\n      91.000000\n      45.000000\n      46.000000\n      3304.000000\n      1730.500000\n      1566.000000\n      ...\n      698.500000\n      921.500000\n      749.000000\n      442.000000\n      300.000000\n      189.000000\n      16.000000\n      25.000000\n      185.000000\n      4.000000\n    \n    \n      75%\n      0.0\n      4992.000000\n      2617.500000\n      2391.00000\n      152.000000\n      75.000000\n      77.000000\n      4852.000000\n      2539.000000\n      2315.000000\n      ...\n      992.500000\n      1348.250000\n      1083.000000\n      671.000000\n      476.000000\n      288.750000\n      27.000000\n      50.000000\n      336.000000\n      7.000000\n    \n    \n      max\n      0.0\n      21198.000000\n      11128.000000\n      10616.00000\n      709.000000\n      350.000000\n      393.000000\n      20530.000000\n      10774.000000\n      10551.000000\n      ...\n      6196.000000\n      7867.000000\n      7512.000000\n      5717.000000\n      5903.000000\n      3056.000000\n      149.000000\n      290.000000\n      1488.000000\n      66.000000\n    \n  \n\n8 rows × 223 columns"
  },
  {
    "objectID": "parte_1/01_transformacion.html#creación-de-variables",
    "href": "parte_1/01_transformacion.html#creación-de-variables",
    "title": "1  Transformación de datos",
    "section": "1.4 Creación de variables",
    "text": "1.4 Creación de variables\nMuchas veces vamos a querer crear nuevas columnas a partir de las ya existentes. Por ejemplo, podemos estar interesados en el porcentaje de población femenina en cada AGEB.\n\npct_fem = db['POBFEM'] / db['POBTOT']\npct_fem.head()\n\n3      0.532516\n30     0.521187\n82     0.527037\n116    0.535025\n163    0.512408\ndtype: float64\n\n\nFíjense cómo usamos / para dividir dos columnas. El resultado de la operación lo guardamos en la variable pct_fem ¿De qué tipo será esta variable?\n\npct_fem.info()\n\n<class 'pandas.core.series.Series'>\nInt64Index: 2433 entries, 3 to 68915\nSeries name: None\nNon-Null Count  Dtype  \n--------------  -----  \n2405 non-null   float64\ndtypes: float64(1)\nmemory usage: 38.0 KB\n\n\nEs una serie, es decir una columna en nuestro caso. Como esta columna comparte el mismo índice que los datos originales (es resultado de una operación renglón por renglón), entonces la podemos agregar al DataFrame original facilmente:\n\ndb['pct_fem'] = pct_fem\ndb['pct_fem'].head()\n\n/tmp/ipykernel_5237/2610780181.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  db['pct_fem'] = pct_fem\n\n\n3      0.532516\n30     0.521187\n82     0.527037\n116    0.535025\n163    0.512408\nName: pct_fem, dtype: float64\n\n\n\n1.4.1 Modificar valores\nDe la misma forma que podemos agregar columnas (o filas) a nuestro DataFrame, podemos también modificar los valores existentes. Para explorar esto, vamos a crear una nueva columna y llenarla con valores nulos:\n\n# Nueva columna llena de sólamente el número 1\ndb['Nueva'] = None\ndb['Nueva'].head()\n\n/tmp/ipykernel_5237/463547730.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  db['Nueva'] = None\n\n\n3      None\n30     None\n82     None\n116    None\n163    None\nName: Nueva, dtype: object\n\n\nPodemos fácilmente cambiar los valores de todas las filas:\n\ndb['Nueva'] = 1\ndb['Nueva'].head()\n\n3      1\n30     1\n82     1\n116    1\n163    1\nName: Nueva, dtype: int64\n\n\nO también cambiar el valor sólo para una fila específica:\n\ndb.loc[3, 'Nueva'] = 10\ndb['Nueva'].head()\n\n3      10\n30      1\n82      1\n116     1\n163     1\nName: Nueva, dtype: int64\n\n\n\n\n1.4.2 Eliminar columnas\nEliminar columnas es igualmente fácil usando el método drop:\n\ndb = db.drop(columns=['Nueva'])\n'Nueva' in db.columns\n\nFalse\n\n\n¡Fíjense como preguntamos al final si ya habíamos eliminado la columna!\n\n\n1.4.3 Buscando datos\nMuchas veces queremos encontrar observaciones que cumplan con uno o más criterios. Una vez más, el método loc es nuestro amigop para seleccionar datos. Supongamos que queremos encontrar aquelas AGEBs que tengan una población de ‘65 años o más’ mayor a 1,000 personas.\n\ndb_seleccion = db.loc[db['POB65_MAS'] > 1000, :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      444\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0186\n      0\n      11139.0\n      5776.0\n      ...\n      3299.0\n      2878.0\n      1731.0\n      1407.0\n      994.0\n      54.0\n      47.0\n      470.0\n      4.0\n      0.518538\n    \n    \n      3617\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0107\n      0\n      6992.0\n      3673.0\n      ...\n      2205.0\n      2022.0\n      1478.0\n      1117.0\n      650.0\n      21.0\n      26.0\n      256.0\n      4.0\n      0.525315\n    \n    \n      4075\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0287\n      0\n      8213.0\n      4526.0\n      ...\n      2373.0\n      2226.0\n      1503.0\n      1309.0\n      616.0\n      43.0\n      40.0\n      241.0\n      6.0\n      0.551078\n    \n    \n      4886\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0573\n      0\n      12827.0\n      6653.0\n      ...\n      3437.0\n      2878.0\n      1727.0\n      1409.0\n      863.0\n      59.0\n      82.0\n      669.0\n      6.0\n      0.518672\n    \n  \n\n5 rows × 231 columns\n\n\n\nSimplemente pasamos la condición que nos interesa al selector y listo.\nLos criterios de búsquera pueden ser tan sofisticados como se requiera, por ejemplo, podemos seleccionar los AGEBs en los cuales la población de 0 a 14 años sea menor a un cuarto de la población total:\n\ndb_seleccion = db.loc[(db['POB0_14'] / db['POBTOT']) < 0.25, :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183.0\n      1695.0\n      ...\n      772.0\n      692.0\n      313.0\n      221.0\n      145.0\n      8.0\n      14.0\n      148.0\n      5.0\n      0.532516\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235.0\n      2232.0\n      ...\n      1049.0\n      878.0\n      361.0\n      339.0\n      247.0\n      5.0\n      12.0\n      250.0\n      NaN\n      0.527037\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768.0\n      2551.0\n      ...\n      1237.0\n      1076.0\n      481.0\n      452.0\n      294.0\n      10.0\n      17.0\n      254.0\n      NaN\n      0.535025\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176.0\n      1115.0\n      ...\n      562.0\n      507.0\n      276.0\n      260.0\n      153.0\n      4.0\n      3.0\n      70.0\n      0.0\n      0.512408\n    \n  \n\n5 rows × 231 columns\n\n\n\nPodemos hacer combinaciones arbitrarias de selectores utilizando los operadores lógicos & (and) y | (or). Por ejemplo, podemos combinar nuestras selecciones anteriores para encontrar las AGEBs con menos de 50% de mujeres y población de 0 a 14 años sea menor a un cuarto de la población total\n\ndb_seleccion = db.loc[(db['pct_fem'] < 0.5) & \n                      ((db['POB0_14'] / db['POBTOT']) < 0.25), :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      2342\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0877\n      0\n      821.0\n      403.0\n      ...\n      174.0\n      135.0\n      56.0\n      44.0\n      34.0\n      3.0\n      23.0\n      70.0\n      NaN\n      0.490865\n    \n    \n      3292\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      1165\n      0\n      400.0\n      199.0\n      ...\n      91.0\n      51.0\n      26.0\n      26.0\n      15.0\n      3.0\n      8.0\n      49.0\n      NaN\n      0.497500\n    \n    \n      5321\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0770\n      0\n      326.0\n      160.0\n      ...\n      137.0\n      136.0\n      88.0\n      96.0\n      50.0\n      0.0\n      0.0\n      4.0\n      0.0\n      0.490798\n    \n    \n      6016\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1092\n      0\n      5787.0\n      2887.0\n      ...\n      1792.0\n      1744.0\n      1363.0\n      1206.0\n      614.0\n      11.0\n      5.0\n      55.0\n      NaN\n      0.498877\n    \n    \n      7919\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1660\n      0\n      3328.0\n      1653.0\n      ...\n      895.0\n      823.0\n      552.0\n      379.0\n      259.0\n      10.0\n      10.0\n      88.0\n      0.0\n      0.496695\n    \n  \n\n5 rows × 231 columns"
  },
  {
    "objectID": "parte_1/01_transformacion.html#ordenar-valores",
    "href": "parte_1/01_transformacion.html#ordenar-valores",
    "title": "1  Transformación de datos",
    "section": "1.5 Ordenar valores",
    "text": "1.5 Ordenar valores\nFinalmente, vamos a ver cómo ordenar los datos de acuerdo a los valores de un campo. Pensemos que queremos ver las 10 AGEBS más pobladas de la ciudad.\n\ndb.sort_values('POBTOT', ascending = False).head(10)\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      39932\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      Total AGEB urbana\n      0135\n      0\n      21198.0\n      11128.0\n      ...\n      7867.0\n      7512.0\n      5573.0\n      5568.0\n      3056.0\n      60.0\n      39.0\n      346.0\n      3.0\n      0.524955\n    \n    \n      63316\n      09\n      Ciudad de México\n      016\n      Miguel Hidalgo\n      0001\n      Total AGEB urbana\n      0444\n      0\n      18174.0\n      8931.0\n      ...\n      7294.0\n      7187.0\n      5717.0\n      5903.0\n      2640.0\n      149.0\n      9.0\n      144.0\n      NaN\n      0.491416\n    \n    \n      65102\n      09\n      Ciudad de México\n      016\n      Miguel Hidalgo\n      0001\n      Total AGEB urbana\n      1349\n      0\n      15549.0\n      8211.0\n      ...\n      4279.0\n      3756.0\n      2213.0\n      1482.0\n      944.0\n      70.0\n      208.0\n      831.0\n      21.0\n      0.528073\n    \n    \n      9394\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0020\n      Total AGEB urbana\n      0316\n      0\n      15087.0\n      7701.0\n      ...\n      3434.0\n      2289.0\n      1277.0\n      738.0\n      512.0\n      110.0\n      205.0\n      1301.0\n      23.0\n      0.510439\n    \n    \n      9090\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0001\n      Total AGEB urbana\n      0369\n      0\n      14609.0\n      7459.0\n      ...\n      5989.0\n      5970.0\n      5155.0\n      4826.0\n      2486.0\n      23.0\n      9.0\n      19.0\n      0.0\n      0.510576\n    \n    \n      9190\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0001\n      Total AGEB urbana\n      0373\n      0\n      14170.0\n      7457.0\n      ...\n      3293.0\n      2650.0\n      1743.0\n      1430.0\n      889.0\n      59.0\n      130.0\n      782.0\n      13.0\n      0.526253\n    \n    \n      6211\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1162\n      0\n      14061.0\n      7267.0\n      ...\n      3416.0\n      2780.0\n      1316.0\n      971.0\n      689.0\n      67.0\n      120.0\n      837.0\n      23.0\n      0.516820\n    \n    \n      52537\n      09\n      Ciudad de México\n      012\n      Tlalpan\n      0001\n      Total AGEB urbana\n      2121\n      0\n      13974.0\n      7345.0\n      ...\n      3954.0\n      3518.0\n      2477.0\n      2100.0\n      1259.0\n      34.0\n      61.0\n      476.0\n      8.0\n      0.525619\n    \n    \n      26177\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      Total AGEB urbana\n      1994\n      0\n      13946.0\n      3330.0\n      ...\n      1069.0\n      767.0\n      327.0\n      231.0\n      180.0\n      18.0\n      25.0\n      329.0\n      4.0\n      0.238778\n    \n    \n      42074\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      Total AGEB urbana\n      1171\n      0\n      13918.0\n      7438.0\n      ...\n      4387.0\n      3856.0\n      2895.0\n      2351.0\n      1244.0\n      51.0\n      96.0\n      655.0\n      12.0\n      0.534416\n    \n  \n\n10 rows × 231 columns\n\n\n\nEl método sort_values nos permite ordenar los datos de acuerdo al valor (o criterio) que queramos. El argumento ascending = False indica que los queremos ordenar de forma descendente."
  },
  {
    "objectID": "parte_1/01_transformacion.html#exploración-visual",
    "href": "parte_1/01_transformacion.html#exploración-visual",
    "title": "1  Transformación de datos",
    "section": "1.6 Exploración Visual",
    "text": "1.6 Exploración Visual\nYa que nos empezamos a familiarizar con el manejo de datos usando Pandas, podemos empezar a hacer cosas más divertidas, por ejemplo, explorar visualmente los datos.\nLa librería seaborn nos ofrece una serie de herramientas para la exploración visual de los datos. Podemos comenzar con un histograma para ver la distribución de los valores de una columna.\n\n_ = sns.histplot(db['POBTOT'], kde = False)\n\n\n\n\nLa función histplot de seaborn nos regresa el histograma, el argumento kde=False le dice que no queremos que ajuste una distribución empírica.\n\n\n\n\n\n\nNote\n\n\n\nCuando hicimos _ = sns.histplot(db['POBTOT'], kde = False) estamos asignando el resultado a la variable _, esto se hace comunmente cuando no queremos ya hacer nada más con ese resultado. Más adelante haremos operaciones sobre las gráficas.\n\n\n\n1.6.0.1 Densidad de Kernel\nOtra forma de representar la distribución de una variable es ajustando una densidad de kernel, que estima una distribución (empírica) de probabilidad a partir de nuestras observaciones.\n\n_ = sns.kdeplot(db['POBTOT'], fill = True)\n\n\n\n\nOtra visualización muy útil es la de la distribución conjunta de dos variables. Por ejemplo, supongamos que queremos comparar las distribuciones de la población masculina y femenina.\n\n_ = sns.jointplot(data=db, x='POBFEM', y='POBMAS')\n\n\n\n\nLa relación, como es de esperarse, es casi perfectamente lineal, pero ver las distribuciones conjuntas nos permite identificar algunas AGEBS con poblaciones masculinas desproporcionadamente grandes ¿Qué serán?.\nMuchas veces queremos visualizar la distribución conjunta de varias variables al mismo tiempo. Por ejemplo cuando queremos hacer ejercicios de regresión queremos explorar la correlación entre las covariables. Una forma de visualizar rápidamente estas distribuciones conjuntas es con un PairGrid. Utlicemos uno sencillo para ver las distribuciones de algunas variables.\n\nvars = ['P_0A2', 'P_15A17', 'PNACOE', 'P3YM_HLI']\ng = sns.PairGrid(db[vars])\ng = g.map(sns.scatterplot)\n\n\n\n\nLa función PairPlot sólo nos prepara la malla (un cuadrado del número de variables de los datos) y con el map llenamos esa malla con la gráfica que queramos, en nuestro caso un diagrama de dispersión.\nEn este caso la diagonal no es muy informativa, es un diagrama de dispersión de una variable consigo misma. PairPlot es muy flexible y nos permite mapear diferentes funciones para la diagonal y los demás elementos, por ejemplo:\n\ng = sns.PairGrid(db[vars])\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n\n<seaborn.axisgrid.PairGrid at 0x7f70a19c4040>"
  },
  {
    "objectID": "parte_1/01_transformacion.html#organizando-los-datos",
    "href": "parte_1/01_transformacion.html#organizando-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.7 Organizando los datos",
    "text": "1.7 Organizando los datos\nMuchos flujos de análisis requieren organizar los datos en una estructura particular conocida como Tidy Data (algo así como datos ordenados). La idea es tener una estructura estandarizada con principios comunes de manipulación que sirva como entrada a diferentes tipos de análisis.\nLas tres características fundamentales de un conjunto de datos bien ordenado de acuerdo a los principios tidy son:\n\nCada variable en una columna\nCada observación en una fila\nCada unidad de observación en una tabla\n\nPara mayor información sobre el concepto de Tidy Data, puede consultarse el Artículo Académico original (de Acceso Libre), así como el Repositorio Púlico asociado a él.\nTratemos de aplicar el concepto de Tidy Data a los datos de la práctica. Primero, recordando su estructura:\n\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183.0\n      1695.0\n      ...\n      772.0\n      692.0\n      313.0\n      221.0\n      145.0\n      8.0\n      14.0\n      148.0\n      5.0\n      0.532516\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235.0\n      2232.0\n      ...\n      1049.0\n      878.0\n      361.0\n      339.0\n      247.0\n      5.0\n      12.0\n      250.0\n      NaN\n      0.527037\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768.0\n      2551.0\n      ...\n      1237.0\n      1076.0\n      481.0\n      452.0\n      294.0\n      10.0\n      17.0\n      254.0\n      NaN\n      0.535025\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176.0\n      1115.0\n      ...\n      562.0\n      507.0\n      276.0\n      260.0\n      153.0\n      4.0\n      3.0\n      70.0\n      0.0\n      0.512408\n    \n  \n\n5 rows × 231 columns\n\n\n\nEsta base de datos no cumple con las características tidy. En efecto, tenemos las variables en columnas (sin contar los identificadores), pero:\n\nTenemos dos tipos de unidades: personas y viviendas. El principio tidy nos indica que necesitamos dos tablas para representar los datos.\nPara cada tipoi de unidad tenemos en la misma fila tantas observaciones como variables (del mismo tipo). Por ejemplo, el valor de la población para cada grupo de edad en cada AGEB es una observación.\n\nEntonces, vamos a trabajar en acomodar la tabla a los principios tidy. Para comenzar, trabajemos sólo con las variables que representan segmentos de edad de la población. Seleccionar sólo estas columnas puede ser engorroso, pero si nos fijamos en el diccionario, podemos observar que todas las variables que nos interesan empiezan con ‘P_’ Podemos usar esta observación para seleccionar, a partir de la lista de columnas, sólo las que nos interesan:\n\ncols_pob = [c for c in db.columns if c.startswith('P_')]\nprint(cols_pob)               \n\n['P_0A2', 'P_0A2_F', 'P_0A2_M', 'P_3YMAS', 'P_3YMAS_F', 'P_3YMAS_M', 'P_5YMAS', 'P_5YMAS_F', 'P_5YMAS_M', 'P_12YMAS', 'P_12YMAS_F', 'P_12YMAS_M', 'P_15YMAS', 'P_15YMAS_F', 'P_15YMAS_M', 'P_18YMAS', 'P_18YMAS_F', 'P_18YMAS_M', 'P_3A5', 'P_3A5_F', 'P_3A5_M', 'P_6A11', 'P_6A11_F', 'P_6A11_M', 'P_8A14', 'P_8A14_F', 'P_8A14_M', 'P_12A14', 'P_12A14_F', 'P_12A14_M', 'P_15A17', 'P_15A17_F', 'P_15A17_M', 'P_18A24', 'P_18A24_F', 'P_18A24_M', 'P_15A49_F', 'P_60YMAS', 'P_60YMAS_F', 'P_60YMAS_M']\n\n\nAhora, vamos a construir un identificador único de AGEB para cada fila concatenando los identificadores de entidad, municipio, localidad y ageb:\n\ndb['AGEB_cvgeo'] = db['ENTIDAD'] + db['MUN'] + db['LOC'] + db['AGEB']\ndb['AGEB_cvgeo'].head()\n\n3      0900200010010\n30     0900200010025\n82     090020001003A\n116    0900200010044\n163    0900200010097\nName: AGEB_cvgeo, dtype: object\n\n\nYa con este identificador, podemos eliminar de la tabla los identificadores que usamos para construirlo\n\ndb = db.drop(columns=['ENTIDAD', 'MUN', 'LOC', 'AGEB'])\n\nCopiamos las columnas que nos interesan a una nueva tabla\n\nrangos = db[['AGEB_cvgeo'] + cols_pob]\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      P_0A2\n      P_0A2_F\n      P_0A2_M\n      P_3YMAS\n      P_3YMAS_F\n      P_3YMAS_M\n      P_5YMAS\n      P_5YMAS_F\n      P_5YMAS_M\n      ...\n      P_15A17\n      P_15A17_F\n      P_15A17_M\n      P_18A24\n      P_18A24_F\n      P_18A24_M\n      P_15A49_F\n      P_60YMAS\n      P_60YMAS_F\n      P_60YMAS_M\n    \n  \n  \n    \n      3\n      0900200010010\n      60.0\n      32.0\n      28.0\n      3123.0\n      1663.0\n      1460.0\n      3074.0\n      1639.0\n      1435.0\n      ...\n      111.0\n      61.0\n      50.0\n      303.0\n      149.0\n      154.0\n      726.0\n      816.0\n      470.0\n      346.0\n    \n    \n      30\n      0900200010025\n      122.0\n      58.0\n      64.0\n      5470.0\n      2856.0\n      2614.0\n      5363.0\n      2805.0\n      2558.0\n      ...\n      214.0\n      97.0\n      117.0\n      521.0\n      263.0\n      258.0\n      1436.0\n      1293.0\n      732.0\n      561.0\n    \n    \n      82\n      090020001003A\n      88.0\n      49.0\n      39.0\n      4147.0\n      2183.0\n      1964.0\n      4065.0\n      2138.0\n      1927.0\n      ...\n      180.0\n      74.0\n      106.0\n      425.0\n      226.0\n      199.0\n      1067.0\n      931.0\n      546.0\n      385.0\n    \n    \n      116\n      0900200010044\n      110.0\n      49.0\n      61.0\n      4658.0\n      2502.0\n      2156.0\n      4560.0\n      2445.0\n      2115.0\n      ...\n      175.0\n      87.0\n      88.0\n      487.0\n      241.0\n      246.0\n      1215.0\n      1132.0\n      672.0\n      460.0\n    \n    \n      163\n      0900200010097\n      40.0\n      16.0\n      24.0\n      2136.0\n      1099.0\n      1037.0\n      2100.0\n      1076.0\n      1024.0\n      ...\n      90.0\n      45.0\n      45.0\n      204.0\n      96.0\n      108.0\n      508.0\n      562.0\n      311.0\n      251.0\n    \n  \n\n5 rows × 41 columns\n\n\n\nAhora vamos a reorganizar la tabla de forma que cada grupo de edad corresponda a una fila en lugar de una columna, de esta forma tenemos las observaciones en filas, de acuerdo al principio tidy.\nPara lograr esto lo que tenemos que hacer es la operación inversa de un pivote, es decir, un stack. El método stack hace justo lo que necesitamos, sólo tenemos que especificar el índice (lo que distingue a cada observación) que queremos utilizar para cada fila, en este caso AGEB_cvgeo.\n\nrangos = rangos.set_index('AGEB_cvgeo').stack()\nrangos\n\nAGEB_cvgeo               \n0900200010010  P_0A2           60.0\n               P_0A2_F         32.0\n               P_0A2_M         28.0\n               P_3YMAS       3123.0\n               P_3YMAS_F     1663.0\n                              ...  \n0901700011524  P_18A24_M      230.0\n               P_15A49_F     1111.0\n               P_60YMAS       706.0\n               P_60YMAS_F     394.0\n               P_60YMAS_M     312.0\nLength: 96555, dtype: float64\n\n\nPerfecto, eso se parece bastante a lo que buscamos, sólo que en lugar de un DataFrame lo que tenemos es una Serie. Fíjense que para cada valor del índice (AGEB_cvgeo), tenemos todos los valores de los grupos de población.\nPara convertir esto en un DataFrame lo más sencillo es quitar el índice que creamos con la función reset_index:\n\nrangos = rangos.reset_index()\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      level_1\n      0\n    \n  \n  \n    \n      0\n      0900200010010\n      P_0A2\n      60.0\n    \n    \n      1\n      0900200010010\n      P_0A2_F\n      32.0\n    \n    \n      2\n      0900200010010\n      P_0A2_M\n      28.0\n    \n    \n      3\n      0900200010010\n      P_3YMAS\n      3123.0\n    \n    \n      4\n      0900200010010\n      P_3YMAS_F\n      1663.0\n    \n  \n\n\n\n\nAhora tenemos un DataFrame en el que el valor de la columna AGEB_cvgeo viene repetido para cada observación. Ya sólo necesitamos renombrar las columnas restantes para que nos indiquen más claramente su contenido:\n\nrangos = rangos.rename(columns = {'level_1':'Grupo', 0:'Población'})\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      Grupo\n      Población\n    \n  \n  \n    \n      0\n      0900200010010\n      P_0A2\n      60.0\n    \n    \n      1\n      0900200010010\n      P_0A2_F\n      32.0\n    \n    \n      2\n      0900200010010\n      P_0A2_M\n      28.0\n    \n    \n      3\n      0900200010010\n      P_3YMAS\n      3123.0\n    \n    \n      4\n      0900200010010\n      P_3YMAS_F\n      1663.0\n    \n  \n\n\n\n\n!Ahora tenemos nuestra tabla acomodada a los principios tidy!"
  },
  {
    "objectID": "parte_1/01_transformacion.html#agrupamiento-transformación-y-agregación",
    "href": "parte_1/01_transformacion.html#agrupamiento-transformación-y-agregación",
    "title": "1  Transformación de datos",
    "section": "1.8 Agrupamiento, Transformación y Agregación",
    "text": "1.8 Agrupamiento, Transformación y Agregación\nUna ventaja de tener los datos estructurados de acuerdo a los principios tidy es la facilidad con la que podemos realizar procesos de transformación más sofisticados como agrupaciones y sumarios. Las agrupaciones consisten en agrupar observaciones en una tabla de acuerdo a sus valores (o expresiones) en una columna, a los datos agrupados se le pueden aplicar operaciones de agregación más o menos arbitrarias.\nDigamos, por ejemplo, que queremos obtener los totales de población para cada grupo etario a través de todas las AGEBs. Para hacer esto tenemos que agrupar las observaciones por cada Grupo y después obtener el valor agregado por la suma. Vamos por partes.\n\ngrupos = rangos.groupby('Grupo')\ngrupos\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f70945509d0>\n\n\nLa función groupby nos permite agrupar los datos de acuerdo a una (o más) columnas. El resultado, como pueden ver, no es un DataFrame sino un objeto de la clase especial pandas.core.groupby.generic.DataFrameGroupBy. Esta clase sirve para representar DataFrames agregados, estos objetos nos permiten obtener de forma fácil los valores que corresponden a diferentes funciones de agregación. Por ejemplo, para obtener el total de posblación por cada grupo, podemos agregar nuestro objeto con la función sum:\n\ngrupos.sum(numeric_only=True)\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      Grupo\n      \n    \n  \n  \n    \n      P_0A2\n      264424.0\n    \n    \n      P_0A2_F\n      130297.0\n    \n    \n      P_0A2_M\n      134053.0\n    \n    \n      P_12A14\n      364225.0\n    \n    \n      P_12A14_F\n      179955.0\n    \n    \n      P_12A14_M\n      184240.0\n    \n    \n      P_12YMAS\n      7864313.0\n    \n    \n      P_12YMAS_F\n      4141887.0\n    \n    \n      P_12YMAS_M\n      3722424.0\n    \n    \n      P_15A17\n      377178.0\n    \n    \n      P_15A17_F\n      185144.0\n    \n    \n      P_15A17_M\n      191984.0\n    \n    \n      P_15A49_F\n      2490275.0\n    \n    \n      P_15YMAS\n      7500071.0\n    \n    \n      P_15YMAS_F\n      3961914.0\n    \n    \n      P_15YMAS_M\n      3538155.0\n    \n    \n      P_18A24\n      975897.0\n    \n    \n      P_18A24_F\n      483893.0\n    \n    \n      P_18A24_M\n      491985.0\n    \n    \n      P_18YMAS\n      7122878.0\n    \n    \n      P_18YMAS_F\n      3776738.0\n    \n    \n      P_18YMAS_M\n      3346138.0\n    \n    \n      P_3A5\n      321650.0\n    \n    \n      P_3A5_F\n      158674.0\n    \n    \n      P_3A5_M\n      162933.0\n    \n    \n      P_3YMAS\n      8871506.0\n    \n    \n      P_3YMAS_F\n      4637724.0\n    \n    \n      P_3YMAS_M\n      4233780.0\n    \n    \n      P_5YMAS\n      8660874.0\n    \n    \n      P_5YMAS_F\n      4533469.0\n    \n    \n      P_5YMAS_M\n      4127403.0\n    \n    \n      P_60YMAS\n      1487004.0\n    \n    \n      P_60YMAS_F\n      850901.0\n    \n    \n      P_60YMAS_M\n      636074.0\n    \n    \n      P_6A11\n      685511.0\n    \n    \n      P_6A11_F\n      337113.0\n    \n    \n      P_6A11_M\n      348375.0\n    \n    \n      P_8A14\n      829786.0\n    \n    \n      P_8A14_F\n      408494.0\n    \n    \n      P_8A14_M\n      421280.0\n    \n  \n\n\n\n\nComo ve, al usar un agregador sobre el objeto agrupado obtenemos un DataFrame con los valores que corresponden a la agregación que utilizamos.\n\n\n\n\n\n\nNote\n\n\n\nEl parámetro numeric_only=True le dice al agregador que sólo calcule el resultado para las columnas de tipo numérico.\n\n\nEn este caso la función que usamos para agregar los datos es la suma, sin embargo es posible utilizar cualquier función que opere sobre grupos de observaciones, por ejemplo, el promedio:\n\ngrupos.mean(numeric_only=True)\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      Grupo\n      \n    \n  \n  \n    \n      P_0A2\n      109.901912\n    \n    \n      P_0A2_F\n      54.471990\n    \n    \n      P_0A2_M\n      56.089121\n    \n    \n      P_12A14\n      151.130705\n    \n    \n      P_12A14_F\n      74.701121\n    \n    \n      P_12A14_M\n      76.702748\n    \n    \n      P_12YMAS\n      3245.692530\n    \n    \n      P_12YMAS_F\n      1710.110239\n    \n    \n      P_12YMAS_M\n      1536.287247\n    \n    \n      P_15A17\n      156.310816\n    \n    \n      P_15A17_F\n      77.014975\n    \n    \n      P_15A17_M\n      79.993333\n    \n    \n      P_15A49_F\n      1028.614209\n    \n    \n      P_15YMAS\n      3095.365662\n    \n    \n      P_15YMAS_F\n      1635.802642\n    \n    \n      P_15YMAS_M\n      1460.237309\n    \n    \n      P_18A24\n      403.596774\n    \n    \n      P_18A24_F\n      200.038446\n    \n    \n      P_18A24_M\n      204.143154\n    \n    \n      P_18YMAS\n      2939.693768\n    \n    \n      P_18YMAS_F\n      1559.346821\n    \n    \n      P_18YMAS_M\n      1380.989682\n    \n    \n      P_3A5\n      133.298798\n    \n    \n      P_3A5_F\n      65.949293\n    \n    \n      P_3A5_M\n      67.775790\n    \n    \n      P_3YMAS\n      3661.372678\n    \n    \n      P_3YMAS_F\n      1914.832370\n    \n    \n      P_3YMAS_M\n      1747.329757\n    \n    \n      P_5YMAS\n      3574.442427\n    \n    \n      P_5YMAS_F\n      1871.787366\n    \n    \n      P_5YMAS_M\n      1703.426744\n    \n    \n      P_60YMAS\n      615.481788\n    \n    \n      P_60YMAS_F\n      353.511010\n    \n    \n      P_60YMAS_M\n      264.040681\n    \n    \n      P_6A11\n      284.326421\n    \n    \n      P_6A11_F\n      140.055256\n    \n    \n      P_6A11_M\n      144.734109\n    \n    \n      P_8A14\n      343.739022\n    \n    \n      P_8A14_F\n      169.218724\n    \n    \n      P_8A14_M\n      174.587650\n    \n  \n\n\n\n\nLas funciones que usamos para agregar (sum y mean) son funciones de numpyy podemos utiliizar cualquier función de agregación. También es posible calcular diferentes agregaciones al mismo tiempo:\n\ngrupos.aggregate([np.sum, np.mean, np.std])\n\n/tmp/ipykernel_5237/732611272.py:1: FutureWarning: ['AGEB_cvgeo'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n  grupos.aggregate([np.sum, np.mean, np.std])\n\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      \n      sum\n      mean\n      std\n    \n    \n      Grupo\n      \n      \n      \n    \n  \n  \n    \n      P_0A2\n      264424.0\n      109.901912\n      85.636899\n    \n    \n      P_0A2_F\n      130297.0\n      54.471990\n      42.286817\n    \n    \n      P_0A2_M\n      134053.0\n      56.089121\n      43.908616\n    \n    \n      P_12A14\n      364225.0\n      151.130705\n      111.565262\n    \n    \n      P_12A14_F\n      179955.0\n      74.701121\n      55.572013\n    \n    \n      P_12A14_M\n      184240.0\n      76.702748\n      56.746606\n    \n    \n      P_12YMAS\n      7864313.0\n      3245.692530\n      2056.644056\n    \n    \n      P_12YMAS_F\n      4141887.0\n      1710.110239\n      1073.566831\n    \n    \n      P_12YMAS_M\n      3722424.0\n      1536.287247\n      1001.153466\n    \n    \n      P_15A17\n      377178.0\n      156.310816\n      113.532155\n    \n    \n      P_15A17_F\n      185144.0\n      77.014975\n      56.107357\n    \n    \n      P_15A17_M\n      191984.0\n      79.993333\n      57.996607\n    \n    \n      P_15A49_F\n      2490275.0\n      1028.614209\n      692.206450\n    \n    \n      P_15YMAS\n      7500071.0\n      3095.365662\n      1955.668987\n    \n    \n      P_15YMAS_F\n      3961914.0\n      1635.802642\n      1023.764553\n    \n    \n      P_15YMAS_M\n      3538155.0\n      1460.237309\n      950.879515\n    \n    \n      P_18A24\n      975897.0\n      403.596774\n      279.378732\n    \n    \n      P_18A24_F\n      483893.0\n      200.038446\n      138.590941\n    \n    \n      P_18A24_M\n      491985.0\n      204.143154\n      143.125222\n    \n    \n      P_18YMAS\n      7122878.0\n      2939.693768\n      1853.201763\n    \n    \n      P_18YMAS_F\n      3776738.0\n      1559.346821\n      973.233847\n    \n    \n      P_18YMAS_M\n      3346138.0\n      1380.989682\n      899.958704\n    \n    \n      P_3A5\n      321650.0\n      133.298798\n      101.904268\n    \n    \n      P_3A5_F\n      158674.0\n      65.949293\n      50.305709\n    \n    \n      P_3A5_M\n      162933.0\n      67.775790\n      52.251282\n    \n    \n      P_3YMAS\n      8871506.0\n      3661.372678\n      2347.050678\n    \n    \n      P_3YMAS_F\n      4637724.0\n      1914.832370\n      1215.700184\n    \n    \n      P_3YMAS_M\n      4233780.0\n      1747.329757\n      1147.281855\n    \n    \n      P_5YMAS\n      8660874.0\n      3574.442427\n      2284.544513\n    \n    \n      P_5YMAS_F\n      4533469.0\n      1871.787366\n      1185.089392\n    \n    \n      P_5YMAS_M\n      4127403.0\n      1703.426744\n      1115.802146\n    \n    \n      P_60YMAS\n      1487004.0\n      615.481788\n      358.110680\n    \n    \n      P_60YMAS_F\n      850901.0\n      353.511010\n      206.712937\n    \n    \n      P_60YMAS_M\n      636074.0\n      264.040681\n      152.406790\n    \n    \n      P_6A11\n      685511.0\n      284.326421\n      213.690386\n    \n    \n      P_6A11_F\n      337113.0\n      140.055256\n      105.214351\n    \n    \n      P_6A11_M\n      348375.0\n      144.734109\n      109.164209\n    \n    \n      P_8A14\n      829786.0\n      343.739022\n      255.780534\n    \n    \n      P_8A14_F\n      408494.0\n      169.218724\n      126.379228\n    \n    \n      P_8A14_M\n      421280.0\n      174.587650\n      130.222008"
  },
  {
    "objectID": "parte_1/01_transformacion.html#para-practicar",
    "href": "parte_1/01_transformacion.html#para-practicar",
    "title": "1  Transformación de datos",
    "section": "1.9 Para Practicar",
    "text": "1.9 Para Practicar\nLa organización Wikileaks posee una Base de Datos pública en la cual se contiene, entre otras cosas, el número de casualidades existentes durante los primeros años de la Guerra de Afganistán, la cual puede ser consultada a través de la siguiente liga:\n\nhttps://docs.google.com/spreadsheets/d/1EAx8_ksSCmoWW_SlhFyq2QrRn0FNNhcg1TtDFJzZRgc/edit?hl=en#gid=1\n\n\n\n\nWikileaks\n\n\nA partir de los datos, realiza los siguientes ejercidios: * Descarga la tabla como un archivo de tipo .csv (Archivo –> Descargar como –> .csv, hoja actual). * Importa los datos a un DataFrame de Pandas. * Explora los datos generando estadísticas descriptivas y algunas gráficas. * Examina qué tanto se ajusta a los principios del Tidy Data y ajústalo según creas conveniente * Obten una cuenta total de las bajas por mes y genera una gráfica con dicho conteo."
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#exploración-del-contenido",
    "href": "parte_1/02_ejemplo_transformacion.html#exploración-del-contenido",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.1 Exploración del contenido",
    "text": "2.1 Exploración del contenido\nLo primero que vamos a hacer es explorar los datos publicados por la Secretaría de Salud para entender cómo están organizados. En la carpeta de datos del libro puedes encontrar un ejemplo de la base de datos para el 9 de enero de 2023 bajo el nombre datos_abiertos_covid19.zip.\nPara leer los datos vamos a utilizar la función read_csv(), esta función (como pueden ver) acepta que el csv venga comprimido en un zip.\n\ndf = pd.read_csv('datos/datos_abiertos_covid19.zip', dtype=object, encoding='latin-1')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      0\n      2023-01-03\n      01e27d\n      2\n      9\n      25\n      2\n      25\n      25\n      001\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      2\n      2023-01-03\n      06fce8\n      1\n      12\n      07\n      1\n      07\n      07\n      059\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      3\n      2023-01-03\n      1a4a8d\n      1\n      12\n      23\n      2\n      27\n      23\n      008\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nCada renglón en la base de datos corresponde a un caso en seguimiento, el resultado de cada caso se puede actualizar en sucesivas publicaciones de la base de datos. Las columnas describen un conjunto de variables asociadas al seguimiento de cada uno de los casos. Las dos primeras columnas corresponden a la fecha en la que se actualizó el caso y a un id único para cada caso respectivamente, en este taller no vamos a usar esas dos columnas.\nLuego vienen un conjunto de columnas que describen la unidad médica de reporte y, después, las columnas que nos interesan más, que son las que describen al paciente.\nPara entender un poco mejor los datos, conviene leer el archívo de catálogo. Lo pueden descargar del sitio de datos abiertos o bien usar el que viene en la carpeta de datos del libro bajo el nombre 201128 Catalogos.xlsx. Como el catálogo es un archivo de excel con varias hojas, lo vamos a leer usando openpyxl que nos va a devolver un diccionario de DataFrames que relacionan el nombre de la hoja con los datos que contiene.\n\ncatalogos = 'datos/201128 Catalogos.xlsx'\nnombres_catalogos = ['Catálogo de ENTIDADES', # Acá están los nombres de las hojas del excel\n                      'Catálogo MUNICIPIOS',\n                      'Catálogo SI_NO',\n                      'Catálogo TIPO_PACIENTE',\n                      'Catálogo CLASIFICACION_FINAL',\n                      'Catálogo RESULTADO_LAB'\n                     ]\n# read_excel nos regresa un diccionario que relaciona el nombre de cada hoja con \n# el contenido de la hoja como DataFrame\ndict_catalogos = pd.read_excel(catalogos,\n                          nombres_catalogos,\n                          dtype=str,\n                          engine='openpyxl')\nclasificacion_final = dict_catalogos['Catálogo CLASIFICACION_FINAL']\n# Aquí le damos nombre a las columnas porque en el excel se saltan dos líneas\nclasificacion_final.columns = [\"CLAVE\", \"CLASIFICACIÓN\", \"DESCRIPCIÓN\"] \nclasificacion_final\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n    \n      2\n      1\n      CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍ...\n      Confirmado por asociación aplica cuando el cas...\n    \n    \n      3\n      2\n      CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n      Confirmado por dictaminación solo aplica para ...\n    \n    \n      4\n      3\n      CASO DE SARS-COV-2  CONFIRMADO\n      Confirmado aplica cuando:\\nEl caso tiene muest...\n    \n    \n      5\n      4\n      INVÁLIDO POR LABORATORIO\n      Inválido aplica cuando el caso no tienen asoci...\n    \n    \n      6\n      5\n      NO REALIZADO POR LABORATORIO\n      No realizado aplica cuando el caso no tienen a...\n    \n    \n      7\n      6\n      CASO SOSPECHOSO\n      Sospechoso aplica cuando: \\nEl caso no tienen ...\n    \n    \n      8\n      7\n      NEGATIVO A SARS-COV-2\n      Negativo aplica cuando el caso:\\n1. Se le tomo...\n    \n  \n\n\n\n\nLo que estamos viendo aquí es el catálogo de datos de la columna CLASIFICACION_FINAL. Este catálogo relaciona el valor de la CLAVE con su significado. En particular, la columna CLASIFICACION_FINAL es la que nos permite identificar los casos positivos como veremos más adelante.\nEl resto de los catálogos funciona de la misma forma, en este momento sólo vamos a utilizar la clasificación de los pacientes, pero más adelante podemos utilzar algunas de las columnas restantes."
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#aplanado-de-datos",
    "href": "parte_1/02_ejemplo_transformacion.html#aplanado-de-datos",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.2 Aplanado de datos",
    "text": "2.2 Aplanado de datos\nComo acabamos de ver, de alguna forma la información viene distribuida en tres archivos, uno con los datos, otro con las categorías que usa y un tercero con sus descripciones. Para utilizar los datos más fácilmente, sobre todo para poder hablarle a las cosas por su nombre en lugar de referirnos a sus valores codificados, vamos a realizar un conjunto de operaciones para aplanar los datos.\nEn el bajo mundo del análisis de datos, aplanar una base de datos es la operación de substituir los valores codificados a partir de un diccionario. En este caso, los datos que leímos traen valores codificados, entonces la primera misión es substituir esos valores por sus equivalentes en el diccionario.\nComo la base de datos es muy grande, vamos a trabajar sólo con un estado de la república, en este caso la Ciudad de México (pero ustedes podrían elegir otro cualquiera).\nPara seleccionar un estado, tenemos que elegir las filas del DataFrame que contengan el valor que queremos en la columna ENTIDAD, para eso vamos a aprender a usar nuestro primer operador de Pandas, el operador loc que nos permite seleccionar filas a partir de los valores de una o más columnas.\n\n# el copy() nos asegura tener una copia de los datos en lugar de una referencia, \n# con eso podemos liberar la memoria más fácil\ndf = df.loc[df['ENTIDAD_RES'] == '09'].copy()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      09\n      2\n      09\n      09\n      016\n      2\n      ...\n      2\n      1\n      4\n      2\n      97\n      2\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      09\n      1\n      09\n      09\n      012\n      1\n      ...\n      99\n      2\n      97\n      1\n      1\n      3\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      09\n      1\n      18\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nFíjense que lo que hicimos fue reescribir en la variable df el resultado de nuestra selección, de forma que df ahora sólo contiene resultados para la CDMX.\nAhora ya con los datos filtrados y, por lo tanto, con un tamaño más manejable, vamos a empezar a trabajarlos. Lo primero que vamos a hacer es cambiar los valores de la columna MUNICIPIO_RES por la concatenación de las claves de estado y municipio, esto porque nos hará más adelante más fácil el trabajo de unir los datos con las geometrías de los municipios y porque además así tendremos un identificador único para estos (claro que esto sólo tiene sentido al trabajar con varios estados al mismo tiempo).\n\ndf['MUNICIPIO_RES'] = df['ENTIDAD_RES'] + df['MUNICIPIO_RES']\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      09012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      09007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      09\n      2\n      09\n      09\n      09016\n      2\n      ...\n      2\n      1\n      4\n      2\n      97\n      2\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      09\n      1\n      09\n      09\n      09012\n      1\n      ...\n      99\n      2\n      97\n      1\n      1\n      3\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      09\n      1\n      18\n      09\n      09007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nAhora vamos a corregir el nombre de una columna en la base de datos para que coincida con el nombre en el diccionario y después podamos buscar automáticamente. Pra corregir el nombre de la columna vamos a utilizar la función rename de Pandas. Esta función nos sirve para renombrar filas (el índice del DataFrame, que vamos a ver más adelante) o columnas dependiendo de qué eje seleccionemos. El eje 0 son las filas y el 1 las columnas.\n\n# Como estamos usando explícitamente el parámetro columns, \n# no necesitamos especificar el eje\ndf = df.rename(columns={'OTRA_COM': 'OTRAS_COM'})\ndf.columns\n\nIndex(['FECHA_ACTUALIZACION', 'ID_REGISTRO', 'ORIGEN', 'SECTOR', 'ENTIDAD_UM',\n       'SEXO', 'ENTIDAD_NAC', 'ENTIDAD_RES', 'MUNICIPIO_RES', 'TIPO_PACIENTE',\n       'FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF', 'INTUBADO', 'NEUMONIA',\n       'EDAD', 'NACIONALIDAD', 'EMBARAZO', 'HABLA_LENGUA_INDIG', 'INDIGENA',\n       'DIABETES', 'EPOC', 'ASMA', 'INMUSUPR', 'HIPERTENSION', 'OTRAS_COM',\n       'CARDIOVASCULAR', 'OBESIDAD', 'RENAL_CRONICA', 'TABAQUISMO',\n       'OTRO_CASO', 'TOMA_MUESTRA_LAB', 'RESULTADO_LAB',\n       'TOMA_MUESTRA_ANTIGENO', 'RESULTADO_ANTIGENO', 'CLASIFICACION_FINAL',\n       'MIGRANTE', 'PAIS_NACIONALIDAD', 'PAIS_ORIGEN', 'UCI'],\n      dtype='object')\n\n\nFíjense cómo otra vez reescribimos la variable df. La mayor parte de las operaciones en Pandas regresan un DataFrame con el resultado de la operación y no modifican el DataFrame original, entonces para guardar los resultados, necesitamos reescribir la variable (o guardarla con otro nombre)\nAhora sí podemos empezar a aplanar los datos. Vamos a empezar por resolver las claves de resultado de las pruebas COVID. En los datos originales estos vienen codificados en la columna RESULTADO_LAB, pero en el diccionario ese velor se llama RESULTADO, entonces otra vez vamos a empezar por renombrar una columna.\n\ndf = df.rename(columns={'RESULTADO_LAB': 'RESULTADO'})\n\nPara sustituir los valores en nuestros datos originales vamos a usar la función map que toma una serie (una serie es una columna de un dataframe) y mapea sus valores de acuerdo a una correspondencia que podemos pasar como un diccionario. Veamos poco a poco cómo hacer lo que queremos.\nLo primero que necesitamos es un diccionario que relacione los valores en nuestros datos con los nombres en el diccionario. Recordemos cómo se ve el diccionario:\n\nclasificacion_final\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n    \n      2\n      1\n      CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍ...\n      Confirmado por asociación aplica cuando el cas...\n    \n    \n      3\n      2\n      CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n      Confirmado por dictaminación solo aplica para ...\n    \n    \n      4\n      3\n      CASO DE SARS-COV-2  CONFIRMADO\n      Confirmado aplica cuando:\\nEl caso tiene muest...\n    \n    \n      5\n      4\n      INVÁLIDO POR LABORATORIO\n      Inválido aplica cuando el caso no tienen asoci...\n    \n    \n      6\n      5\n      NO REALIZADO POR LABORATORIO\n      No realizado aplica cuando el caso no tienen a...\n    \n    \n      7\n      6\n      CASO SOSPECHOSO\n      Sospechoso aplica cuando: \\nEl caso no tienen ...\n    \n    \n      8\n      7\n      NEGATIVO A SARS-COV-2\n      Negativo aplica cuando el caso:\\n1. Se le tomo...\n    \n  \n\n\n\n\nNecesitamos un diccionario {CLASIFICACION:CLAVE} (ya sé que hay unos valores espurios, pero no nos importan porque simplemente esos no los va a encontrar en nuestra base de datos).\nPara construir este diccionario, vamos a empezar por construir la tupla que mantiene la relación que buscamos, para eso vamos a utilizar la función zip que toma dos iteradores como entrada y regresa un iterador que tiene por elementos las tuplas hechas elemento a elemento entre los dos iteradores de inicio. Veamoslo con calma:\n\nl1 = ['a', 'b', 'c']\nl2 = [1, 2, 3]\nl3 = list(zip(l1,l2))\nl3\n\n[('a', 1), ('b', 2), ('c', 3)]\n\n\nLo que nos regresa zip es un iterado con las tuplas formadas por los pares ordenados de los iteradores de entrada. En Python un iterador es cualquier cosa que se pueda recorrer en orden, a veces estos iteradores, como en el caso de zip no regresan todas las entradas sino, para ahorrar memoria, las generan conforme se recorren, por eso hay que hacer list(zip) para que se generen las entradas.\nAhora sí podemos entonces crear el diccionario con el que vamos a actualizar los datos:\n\nclasificacion_final = dict(zip(clasificacion_final['CLAVE'], clasificacion_final['CLASIFICACIÓN']))\nclasificacion_final\n\n{nan: nan,\n 'CLAVE': 'CLASIFICACIÓN',\n '1': 'CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n '2': 'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n '3': 'CASO DE SARS-COV-2  CONFIRMADO',\n '4': 'INVÁLIDO POR LABORATORIO',\n '5': 'NO REALIZADO POR LABORATORIO',\n '6': 'CASO SOSPECHOSO',\n '7': 'NEGATIVO A SARS-COV-2'}\n\n\nY entonces pasarlo como argumento a la función map. Hay un truco aquí, map toma como argumento una función que, para cada llave, regresa el valor correspondiente, entonces no es propiamente el diccionario lo que vamos a pasar, sino la función get del diccionario que hace justo lo que queremos. Esto nos revela una prpiedad curiosa de Python, los argumentos de una función pueden ser funciones.\n\ndf['CLASIFICACION_FINAL'] = df['CLASIFICACION_FINAL'].map(clasificacion_final.get)\ndf['CLASIFICACION_FINAL'].head()\n\n1                                 NEGATIVO A SARS-COV-2\n4                                 NEGATIVO A SARS-COV-2\n8     CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n13                       CASO DE SARS-COV-2  CONFIRMADO\n15                                NEGATIVO A SARS-COV-2\nName: CLASIFICACION_FINAL, dtype: object\n\n\nAhora vamos a hacer una sustitución un poco más compleja, tenemos que encontrar todos los campos de tipo “SI - NO” y resolverlos (sustituir por valores que podamos manejar más fácil). Los campos que tienen este tipo de datos vienen en el excel de descriptores:\n\ndescriptores = pd.read_excel('datos/201128 Descriptores_.xlsx',\n                             index_col='Nº',\n                             engine='openpyxl')\ndescriptores\n\n\n\n\n\n  \n    \n      \n      NOMBRE DE VARIABLE\n      DESCRIPCIÓN DE VARIABLE\n      FORMATO O FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      1\n      FECHA_ACTUALIZACION\n      La base de datos se alimenta diariamente, esta...\n      AAAA-MM-DD\n    \n    \n      2\n      ID_REGISTRO\n      Número identificador del caso\n      TEXTO\n    \n    \n      3\n      ORIGEN\n      La vigilancia centinela se realiza a través de...\n      CATÁLOGO: ORIGEN                              ...\n    \n    \n      4\n      SECTOR\n      Identifica el tipo de institución del Sistema ...\n      CATÁLOGO: SECTOR                              ...\n    \n    \n      5\n      ENTIDAD_UM\n      Identifica la entidad donde se ubica la unidad...\n      CATALÓGO: ENTIDADES\n    \n    \n      6\n      SEXO\n      Identifica al sexo del paciente.\n      CATÁLOGO: SEXO\n    \n    \n      7\n      ENTIDAD_NAC\n      Identifica la entidad de nacimiento del paciente.\n      CATALÓGO: ENTIDADES\n    \n    \n      8\n      ENTIDAD_RES\n      Identifica la entidad de residencia del paciente.\n      CATALÓGO: ENTIDADES\n    \n    \n      9\n      MUNICIPIO_RES\n      Identifica el municipio de residencia del paci...\n      CATALÓGO: MUNICIPIOS\n    \n    \n      10\n      TIPO_PACIENTE\n      Identifica el tipo de atención que recibió el ...\n      CATÁLOGO: TIPO_PACIENTE\n    \n    \n      11\n      FECHA_INGRESO\n      Identifica la fecha de ingreso del paciente a ...\n      AAAA-MM-DD\n    \n    \n      12\n      FECHA_SINTOMAS\n      Idenitifica la fecha en que inició la sintomat...\n      AAAA-MM-DD\n    \n    \n      13\n      FECHA_DEF\n      Identifica la fecha en que el paciente falleció.\n      AAAA-MM-DD\n    \n    \n      14\n      INTUBADO\n      Identifica si el paciente requirió de intubación.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      15\n      NEUMONIA\n      Identifica si al paciente se le diagnosticó co...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      16\n      EDAD\n      Identifica la edad del paciente.\n      NÚMERICA EN AÑOS\n    \n    \n      17\n      NACIONALIDAD\n      Identifica si el paciente es mexicano o extran...\n      CATÁLOGO: NACIONALIDAD\n    \n    \n      18\n      EMBARAZO\n      Identifica si la paciente está embarazada.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      19\n      HABLA_LENGUA_INDIG\n      Identifica si el paciente habla lengua índigena.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      20\n      INDIGENA\n      Identifica si el paciente se autoidentifica co...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      21\n      DIABETES\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      22\n      EPOC\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      23\n      ASMA\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      24\n      INMUSUPR\n      Identifica si el paciente presenta inmunosupre...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      25\n      HIPERTENSION\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      26\n      OTRAS_COM\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      27\n      CARDIOVASCULAR\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      28\n      OBESIDAD\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      29\n      RENAL_CRONICA\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      30\n      TABAQUISMO\n      Identifica si el paciente tiene hábito de taba...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      31\n      OTRO_CASO\n      Identifica si el paciente tuvo contacto con al...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      32\n      TOMA_MUESTRA_LAB\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      33\n      RESULTADO_LAB\n      Identifica el resultado del análisis de la mue...\n      CATÁLOGO: RESULTADO_LAB\n    \n    \n      34\n      TOMA_MUESTRA_ANTIGENO\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      35\n      RESULTADO_ANTIGENO\n      Identifica el resultado del análisis de la mue...\n      CATÁLOGO: RESULTADO_ANTIGENO\n    \n    \n      36\n      CLASIFICACION_FINAL\n      Identifica si el paciente es un caso de COVID-...\n      CATÁLOGO: CLASIFICACION_FINAL\n    \n    \n      37\n      MIGRANTE\n      Identifica si el paciente es una persona migra...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      38\n      PAIS_NACIONALIDAD\n      Identifica la nacionalidad del paciente.\n      TEXTO, 99= SE IGNORA\n    \n    \n      39\n      PAIS_ORIGEN\n      Identifica el país del que partió el paciente ...\n      TEXTO, 97= NO APLICA\n    \n    \n      40\n      UCI\n      Identifica si el paciente requirió ingresar a ...\n      CATÁLOGO: SI_ NO                              ...\n    \n  \n\n\n\n\nFíjense en alguno de estos campos en los datos:\n\ndf['OBESIDAD'].unique()\n\narray(['2', '98', '1'], dtype=object)\n\n\nTenemos tres valores diferentes que corresponden (vean el diccionario) a SI, NO y NO ESPECIFICADO. Para todos los análisis que vamos a hacer en general sólo nos van a interesar los casos que sabemos que son SI, entonces lo que más nos conviene es codificar todos estos como binarios, es decir, sólo SI o NO. Además, podemos mejor decirles 1,0 respectivamente y así vamos a poder hacer cuentas mucho más fácil\nDe estos descriptores nos interesan los que tienen CATÁLOGO: SI_ NO en el campo FORMATO O FUENTE. Para poder encontrar y sustituir de forma más sencilla y automática vamos a hacer un par de modificaciones a los datos:\n\nReemplazar los espacios en los nombres de columnas por guiones bajos (para poder “hablarles” más fácil a las columnas)\nQuitar espacios al principio o al final de los valores de los campos (para asegurarnos de que siempre van a ser los mismos)\n\n\ndescriptores.columns = list(map(lambda col: col.replace(' ', '_'), descriptores.columns))\ndescriptores.head()\n\n\n\n\n\n  \n    \n      \n      NOMBRE_DE_VARIABLE\n      DESCRIPCIÓN_DE_VARIABLE\n      FORMATO_O_FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      1\n      FECHA_ACTUALIZACION\n      La base de datos se alimenta diariamente, esta...\n      AAAA-MM-DD\n    \n    \n      2\n      ID_REGISTRO\n      Número identificador del caso\n      TEXTO\n    \n    \n      3\n      ORIGEN\n      La vigilancia centinela se realiza a través de...\n      CATÁLOGO: ORIGEN                              ...\n    \n    \n      4\n      SECTOR\n      Identifica el tipo de institución del Sistema ...\n      CATÁLOGO: SECTOR                              ...\n    \n    \n      5\n      ENTIDAD_UM\n      Identifica la entidad donde se ubica la unidad...\n      CATALÓGO: ENTIDADES\n    \n  \n\n\n\n\nPoco a poco:\n\ndescriptores.columns nos regresa (o les da valor, cuando está del lado izquierdo de un =) los nombres de las columnas del DataFrame\nmap(lambda col: col.replace(' ', '_'), descriptores.columns) la función map regresa una asosiación, como ya vimos. En este caso esta asosiación se hace a través de una función anónima lambda que toma como argumento el nombre de una columna y regresa el mismo nombre pero con los espacios sustituidos por guines bajos\n\nAl final, lo que hacemos es sustituir los nombres de las columnas por una lista hecha por nosotros, para que esto funcione la lista que pasamos debe ser de igual tamaño que la lista original de columnas.\nAhora vamos a hacer lo mismo pero con los valores de los campos:\n\ndescriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\ndescriptores['FORMATO_O_FUENTE'].head()\n\nNº\n1             AAAA-MM-DD\n2                  TEXTO\n3       CATÁLOGO: ORIGEN\n4       CATÁLOGO: SECTOR\n5    CATALÓGO: ENTIDADES\nName: FORMATO_O_FUENTE, dtype: object\n\n\nEste fué más fácil. Fíjense cómo pedimos el campo del lado derecho: descriptores.FORMATO_O_FUENTE, esto es equivalente a descriptores['FORMATO_O_FUENTE'] y los pueden usar indistintamente (claro, el primero sólo funciona si el nombre del campo no tiene espacios).\nFiltremos ahora los descriptores para quedarnos sólo con los que nos interesan, para eso vamos a usar la función query de Pandas, que nos permite filtrar un DataFrame de forma conveniente usando una expresión booleana:\n\ndatos_si_no = descriptores.query('FORMATO_O_FUENTE == \"CATÁLOGO: SI_ NO\"')\ndatos_si_no\n\n\n\n\n\n  \n    \n      \n      NOMBRE_DE_VARIABLE\n      DESCRIPCIÓN_DE_VARIABLE\n      FORMATO_O_FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      14\n      INTUBADO\n      Identifica si el paciente requirió de intubación.\n      CATÁLOGO: SI_ NO\n    \n    \n      15\n      NEUMONIA\n      Identifica si al paciente se le diagnosticó co...\n      CATÁLOGO: SI_ NO\n    \n    \n      18\n      EMBARAZO\n      Identifica si la paciente está embarazada.\n      CATÁLOGO: SI_ NO\n    \n    \n      19\n      HABLA_LENGUA_INDIG\n      Identifica si el paciente habla lengua índigena.\n      CATÁLOGO: SI_ NO\n    \n    \n      20\n      INDIGENA\n      Identifica si el paciente se autoidentifica co...\n      CATÁLOGO: SI_ NO\n    \n    \n      21\n      DIABETES\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      22\n      EPOC\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      23\n      ASMA\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      24\n      INMUSUPR\n      Identifica si el paciente presenta inmunosupre...\n      CATÁLOGO: SI_ NO\n    \n    \n      25\n      HIPERTENSION\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      26\n      OTRAS_COM\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      27\n      CARDIOVASCULAR\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      28\n      OBESIDAD\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      29\n      RENAL_CRONICA\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      30\n      TABAQUISMO\n      Identifica si el paciente tiene hábito de taba...\n      CATÁLOGO: SI_ NO\n    \n    \n      31\n      OTRO_CASO\n      Identifica si el paciente tuvo contacto con al...\n      CATÁLOGO: SI_ NO\n    \n    \n      32\n      TOMA_MUESTRA_LAB\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      34\n      TOMA_MUESTRA_ANTIGENO\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      37\n      MIGRANTE\n      Identifica si el paciente es una persona migra...\n      CATÁLOGO: SI_ NO\n    \n    \n      40\n      UCI\n      Identifica si el paciente requirió ingresar a ...\n      CATÁLOGO: SI_ NO\n    \n  \n\n\n\n\nPor si acaso, quitémosle también los espacios al campo FORMATO_O_FUENTE\n\ndescriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\n\nAhora sí, vamos a sustituir los valores como queremos en los datos originales. Para eso, lo primero que tenemos que hacer es fijarnos en el catálogo de estos campos:\n\ncat_si_no = dict_catalogos['Catálogo SI_NO']\ncat_si_no\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      1\n      SI\n    \n    \n      1\n      2\n      NO\n    \n    \n      2\n      97\n      NO APLICA\n    \n    \n      3\n      98\n      SE IGNORA\n    \n    \n      4\n      99\n      NO ESPECIFICADO\n    \n  \n\n\n\n\nJusto estos valores los queremos cambiar por claves binarias (acuérdense, para distinguirlos fácilmente). Entonces lo que necesitamos ahora es:\n\nUna lista de los nombres de los campos en donde vamos a hacer la sustitución\nUn mapeo de los valores con los que vamos a sustituir\nHacer la sustitución primero en el diccionario y a partir de eso en los datos originales\n\n\n# lista de los nombres de los campos\ncampos_si_no = datos_si_no.NOMBRE_DE_VARIABLE\n# sustituimos en el catálogo de acuerdo a lo que nos interesa\ncat_si_no['DESCRIPCIÓN'] = list(map(lambda val: 1 if val == 'SI' else 0, cat_si_no['DESCRIPCIÓN']))\n# sustituimos en los datos originales\ndf[campos_si_no] = df[datos_si_no.NOMBRE_DE_VARIABLE].replace(\n                                            to_replace=cat_si_no['CLAVE'].values,\n                                            value=cat_si_no['DESCRIPCIÓN'].values)\ndf[campos_si_no]\n\n\n\n\n\n  \n    \n      \n      INTUBADO\n      NEUMONIA\n      EMBARAZO\n      HABLA_LENGUA_INDIG\n      INDIGENA\n      DIABETES\n      EPOC\n      ASMA\n      INMUSUPR\n      HIPERTENSION\n      OTRAS_COM\n      CARDIOVASCULAR\n      OBESIDAD\n      RENAL_CRONICA\n      TABAQUISMO\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      TOMA_MUESTRA_ANTIGENO\n      MIGRANTE\n      UCI\n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      13\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6393642\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394417\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394626\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394988\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6395781\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n1896084 rows × 20 columns\n\n\n\nAcá utilizamos para la última sustitución la función replace de Pandas que toma dos parámetros: la lista de valores a reemplazar y la lista de los valoresa de reemplazo. El reemplazo sucede elemento a elemento, es decir, se sustituye el primer elemento de la lista to_replace por el primer elemento de la lista value y así sucesivamente.\nHay más campos que podemos aplanar en la base de datos, como ejercicio pueden explorar algunos de ellos y sustituir como le hemos hecho aquí. Regresaremos a esto más adelante en el taller, pero por lo pronto nos vamos a mover a otra etapa del pre-procesamiento: el manejo de las fechas"
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#manejo-de-fechas",
    "href": "parte_1/02_ejemplo_transformacion.html#manejo-de-fechas",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.3 Manejo de fechas",
    "text": "2.3 Manejo de fechas\nEn Python las fechas son un tipo especial de datos, nosotros estamos acostumbrados a verlas como cadenas de caractéres: 20 de febrero de 2010, por ejemplo. Python puede hacer muchas cosas con las fechas, pero para eso tienen que estar codificados de la forma correcta.\nEn general el módulo datetime de Python provee las utilerías necesarias para manejar/transformar objetos del tipo fecha. Una de las cosas más útiles es transformar strings en objetos datetime:\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime_object\n\ndatetime.datetime(2005, 6, 1, 13, 33)\n\n\nAcá usamos un formato de fecha, '%b %d %Y %I:%M%p', para convertir el string 'Jun 1 2005  1:33PM'. De esa misma forma podemos especificar formatos diferentes:\n\ndatetime_object = datetime.strptime('06-01-2005  1:33PM', '%m-%d-%Y %I:%M%p')\ndatetime_object\n\ndatetime.datetime(2005, 6, 1, 13, 33)\n\n\nPandas tiene la interfase to_datetime para este tipo de operaciones que nos permite transformar campos de forma muy sencilla, por ejemplo, para transformar la columna FECHA_INGRESO de los datos originales en objetos de tipo datetime podemos hacer:\n\npd.to_datetime(df['FECHA_INGRESO'].head())\n\n1    2022-01-19\n4    2022-03-09\n8    2022-02-20\n13   2022-01-01\n15   2022-06-28\nName: FECHA_INGRESO, dtype: datetime64[ns]\n\n\nVean la diferencia con el tipo de datos original:\n\ndf['FECHA_INGRESO'].head()\n\n1     2022-01-19\n4     2022-03-09\n8     2022-02-20\n13    2022-01-01\n15    2022-06-28\nName: FECHA_INGRESO, dtype: object\n\n\nPandas intenta transformar los datos al tipo fecha usando formatos comunes. En general hace un buen trabajo, sin embargo, si nosotros conocemos el formato en el que están escritas las fechas, siempre es mejor ser explícito y usarlo para la transformación. En el caso de nuestros datos, el formato es: %Y-%m-%d, es decir, el año en cuatro caractéres, dos para el mes y dos para los días, separados por guiones medios. Para pasar el formato utilizamos la opción format de pd.to_datetime()\n\npd.to_datetime(df.FECHA_INGRESO, format=\"%Y-%m-%d\")\n\n1         2022-01-19\n4         2022-03-09\n8         2022-02-20\n13        2022-01-01\n15        2022-06-28\n             ...    \n6393642   2022-01-23\n6394417   2022-11-17\n6394626   2022-11-16\n6394988   2022-12-01\n6395781   2022-12-16\nName: FECHA_INGRESO, Length: 1896084, dtype: datetime64[ns]\n\n\nAunque el resultado debería ser el mismo, ser explícito nos ayuda a entender mejor el código y a asegurarnos de que nuestros datos se comportan como nosotros esperamos. Por ejemplo, ¿qué sucedería si algún registro no se contiene datos en el formato que especificamos? Veamos el campo FECHA_DEF que contiene registros intencionalmente inválidos.\n\npd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\")\n\nValueError: time data \"9999-99-99\" at position 0 doesn't match format specified\n\n\n!Tenemos un ERROR! Pandas no puede transformar algunos datos utilizando el formato que le especificamos. En estos casos hay que especificar el copmportamiento que queremos cuando Pandas encuentra una fecha que no se ajusta al formato. El comportamiento por defecto es arrojar una excepción, es decir, detenerse al encontrar un error y reportárnoslo. Eso puede resultar útil en algunos casos, sin embargo no en el nuestro en el que una fecha que no se ajusta al formato significa que el paciente no ha fallecido, es decit las fechas codificadas como 9999-99-99 corresponden a valores nulos en el campo. Para que pandas regrese un valor nulo cuiando encuentre un error en la conversión de fechas, usamos la opción coerce:\n\npd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\", errors='coerce')\n\n1                NaT\n4                NaT\n8         2022-02-21\n13               NaT\n15               NaT\n             ...    \n6393642          NaT\n6394417          NaT\n6394626          NaT\n6394988          NaT\n6395781          NaT\nName: FECHA_DEF, Length: 1896084, dtype: datetime64[ns]\n\n\nAhora los registros que no se pueden convertir en fechas con el formato que especificamos regresan NaT (Not a Time) en lugar de error.\nUna vez que entendimos las formas en las que queremos convertir las columnas con fechas, podemos transformar todas:\n\ndf['FECHA_INGRESO'] = pd.to_datetime(df['FECHA_INGRESO'], format=\"%Y-%m-%d\")\ndf['FECHA_SINTOMAS'] = pd.to_datetime(df['FECHA_SINTOMAS'], format=\"%Y-%m-%d\")\ndf['FECHA_DEF'] = pd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\", errors='coerce')\ndf[['FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF']].head()\n\n\n\n\n\n  \n    \n      \n      FECHA_INGRESO\n      FECHA_SINTOMAS\n      FECHA_DEF\n    \n  \n  \n    \n      1\n      2022-01-19\n      2022-01-17\n      NaT\n    \n    \n      4\n      2022-03-09\n      2022-03-09\n      NaT\n    \n    \n      8\n      2022-02-20\n      2022-02-13\n      2022-02-21\n    \n    \n      13\n      2022-01-01\n      2022-01-01\n      NaT\n    \n    \n      15\n      2022-06-28\n      2022-06-28\n      NaT"
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#exportar-datos",
    "href": "parte_1/02_ejemplo_transformacion.html#exportar-datos",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.4 Exportar datos",
    "text": "2.4 Exportar datos\nYa que tenemos procesados los datos, es muy posible que los querramos guardar para usarlos más adelante. La forma más sencilla de exportar los datos es guardarlos como un csv. Para esto Pandas tiene el método to_csv\n\ndf.to_csv(\"datos/covid_enero_2023_procesados.csv\")\n\nHasta aquí hemos cubierto más o menos todo el pre-proceso de los datos. Claro no vimos todas las columnas, sólo nos fijamos en algunas, pero eso basta para darnos una buena idea de cómo se hacen las demás.\n\n2.4.1 Tarea\nSustituyan los valores de la columna TIPO_PACIENTE por sus valores en el catálogo correspondiente"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#bajar-y-guardar-datos",
    "href": "parte_1/03_automatizacion_transformacion.html#bajar-y-guardar-datos",
    "title": "3  Automatización",
    "section": "3.1 Bajar y guardar datos",
    "text": "3.1 Bajar y guardar datos\nEn el taller anterior bajamos los datos directamente del sitio de la Secretaría de Salud, ahora vamos a automatizar el proceso de descarga de datos de forma que, desde Python, podamos descargar los datos y asegurarnos de que tenemos la última versión disponible.\nDescargar y guardar archivos en Python es relativamente sencillo, vamos a usar tres módulos de la distribución base de Python:\n\nos. Este módulo provee herramientas para interactuar con el sistema operativo. La vamos a usar para construir los paths en donde vamos a guardar los datos y preguntar si el archivo ya existe.\nrequests. Esta librería provee diferentes formas de interactuar con el protocolo HTTP. La vamos a usar para hacer las peticiones a la página y procesar la respuesta.\nzipfile. Esta librería sirve para trabajar con archivos comprimidos en formato zip. En nuestro caso la usaremos para descomprimir los diccionarios.\n\nLa parte complicada de entender es el uso de requests pra comunicarse con la página en donde están los datos.\n\nr = requests.get(\"https://www.centrogeo.org.mx/\")\nr.content[0:500]\n\nb'\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"es-es\" dir=\"ltr\" class=\\'com_content view-featured itemid-101 home j31 mm-hover\\'>\\r\\n<head>\\r\\n<base href=\"https://www.centrogeo.org.mx/\" />\\n\\t<meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\\n\\t<meta name=\"keywords\" content=\"M\\xc3\\xa9xico, CONACYT, CentroGeo, Ciencias de Informaci\\xc3\\xb3n Geoespacial, Centro de Investigaci\\xc3\\xb3n, Ciencias de Informaci\\xc3\\xb3n, Investigaci\\xc3\\xb3n, Geoespacial\" />\\n\\t<meta name=\"rights\" content=\"Esta obra est\\xc3\\xa1 bajo una licencia d'\n\n\nComo ven, una petición de tipo get simplemente nos regresa, a través de la propiedad content, el contenido de la respuesta del servidor. En el caso de la página de CentroGeo, el contenido es el HTML de la página (que podríamos ver mejor con un browser), pero en el caso de que la dirección apunte a un archivo de descarga, el contenido es el stream de datos del archivo. Este stream de datos lo podemos usar como entrada para escribir un archivo utilizando la función open.\nLa función open va a tomar como entrada el path en donde queremos guardar el archivo, este path puede ser simplemente una cadena de caracteres, sin embargo esto haría que nuestro código no fuera interoperable entre sistemas operativos, entonces, en lugar de escribir el path como cadena de caracteres, vamos a escribirlo como un objeto de os:\n\nos.path.join(\"datos\", \"datos_covid.zip\")\n\n'datos/datos_covid.zip'\n\n\nEsta forma de construir el path nos asegura que va a funcionar en cualquier sistema operativo.\nYa con estas explicaciones, podemos escribir la función que descarga los datos:\n\ndef bajar_datos_salud(directorio_datos='data/'):\n    '''\n        Descarga el ultimo archivo disponible en datos abiertos y los diccionarios correspondientes.\n    '''\n    fecha_descarga = datetime.now().date()\n    url_datos = 'https://datosabiertos.salud.gob.mx/gobmx/salud/datos_abiertos/datos_abiertos_covid19.zip'    \n    archivo_nombre = f'{fecha_descarga.strftime(\"%y%m%d\")}COVID19MEXICO.csv.zip'\n    archivo_ruta = os.path.join(directorio_datos, archivo_nombre)\n    url_diccionario = 'https://datosabiertos.salud.gob.mx/gobmx/salud/datos_abiertos/diccionario_datos_covid19.zip'\n    diccionario_ruta = os.path.join(directorio_datos, 'diccionario.zip')\n    if os.path.exists(archivo_ruta):\n        logging.debug(f'Ya existe {archivo_nombre}')\n    else:\n        print(f'Bajando datos...')\n        r = requests.get(url_datos, allow_redirects=True)\n        open(archivo_ruta, 'wb').write(r.content)\n        r = requests.get(url_diccionario, allow_redirects=True)\n        open(diccionario_ruta, 'wb').write(r.content)\n        with zipfile.ZipFile(diccionario_ruta, 'r') as zip_ref:\n          zip_ref.extractall(directorio_datos)\n\nPara utilizar la función hacemos:\n\nbajar_datos_salud('datos/')\n\nBajando datos..."
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#preproceso",
    "href": "parte_1/03_automatizacion_transformacion.html#preproceso",
    "title": "3  Automatización",
    "section": "3.2 Preproceso",
    "text": "3.2 Preproceso\nAhora, ya que tenemos los datos descargados, vamos a empaquetar en una función el flujo de preproceso que trabajamos en el taller anterior. Esta función va a tomar como entrada la carpeta en donde se encuentran los datos y dicionariosy el nombre del archivo de datos que queremos procesar. Toma dos parámetros adicionales, uno para decidir si queremos resolver o no las claves binarias y otro para definir la entidad que queremos procesar.\n\ndef carga_datos_covid19_MX(data_dir='datos/', archivo='datos_abiertos_covid19.zip', resolver_claves='si_no_binarias', entidad='09'):\n    \"\"\"\n        Lee en un DataFrame el CSV con el reporte de casos de la Secretaría de Salud de México publicado en una fecha dada. Esta función\n        también lee el diccionario de datos que acompaña a estas publicaciones para preparar algunos campos, en particular permite la funcionalidad\n        de generar columnas binarias para datos con valores 'SI', 'No'.\n\n        **Nota 2**: Por las actualizaciones a los formatos de datos, esta función sólo va a servir para archivos posteriores a 20-11-28\n\n        resolver_claves: 'sustitucion', 'agregar', 'si_no_binarias', 'solo_localidades'. Resuelve los valores del conjunto de datos usando el\n        diccionario de datos y los catálogos. 'sustitucion' remplaza los valores en las columnas, 'agregar'\n        crea nuevas columnas. 'si_no_binarias' cambia valores SI, NO, No Aplica, SE IGNORA, NO ESPECIFICADO por 1, 0, 0, 0, 0 respectivamente.\n\n    \"\"\"\n    catalogo_nombre ='201128 Catalogos.xlsx'\n    catalogo_path = os.path.join(data_dir, catalogo_nombre)\n    descriptores_nombre = '201128 Descriptores.xlsx'\n    descriptores_path = os.path.join(data_dir, descriptores_nombre)\n    data_file = os.path.join(data_dir, archivo)\n    print(data_file)\n    df = pd.read_csv(data_file, dtype=object, encoding='latin-1')\n    if entidad is not None:\n      df = df[df['ENTIDAD_RES'] == entidad]\n    # Hay un error y el campo OTRA_COMP es OTRAS_COMP según los descriptores\n    df.rename(columns={'OTRA_COM': 'OTRAS_COM'}, inplace=True)\n    # Asignar clave única a municipios\n    df['MUNICIPIO_RES'] = df['ENTIDAD_RES'] + df['MUNICIPIO_RES']\n    df['CLAVE_MUNICIPIO_RES'] = df['MUNICIPIO_RES']\n    # Leer catalogos\n    nombres_catalogos = ['Catálogo de ENTIDADES',\n                         'Catálogo MUNICIPIOS',\n                         'Catálogo RESULTADO',\n                         'Catálogo SI_NO',\n                         'Catálogo TIPO_PACIENTE']\n    nombres_catalogos.append('Catálogo CLASIFICACION_FINAL')\n    nombres_catalogos[2] = 'Catálogo RESULTADO_LAB'\n\n    dict_catalogos = pd.read_excel(catalogo_path,\n                              nombres_catalogos,\n                              dtype=str,\n                              engine='openpyxl')\n\n    entidades = dict_catalogos[nombres_catalogos[0]]\n    municipios = dict_catalogos[nombres_catalogos[1]]\n    tipo_resultado = dict_catalogos[nombres_catalogos[2]]\n    cat_si_no = dict_catalogos[nombres_catalogos[3]]\n    cat_tipo_pac = dict_catalogos[nombres_catalogos[4]]\n    # Arreglar los catálogos que tienen mal las primeras líneas\n    dict_catalogos[nombres_catalogos[2]].columns = [\"CLAVE\", \"DESCRIPCIÓN\"]\n    dict_catalogos[nombres_catalogos[5]].columns = [\"CLAVE\", \"CLASIFICACIÓN\", \"DESCRIPCIÓN\"]\n\n\n    clasificacion_final = dict_catalogos[nombres_catalogos[5]]\n\n\n    # Resolver códigos de entidad federal\n    cols_entidad = ['ENTIDAD_RES', 'ENTIDAD_UM', 'ENTIDAD_NAC']\n    df['CLAVE_ENTIDAD_RES'] = df['ENTIDAD_RES']\n    df[cols_entidad] = df[cols_entidad].replace(to_replace=entidades['CLAVE_ENTIDAD'].values,\n                                               value=entidades['ENTIDAD_FEDERATIVA'].values)\n\n    # Construye clave unica de municipios de catálogo para resolver nombres de municipio\n    municipios['CLAVE_MUNICIPIO'] = municipios['CLAVE_ENTIDAD'] + municipios['CLAVE_MUNICIPIO']\n\n    # Resolver códigos de municipio\n    municipios_dict = dict(zip(municipios['CLAVE_MUNICIPIO'], municipios['MUNICIPIO']))\n    df['MUNICIPIO_RES'] = df['MUNICIPIO_RES'].map(municipios_dict.get)\n\n    # Resolver resultados\n\n    df.rename(columns={'RESULTADO_LAB': 'RESULTADO'}, inplace=True)\n    tipo_resultado['DESCRIPCIÓN'].replace({'POSITIVO A SARS-COV-2': 'Positivo SARS-CoV-2'}, inplace=True)\n\n    tipo_resultado = dict(zip(tipo_resultado['CLAVE'], tipo_resultado['DESCRIPCIÓN']))\n    df['RESULTADO'] = df['RESULTADO'].map(tipo_resultado.get)\n    clasificacion_final = dict(zip(clasificacion_final['CLAVE'], clasificacion_final['CLASIFICACIÓN']))\n    df['CLASIFICACION_FINAL'] = df['CLASIFICACION_FINAL'].map(clasificacion_final.get)\n    # Resolver datos SI - NO\n\n    # Necesitamos encontrar todos los campos que tienen este tipo de dato y eso\n    # viene en los descriptores, en el campo FORMATO_O_FUENTE\n    descriptores = pd.read_excel(f'{data_dir}201128 Descriptores_.xlsx',\n                                 index_col='Nº',\n                                 engine='openpyxl')\n    descriptores.columns = list(map(lambda col: col.replace(' ', '_'), descriptores.columns))\n    descriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\n\n    datos_si_no = descriptores.query('FORMATO_O_FUENTE == \"CATÁLOGO: SI_ NO\"')\n    cat_si_no['DESCRIPCIÓN'] = cat_si_no['DESCRIPCIÓN'].str.strip()\n\n    campos_si_no = datos_si_no.NOMBRE_DE_VARIABLE\n    nuevos_campos_si_no = campos_si_no\n\n    if resolver_claves == 'agregar':\n        nuevos_campos_si_no = [nombre_var + '_NOM' for nombre_var in campos_si_no]\n    elif resolver_claves == 'si_no_binarias':\n        nuevos_campos_si_no = [nombre_var + '_BIN' for nombre_var in campos_si_no]\n        cat_si_no['DESCRIPCIÓN'] = list(map(lambda val: 1 if val == 'SI' else 0, cat_si_no['DESCRIPCIÓN']))\n\n    df[nuevos_campos_si_no] = df[datos_si_no.NOMBRE_DE_VARIABLE].replace(\n                                                to_replace=cat_si_no['CLAVE'].values,\n                                                value=cat_si_no['DESCRIPCIÓN'].values)\n\n    # Resolver tipos de paciente\n    cat_tipo_pac = dict(zip(cat_tipo_pac['CLAVE'], cat_tipo_pac['DESCRIPCIÓN']))\n    df['TIPO_PACIENTE'] = df['TIPO_PACIENTE'].map(cat_tipo_pac.get)\n\n    df = procesa_fechas(df)\n\n    return df\n\ndef procesa_fechas(covid_df):\n    df = covid_df.copy()\n    df['FECHA_INGRESO'] = pd.to_datetime(df['FECHA_INGRESO'], format=\"%Y-%m-%d\")\n    df['FECHA_SINTOMAS'] = pd.to_datetime(df['FECHA_SINTOMAS'], format=\"%Y-%m-%d\")\n    df['FECHA_DEF'] = pd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\", errors='coerce')\n    df['DEFUNCION'] = (df['FECHA_DEF'].notna()).astype(int)\n    df['EDAD'] = df['EDAD'].astype(int)\n    return df"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#preprocesar-usando-nuestras-funciones",
    "href": "parte_1/03_automatizacion_transformacion.html#preprocesar-usando-nuestras-funciones",
    "title": "3  Automatización",
    "section": "3.3 Preprocesar usando nuestras funciones",
    "text": "3.3 Preprocesar usando nuestras funciones\n\ndf = carga_datos_covid19_MX(entidad='09')\ndf\n\ndatos/datos_abiertos_covid19.zip\n\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      CARDIOVASCULAR_BIN\n      OBESIDAD_BIN\n      RENAL_CRONICA_BIN\n      TABAQUISMO_BIN\n      OTRO_CASO_BIN\n      TOMA_MUESTRA_LAB_BIN\n      TOMA_MUESTRA_ANTIGENO_BIN\n      MIGRANTE_BIN\n      UCI_BIN\n      DEFUNCION\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      HOSPITALIZADO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      IZTAPALAPA\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      MIGUEL HIDALGO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      CIUDAD DE MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      CIUDAD DE MÉXICO\n      1\n      NAYARIT\n      CIUDAD DE MÉXICO\n      IZTAPALAPA\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6393642\n      2023-01-03\n      m1cd235\n      2\n      12\n      MÉXICO\n      1\n      MÉXICO\n      CIUDAD DE MÉXICO\n      GUSTAVO A. MADERO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394417\n      2023-01-03\n      m0dbc4c\n      2\n      12\n      MÉXICO\n      1\n      AGUASCALIENTES\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      6394626\n      2023-01-03\n      m13431e\n      2\n      12\n      MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      6394988\n      2023-01-03\n      m1493ea\n      2\n      12\n      MÉXICO\n      1\n      MÉXICO\n      CIUDAD DE MÉXICO\n      GUSTAVO A. MADERO\n      AMBULATORIO\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      6395781\n      2023-01-03\n      m0a22b8\n      2\n      12\n      MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n1896084 rows × 63 columns"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#guardando-el-resultado",
    "href": "parte_1/03_automatizacion_transformacion.html#guardando-el-resultado",
    "title": "3  Automatización",
    "section": "3.4 Guardando el resultado",
    "text": "3.4 Guardando el resultado\nListo, con nuestras funciones tenemos ya nuestros datos preprocesados, ahora vamos a guardarlos para poder utlizarlos rápidamente en otros notebooks. En general tenemos muchas opciones para guardar los datos, csv, por ejemplo. En esta ocasión vamos a usar un formato nativo de Python el pickle, que es una forma de serializar un objeto de Python. Pandas nos provee una función para guardar directamente un dataframe como pickle:\n\ndf.to_pickle(\"data/datos_covid_ene19.pkl\")\n\nEn la documentación de to_pickle pueden ver las opcioones completas.\n\ndf.to_csv(\"datos/covid_enero_2023_procesados.csv\")"
  },
  {
    "objectID": "parte_1/04_visualizacion_covid_1.html#curvas-epidémicas",
    "href": "parte_1/04_visualizacion_covid_1.html#curvas-epidémicas",
    "title": "4  Curvas epidémicas",
    "section": "4.1 Curvas epidémicas",
    "text": "4.1 Curvas epidémicas\nLo primero que haremos será el desarrollo de Curvas Epidémicas es decir, la evolución temporal de los casos confirmados y las defunciones. Si consultamos los diccionarios de datos, podemos ver que los casos confirmados para COVID-19 corresponden a 3 categorías de la columna clasificación final:\n\nCASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA\nCASO DE COVID-19 CONFIRMADO POR COMITÉ DE DICTAMINACIÓN\nCASO DE SARS-COV-2 CONFIRMADO\n\nmientras que las defunciones corresponden a todos aquellos registros que tengan una fecha de defunción válida, es decir, en nuestros datos preprocesados, todas las fechas válidas.\n\n4.1.1 Curva de casos confirmados\nEl primer paso es extraer las filas que corresponden a casos confirmados\n\ndf.CLASIFICACION_FINAL.unique()\n\narray(['NEGATIVO A SARS-COV-2',\n       'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n       'CASO DE SARS-COV-2  CONFIRMADO', 'CASO SOSPECHOSO',\n       'NO REALIZADO POR LABORATORIO',\n       'CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n       'INVÁLIDO POR LABORATORIO'], dtype=object)\n\n\nA partir de estos valores podemos seleccionar las filas que queremos\n\nvalores_confirmados = ['CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n                       'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n                       'CASO DE SARS-COV-2  CONFIRMADO']\nconfirmados = df.loc[df['CLASIFICACION_FINAL'].isin(valores_confirmados)]\nconfirmados.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      CARDIOVASCULAR_BIN\n      OBESIDAD_BIN\n      RENAL_CRONICA_BIN\n      TABAQUISMO_BIN\n      OTRO_CASO_BIN\n      TOMA_MUESTRA_LAB_BIN\n      TOMA_MUESTRA_ANTIGENO_BIN\n      MIGRANTE_BIN\n      UCI_BIN\n      DEFUNCION\n    \n  \n  \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      MIGUEL HIDALGO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      CIUDAD DE MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      25\n      2023-01-03\n      0a98b4\n      2\n      12\n      CIUDAD DE MÉXICO\n      1\n      MICHOACÁN DE OCAMPO\n      CIUDAD DE MÉXICO\n      MILPA ALTA\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      42\n      2023-01-03\n      13cf10\n      2\n      9\n      CIUDAD DE MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      BENITO JUÁREZ\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      54\n      2023-01-03\n      0fef08\n      1\n      12\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 63 columns\n\n\n\nAhora tenemos una tabla con todos los casos confirmados, para hacer una curva epidémica, tenemos que agregar en una escala temporal. Lo más sencillo es primero agregar por día y a partir de ahí podemos construir agregados para cualquier intervalo que queramos.\nNecesitamos decidir cuál fecha de todas las disponibles vamos a utilizar para agregar los casos. En este caso, la DGE sugiere utilizar la fecha de inicio de síntomas (FECHA_SINTOMAS) para construir la curva de casos confirmados y la de defunción (FECHA_DEF) para la curva de defunciones.\nEntonces, para construir la curva de confirmados lo primero que tenemos que hacer es indexar el DataFrame por la fecha de inicio de síntomas\n\nconfirmados = confirmados.set_index('FECHA_SINTOMAS')\nconfirmados.index\n\nDatetimeIndex(['2022-02-13', '2022-01-01', '2022-04-22', '2022-08-07',\n               '2022-01-10', '2022-01-14', '2022-12-01', '2022-05-25',\n               '2022-12-23', '2022-08-02',\n               ...\n               '2022-06-23', '2022-08-16', '2022-08-19', '2022-08-01',\n               '2022-07-05', '2022-07-08', '2022-09-05', '2022-06-19',\n               '2022-06-20', '2022-09-23'],\n              dtype='datetime64[ns]', name='FECHA_SINTOMAS', length=769894, freq=None)\n\n\nYa con los datos indexados es fácil construir agregados diarios, sólo tenemos que seleccionar qué columnas queremos agregar. Por lo pronto hagamos un conteo sólo de casos confirmados. Para eso sólo tenemos que agrupár el índice usando una frecuencia diaría y tomar el tamaño de los grupos (de alguna columna, realmente no importa cual).\n\nconfirmados_diarios = (confirmados\n                       .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                       .size() # Calculamos el tamaño de cada grupo\n                       .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                       .rename({0:'Confirmados'}, axis=1) # Le damos nombre a la columna que obtenemos\n                       )\nconfirmados_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      Confirmados\n    \n  \n  \n    \n      0\n      2022-01-01\n      6748\n    \n    \n      1\n      2022-01-02\n      6585\n    \n    \n      2\n      2022-01-03\n      10398\n    \n    \n      3\n      2022-01-04\n      9729\n    \n    \n      4\n      2022-01-05\n      10924\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      362\n      2022-12-29\n      715\n    \n    \n      363\n      2022-12-30\n      399\n    \n    \n      364\n      2022-12-31\n      258\n    \n    \n      365\n      2023-01-01\n      187\n    \n    \n      366\n      2023-01-02\n      80\n    \n  \n\n367 rows × 2 columns\n\n\n\nHay muchas formas de visualizar estos datos, la primera y más sencilla es utilizar los métodos que provee Pandas, por ejemplo:\n\nconfirmados_diarios.set_index('FECHA_SINTOMAS').plot()\n\n<AxesSubplot: xlabel='FECHA_SINTOMAS'>\n\n\n\n\n\nUna alternativa que nos provee herramientas interactivas para visualizar los datos y que es muy fácil de usar es Plotly. A través del módulo Plotly express podemos crear de forma muy simple gráficas que nos permitan interactuar con ellas.\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y=\"Confirmados\")\nfig.show()\n\n\n                                                \n\n\nComo pueden ver, fue muy simple hacer una gráfica con herramientas para pan y zoom. Estas herramientas hacen más fácil ver que los datos de casos confirmados contienen la mezcla de dos señales: una de alta frecuancia que representa la variación diaria, con una especie de periodicidad semanal y una seññal de baja frecuencia que contiene las olas epidémicas.\nLa señal de alta frecuencia contiene mucho ruido que corresponde a los ciclos de actualización de la información y que realmente nos dice poco de la tendencia de los datos. Una forma sencilla de filtrar este ruido es utilizando la media móvil. Para calcular este promedio, Pandas provee la función rolling\n\nconfirmados_diarios['Media Móvil'] = (confirmados_diarios\n                                      .rolling(window=7)\n                                      .mean())\nconfirmados_diarios.head(10)\n\n/tmp/ipykernel_89594/2896601829.py:3: FutureWarning:\n\nDropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['FECHA_SINTOMAS'], dtype='object')\n\n\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      Confirmados\n      Media Móvil\n    \n  \n  \n    \n      0\n      2022-01-01\n      6748\n      NaN\n    \n    \n      1\n      2022-01-02\n      6585\n      NaN\n    \n    \n      2\n      2022-01-03\n      10398\n      NaN\n    \n    \n      3\n      2022-01-04\n      9729\n      NaN\n    \n    \n      4\n      2022-01-05\n      10924\n      NaN\n    \n    \n      5\n      2022-01-06\n      9816\n      NaN\n    \n    \n      6\n      2022-01-07\n      11910\n      9444.285714\n    \n    \n      7\n      2022-01-08\n      11229\n      10084.428571\n    \n    \n      8\n      2022-01-09\n      11794\n      10828.571429\n    \n    \n      9\n      2022-01-10\n      19673\n      12153.571429\n    \n  \n\n\n\n\nY ahora la podemos graficar\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y='Media Móvil')\nfig.show()\n\n\n                                                \n\n\nPara graficar las dos series en la misma gráfica lo más sencillo es pasar los datos del formato ancho (en columnas) al formato largo (en filas con una columna que los distinga). Para esto vamos a usar la función melt de Pandas\n\n confirmados_diarios = confirmados_diarios.melt(id_vars=['FECHA_SINTOMAS'], value_vars=['Confirmados', 'Media Móvil'])\n confirmados_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      variable\n      value\n    \n  \n  \n    \n      0\n      2022-01-01\n      Confirmados\n      6748.000000\n    \n    \n      1\n      2022-01-02\n      Confirmados\n      6585.000000\n    \n    \n      2\n      2022-01-03\n      Confirmados\n      10398.000000\n    \n    \n      3\n      2022-01-04\n      Confirmados\n      9729.000000\n    \n    \n      4\n      2022-01-05\n      Confirmados\n      10924.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      2022-12-29\n      Media Móvil\n      1137.142857\n    \n    \n      730\n      2022-12-30\n      Media Móvil\n      1045.571429\n    \n    \n      731\n      2022-12-31\n      Media Móvil\n      934.571429\n    \n    \n      732\n      2023-01-01\n      Media Móvil\n      796.714286\n    \n    \n      733\n      2023-01-02\n      Media Móvil\n      573.714286\n    \n  \n\n734 rows × 3 columns\n\n\n\nCon los datos de esta forma, ahora podemos usar Plotly para graficar ambas variables utilizando como color la columna variable. El parámetro color nos permite separar dos series de datos cuando estas vienen en formato largo.\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y='value', color='variable')\nfig.show()\n\n\n                                                \n\n\n\n\n4.1.2 Curva de defunciones\nYa que construimos la curva de casos confirmados, la de defunciones es exáctamente igual, sólo necesitamos seleccionar al inicio del proceso los renglones que tengan una fecha de defunción válida e indexar por fecha de defunción\n\ndefunciones = confirmados.loc[confirmados['FECHA_DEF'].notnull()] # Seleccionamos los casos con fecha de defunción\ndefunciones = defunciones.set_index('FECHA_DEF') # indexamos por fecha de defunción\ndefunciones_diarios = (defunciones\n                       .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                       .size() # Calculamos el tamaño de cada grupo\n                       .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                       .rename({0:'Defunciones'}, axis=1) # Le damos nombre a la columna que obtenemos\n                       )\ndefunciones_diarios['Media Móvil'] = defunciones_diarios.rolling(window=7).mean() # Calculamos la media móvil\ndefunciones_diarios = defunciones_diarios.melt(id_vars=['FECHA_DEF'], value_vars=['Defunciones', 'Media Móvil']) # Pasamos al formato largo\nfig = px.line(defunciones_diarios, x='FECHA_DEF', y='value', color='variable')\nfig.show()\n\n/tmp/ipykernel_89594/3052884713.py:9: FutureWarning:\n\nDropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['FECHA_DEF'], dtype='object')\n\n\n\n\n                                                \n\n\n\n\n4.1.3 Combinando las dos gráficas\nPara entenmder la evolución de la epidemia conviene poder ver las dos gráficas al mismo tiempo y explorarlas de forma conjunta. En general este tipo de combinaciones en las que dos o más gráficas comparten por lo menos un eje (en nuestro caso el tiempo) se llaman Facetas. Plotly nos permite crear este tipo de visualizaciones de forma muy sencilla, lo que necesitamos es combinar ambos datos (casos y defunciones) en un sólo DataFrame en formato largo y asegurarnos de que cada fila pueda distinguir a qué se refiere. En nuestros datos vamos a tener cuatro series diferentes: datos crudos y media móvil para casos y definciones.\nComencemos con la serie de defunciones, lo primero que tenemos que hacer es agregar una columna con el tipo de serie, es decir, defunciones:\n\ndefunciones_diarios['Tipo'] = 'Defunciones'\ndefunciones_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_DEF\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-03\n      Defunciones\n      1.000000\n      Defunciones\n    \n    \n      1\n      2022-01-04\n      Defunciones\n      2.000000\n      Defunciones\n    \n    \n      2\n      2022-01-05\n      Defunciones\n      1.000000\n      Defunciones\n    \n    \n      3\n      2022-01-06\n      Defunciones\n      3.000000\n      Defunciones\n    \n    \n      4\n      2022-01-07\n      Defunciones\n      8.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n726 rows × 4 columns\n\n\n\nEn la columna variable tenemos los valores Defunciones y Media Móvil, necesitamos cambiar el valor de Defunciones por algo que sea compatible con tener los casois confirmados en el mismo DataFrame, pienses que al combinar ambas series queremos tener sólo dos valores diferentes en esta columna. Cambiemos entonces el valor de Defunciones por Conteo:\n\ndefunciones_diarios.loc[defunciones_diarios['variable'] == 'Defunciones', 'variable'] = 'Conteo'\ndefunciones_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_DEF\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-03\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      1\n      2022-01-04\n      Conteo\n      2.000000\n      Defunciones\n    \n    \n      2\n      2022-01-05\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      3\n      2022-01-06\n      Conteo\n      3.000000\n      Defunciones\n    \n    \n      4\n      2022-01-07\n      Conteo\n      8.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n726 rows × 4 columns\n\n\n\nAhora tenemos dos nombres diferentes para los campos con los que vamos a construir el eje de las X: FECHA_DEF y FECHA_SINTOMAS. Para poder combinar ambas series en una sola, necesitamos que esos campos tengan el mismo nombre en las dos series.\n\ndefunciones_diarios = defunciones_diarios.rename({'FECHA_DEF': 'Fecha'}, axis=1)\ndefunciones_diarios\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-03\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      1\n      2022-01-04\n      Conteo\n      2.000000\n      Defunciones\n    \n    \n      2\n      2022-01-05\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      3\n      2022-01-06\n      Conteo\n      3.000000\n      Defunciones\n    \n    \n      4\n      2022-01-07\n      Conteo\n      8.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n726 rows × 4 columns\n\n\n\nRepetimos el proceso para los datos de casos confirmados:\n\nconfirmados_diarios['Tipo'] = 'Casos Confirmados'\nconfirmados_diarios.loc[confirmados_diarios['variable'] == 'Confirmados', 'variable'] = 'Conteo'\nconfirmados_diarios = confirmados_diarios.rename({'FECHA_SINTOMAS': 'Fecha'}, axis=1)\nconfirmados_diarios\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      6748.000000\n      Casos Confirmados\n    \n    \n      1\n      2022-01-02\n      Conteo\n      6585.000000\n      Casos Confirmados\n    \n    \n      2\n      2022-01-03\n      Conteo\n      10398.000000\n      Casos Confirmados\n    \n    \n      3\n      2022-01-04\n      Conteo\n      9729.000000\n      Casos Confirmados\n    \n    \n      4\n      2022-01-05\n      Conteo\n      10924.000000\n      Casos Confirmados\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      2022-12-29\n      Media Móvil\n      1137.142857\n      Casos Confirmados\n    \n    \n      730\n      2022-12-30\n      Media Móvil\n      1045.571429\n      Casos Confirmados\n    \n    \n      731\n      2022-12-31\n      Media Móvil\n      934.571429\n      Casos Confirmados\n    \n    \n      732\n      2023-01-01\n      Media Móvil\n      796.714286\n      Casos Confirmados\n    \n    \n      733\n      2023-01-02\n      Media Móvil\n      573.714286\n      Casos Confirmados\n    \n  \n\n734 rows × 4 columns\n\n\n\nAhora que ambas series tienen la misma forma y columnas que distinguen los cuatro casos que nos interesan, sólo resta combinar las series. En este caso, lo que queremos es pegar los datos de una abajo de la otra (el orden da igual). Para esto vamos a usar la función concat que toma una lista de DataFrames y regresa un DataFrame concatenado a lo largo del eje que queramos.\n\ncasos_defunciones = pd.concat([confirmados_diarios, defunciones_diarios], axis=0)\ncasos_defunciones\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      6748.000000\n      Casos Confirmados\n    \n    \n      1\n      2022-01-02\n      Conteo\n      6585.000000\n      Casos Confirmados\n    \n    \n      2\n      2022-01-03\n      Conteo\n      10398.000000\n      Casos Confirmados\n    \n    \n      3\n      2022-01-04\n      Conteo\n      9729.000000\n      Casos Confirmados\n    \n    \n      4\n      2022-01-05\n      Conteo\n      10924.000000\n      Casos Confirmados\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n1460 rows × 4 columns\n\n\n\nYa con la nueva serie como la queremos, podemos hacer nuestras Facetas utlizando el parámetro facet_col que le dice a Plotly qué columna usar para distinguir las dos series. Es importante decirle que no queremos que compartan el eje \\(y\\) porque las escalas son muy diferentes\n\nfig = px.line(casos_defunciones, x='Fecha', y='value', color='variable', facet_col='Tipo', facet_col_wrap=1)\nfig.update_yaxes(matches=None)\nfig.show()\n\n\n                                                \n\n\n\n\n4.1.4 Hospitalizaciones\nOtra gráfica muy interesante para comprender la evolucióón de la epidemia es la de hospitalizaciones. Para obtener esta gráfica primero tenemos que seleccionar los pacientes confirmados como positivos a COVID-19 y que además fueron hospitalizados.\nLos casos confirmados ya los tenemos calculados en la variable confirmados, entonces falta ver cómo obtener los pacientes hospitalizados\n\nconfirmados.TIPO_PACIENTE.unique()\n\narray(['HOSPITALIZADO', 'AMBULATORIO'], dtype=object)\n\n\nGracias a nuentra base aplanada es muy fácil distinguirlos, entonces sólo los tenemos que seleccionar, agregar por día y podemos hacer una gráfica como las anteriores (incluyendo la media móvil). Recordemos que confirmados está indexado por fecha de inicio de síntomas, entonces nuestra curva de hospitalización estará indexada por la misma fecha\n\nhospitalizados = confirmados[confirmados.TIPO_PACIENTE == 'HOSPITALIZADO']\nhospitalizados_diarios = (hospitalizados\n                          .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                          .size() # Calculamos el tamaño de cada grupo\n                          .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                          .rename({0:'Hospitalizaciones'}, axis=1) # Le damos nombre a la columna que obtenemos\n                        )\nhospitalizados_diarios['Media Móvil'] = hospitalizados_diarios.rolling(window=7).mean()\nhospitalizados_diarios = hospitalizados_diarios.melt(id_vars=['FECHA_SINTOMAS'], value_vars=['Hospitalizaciones', 'Media Móvil'])\nfig = px.line(hospitalizados_diarios, x='FECHA_SINTOMAS', y='value', color='variable')\nfig.show()\n\n/tmp/ipykernel_89594/1651814779.py:8: FutureWarning:\n\nDropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['FECHA_SINTOMAS'], dtype='object')\n\n\n\n\n                                                \n\n\nY, una vez más, para comparar vamos a poner las tres gráficas (casos confirmados, defunciones y hospitalizacones) en un Facet\n\nhospitalizados_diarios['Tipo'] = 'Hospitalizaciones'\nhospitalizados_diarios.loc[hospitalizados_diarios['variable'] == 'Hospitalizaciones', 'variable'] = 'Conteo'\nhospitalizados_diarios = hospitalizados_diarios.rename({'FECHA_SINTOMAS': 'Fecha'}, axis=1)\nhospitalizados_diarios\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      117.000000\n      Hospitalizaciones\n    \n    \n      1\n      2022-01-02\n      Conteo\n      91.000000\n      Hospitalizaciones\n    \n    \n      2\n      2022-01-03\n      Conteo\n      124.000000\n      Hospitalizaciones\n    \n    \n      3\n      2022-01-04\n      Conteo\n      120.000000\n      Hospitalizaciones\n    \n    \n      4\n      2022-01-05\n      Conteo\n      144.000000\n      Hospitalizaciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      2022-12-29\n      Media Móvil\n      35.285714\n      Hospitalizaciones\n    \n    \n      730\n      2022-12-30\n      Media Móvil\n      33.857143\n      Hospitalizaciones\n    \n    \n      731\n      2022-12-31\n      Media Móvil\n      31.285714\n      Hospitalizaciones\n    \n    \n      732\n      2023-01-01\n      Media Móvil\n      27.857143\n      Hospitalizaciones\n    \n    \n      733\n      2023-01-02\n      Media Móvil\n      21.571429\n      Hospitalizaciones\n    \n  \n\n734 rows × 4 columns\n\n\n\nCombinamos con la serie de casos y defunciones\n\ncasos_defunciones_hospitalizaciones = pd.concat([hospitalizados_diarios, casos_defunciones], axis=0)\ncasos_defunciones_hospitalizaciones\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      117.000000\n      Hospitalizaciones\n    \n    \n      1\n      2022-01-02\n      Conteo\n      91.000000\n      Hospitalizaciones\n    \n    \n      2\n      2022-01-03\n      Conteo\n      124.000000\n      Hospitalizaciones\n    \n    \n      3\n      2022-01-04\n      Conteo\n      120.000000\n      Hospitalizaciones\n    \n    \n      4\n      2022-01-05\n      Conteo\n      144.000000\n      Hospitalizaciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n2194 rows × 4 columns\n\n\n\nGraficamos las tres series\n\nfig = px.line(casos_defunciones_hospitalizaciones, x='Fecha', y='value', color='variable', facet_col='Tipo', facet_col_wrap=1)\nfig.update_yaxes(matches=None)\nfig.show()"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#leyendo-datos-geográficos",
    "href": "parte_1/05_intro_geopandas.html#leyendo-datos-geográficos",
    "title": "5  Objetos geográficos",
    "section": "5.1 Leyendo datos geográficos",
    "text": "5.1 Leyendo datos geográficos\nA través del método read_file(), Geopandas nos permite leer archivos en una variedad de formatos. Vamos a comenzar por leer el shapefile de las AGEBS para la Ciudad de México.\n\nagebs = gpd.read_file(\"datos/agebs_cdmx.zip\")  # Importar los datos espaciales\nagebs\n\n\n\n\n\n  \n    \n      \n      CVEGEO\n      CVE_ENT\n      CVE_MUN\n      CVE_LOC\n      CVE_AGEB\n      geometry\n    \n  \n  \n    \n      0\n      0901000011716\n      09\n      010\n      0001\n      1716\n      POLYGON ((2787237.541 816989.461, 2787288.728 ...\n    \n    \n      1\n      0901000012150\n      09\n      010\n      0001\n      2150\n      POLYGON ((2794154.458 823013.444, 2794155.774 ...\n    \n    \n      2\n      0901000011133\n      09\n      010\n      0001\n      1133\n      POLYGON ((2795690.723 820050.788, 2795684.238 ...\n    \n    \n      3\n      0901000011307\n      09\n      010\n      0001\n      1307\n      POLYGON ((2792584.475 815678.668, 2792624.325 ...\n    \n    \n      4\n      0901000010281\n      09\n      010\n      0001\n      0281\n      POLYGON ((2788845.392 823526.074, 2788840.549 ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2426\n      0900700012564\n      09\n      007\n      0001\n      2564\n      POLYGON ((2814016.268 821043.511, 2814019.319 ...\n    \n    \n      2427\n      0900700012615\n      09\n      007\n      0001\n      2615\n      POLYGON ((2814358.791 820744.850, 2814405.087 ...\n    \n    \n      2428\n      0900700012969\n      09\n      007\n      0001\n      2969\n      POLYGON ((2815993.470 819777.763, 2816019.848 ...\n    \n    \n      2429\n      0900700013721\n      09\n      007\n      0001\n      3721\n      POLYGON ((2807966.150 821578.350, 2807941.550 ...\n    \n    \n      2430\n      0900700011034\n      09\n      007\n      0001\n      1034\n      POLYGON ((2808319.196 821552.683, 2808243.251 ...\n    \n  \n\n2431 rows × 6 columns\n\n\n\nComo pueden ver, el resultado es similar a un DataFrame, pero con la columna especial geometry\n\nagebs.geometry.head()\n\n0    POLYGON ((2787237.541 816989.461, 2787288.728 ...\n1    POLYGON ((2794154.458 823013.444, 2794155.774 ...\n2    POLYGON ((2795690.723 820050.788, 2795684.238 ...\n3    POLYGON ((2792584.475 815678.668, 2792624.325 ...\n4    POLYGON ((2788845.392 823526.074, 2788840.549 ...\nName: geometry, dtype: geometry\n\n\nEsta columna guarda las geometrías de nuestros datos. Una primera cosa que siempre queremos hacer cuando leemos una capa es visualizarla rápidamente, de la misma forma en la que lo hacemos en QGis o Arc. Para esto, los GeoDataFrames tienen el método plot que nos permite una exploración rápida.\n\nagebs.plot(figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nEsta es sólo una primera visualización rápida que nos permite asegurarnos explorar los datos, más addelante vamos a ver diferentes formas de mejorar estos mapas.\n\n5.1.0.1 Líneas\nDe la misma forma en que leimos un archivo con polígonos, podemos leer un archivo que contiene líneas, en este caso las calles de la delegación Cuahutemoc. Noten además que este archivo es un geojson y GeoPandas es capaz de inferir el tipo de archiovo a través de la extensión.\n\nvias = gpd.read_file(\"datos/vias_cuauhtemoc.geojson\") # Se importan los datos espaciales\nvias = vias.set_index('id')                         # Se establece una columna como índice\nvias.head()                                         # Visualizar los primeros registros de la fila\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n    \n      id\n      \n    \n  \n  \n    \n      1\n      LINESTRING (-99.17041 19.40092, -99.17047 19.4...\n    \n    \n      2\n      LINESTRING (-99.17850 19.40720, -99.17868 19.4...\n    \n    \n      3\n      LINESTRING (-99.14905 19.43796, -99.14871 19.4...\n    \n    \n      4\n      LINESTRING (-99.14735 19.44531, -99.14718 19.4...\n    \n    \n      5\n      LINESTRING (-99.17655 19.42105, -99.17645 19.4...\n    \n  \n\n\n\n\nAl igual que con los polígonos, es posible utilizar la función .plot() para graficar las líneas rápidamente:\n\nvias.plot(figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nUna vez más, estas no son las mejores visualizaciones pero nos ayudan a explorar rápidamente la capa.\n\n\n5.1.0.2 Puntos\nLas capas de puntos se comportan de la misma forma. Por ejemplo, leamos un shape con las estaciones de metro:\n\nestaciones = gpd.read_file(\"datos/estaciones_metro.zip\")\nestaciones.head()\n\n\n\n\n\n  \n    \n      \n      stop_lat\n      stop_lon\n      geopoint\n      agency_id\n      stop_id\n      stop_desc\n      stop_name\n      trip_heads\n      geometry\n    \n  \n  \n    \n      0\n      19.443082\n      -99.139034\n      (2:19.443082,-99.139034)\n      METRO\n      14169.0\n      Metro Línea 8 correspondencia con línea B.\n      Garibaldi_1\n      Garibaldi - Constitución de 1917\n      POINT (485405.843 2149860.572)\n    \n    \n      1\n      19.468965\n      -99.136176\n      (2:19.468965,-99.13617600000001)\n      METRO\n      14103.0\n      Metro Línea 5 correspondencia con línea 3.\n      La Raza_1_3\n      Pantitlán - Politécnico\n      POINT (485708.110 2152724.378)\n    \n    \n      2\n      19.376256\n      -99.187746\n      (2:19.37625563,-99.18774605)\n      METRO\n      14079.0\n      Metro Línea 7 correspondencia con línea 12.\n      Mixcoac_1\n      Tláhuac - Mixcoac\n      POINT (480284.558 2142470.874)\n    \n    \n      3\n      19.408944\n      -99.122279\n      (2:19.40894369,-99.12227869)\n      METRO\n      14144.0\n      Metro Línea 9 correspondencia con línea 4.\n      Jamaica\n      Tacubaya - Pantitlán\n      POINT (487161.939 2146081.726)\n    \n    \n      4\n      19.375679\n      -99.186866\n      (2:19.37567873,-99.18686628)\n      METRO\n      132131.0\n      Metro Línea 12 correspondencia con línea 7.\n      Mixcoac\n      Mixcoac - Tláhuac\n      POINT (480376.875 2142406.938)\n    \n  \n\n\n\n\nY la visualización se produce de forma idéntica a los casos anteriores:\n\nestaciones.plot(figsize = (10,10))\n\n<AxesSubplot: >"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#produciendo-mejores-mapas",
    "href": "parte_1/05_intro_geopandas.html#produciendo-mejores-mapas",
    "title": "5  Objetos geográficos",
    "section": "5.2 Produciendo mejores mapas",
    "text": "5.2 Produciendo mejores mapas\nLos mapas que hicimos hasta aquí sirven como forma de explorar los datos rápidamente, sin embargo, es posible producir mucho mejores visualizaciones de forma relativamente sencilla cambiando los parámetros que usamos para graficar.\n\n5.2.1 Transpariencia\nLa intensidad del color de un polígono puede ser cambiado a través del parámetro alpha del método plot(). Este parámetro es un valor entre cero y uno, donde el 0 representa transparencia completa y el 1 completa opacidad (máxima intensidad):\n\nagebs.plot(alpha = 0.5 , figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\n\n5.2.1.1 Eliminar Ejes\nLos mapas que hemos visto son, por debajo, gráficas de matplotlib, matplotlib está hecha para producir gráficas generales sobre una infinidad de temáticas. En general, entender una gráfica necesita información sobre la escala de variación de los datos, esta información suele proveerse a través de ejes. Por otro lado, en los mapas, a veces los ejes resultan redundantes y es mejor omitirlos.\nElimiar los ejes (y otras operaciones) involucra hablar con la figura que contiene nuestro mapa. Las gráficas de matplotlib están organizadas en una estructura jerárquica que, en su forma más sencilla contiene un objeto de tipo figure, adentro del cual hay uno o más objetos de tipo axes. Son estos últimos los que se encargan propiamente de dibujar la gráfica.\nPara poder manipular las propiedades de nuestros mapas (o gráficas en general), necesitamos tener acceso a los objetos figure y axes. La forma más simple es instanciándolos diréctamente a la hora de crear nuestra gráfica.\n\nfig , ax = plt.subplots(1, figsize=(10,10))  # Preparación de la Figura y sus Ejes, así como el tamaño\nagebs.plot(ax = ax)                          # Grafica la capa de polígonos sobre la fila\nax.set_axis_off()                            # Eliminar las ventanas de los ejes\nplt.show()                                     # Mostrar el resultado\n\n\n\n\nAnalicemos a detalle cada una de las líneas anteriores: 1. Creamos una figura con el nombre fig que contiene un sólo eje llamado ax. Para eso usamos la función subplots(). Este método genera dos elementos que pueden ser asignados a dos variables diferentes (fig y ax), lo cual se logra colocando sus nombres al inicio de la línea, separándolos por comas. 2. Graficamos las geometrías de la misma forma que en los ejemplos anteriores, esta vez indicándole a la función que dibuje los polígonos en el eje que generamos anteriormente, a través del argumento ax. 3. Finalmente, eliminamos los ejes llamando al método set_axis_off() del eje.\n\n\n\n5.2.2 Añadir un Título\nAsí como los ejes son parte del objeto axes, el título de la gráfica es propiedad del objeto figure. Para cambiarlo simplemente llamamos al método suptitle de la figura.\n\nfig , ax = plt.subplots(1, figsize=(10,10))\nagebs.plot(ax = ax)\nfig.suptitle(\"AGEB's de la CDMX\") # A través de la función '.suptitle()' aplicada a la figura se coloca el título.\nplt.show()\n\n\n\n\n\n5.2.2.1 Cambiar el Tamaño del Mapa\nEn los primeros ejemplos fijamos el tamaño de la gráfica a través del argumento figsize del m´todo plot() de nuestros GeoDataFrames. También podemos cambiarlo en el momento de crear las figuras.\n\nfig , ax = plt.subplots(1, figsize=(4,4))\nagebs.plot(ax = ax)\nfig.suptitle(\"AGEB's de la CDMX\")\nplt.show()\n\n\n\n\n\n\n\n5.2.3 Propiedades del dibujo\nHasta aquí hemos modificado algunas propiedades generales de la figura y los ejes. También queremos tener una forma de modoficar propiedades del dibujo como el estilo de las líneas y de los polígonos.\nCambiar estas propiedades se hace desde el método plot()y, aunque tienen unos nombres medio extraños, es muy fácil cambiarlos.\n\nfig , ax = plt.subplots(1, figsize=(10,10))\n# En la siguiente línea se modifica el color de los polígonos ('facecolor'), del borde ('edgecolor') y su ancho ('linewidth')\nagebs.plot(linewidth = 0.1, facecolor = '#50C879', edgecolor='#000702',ax = ax)\nplt.show()\n\n\n\n\nSimplemente modoficamos los valores de los parámetros linewidth, facecolor y edgecolor que modifican el ancho de la línea, el color del polígono y el color del borde respectivamente.\nDe la misma forma podemos cambiar los colores y ancho de una capa de líneas.\n\nfig , ax = plt.subplots(1, figsize=(10,10))\nvias.plot(linewidth = 0.5, color = 'red',ax = ax)\nplt.show()\n\n\n\n\nAhora para cambiar el color usamos la propiedad color, las líneas no tienen facecolor."
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#sistemas-de-coordenadas",
    "href": "parte_1/05_intro_geopandas.html#sistemas-de-coordenadas",
    "title": "5  Objetos geográficos",
    "section": "5.3 Sistemas de coordenadas",
    "text": "5.3 Sistemas de coordenadas\nUna de las características distintivas de los datos geográficos es el Sistema de Referencia espacial (CRS, en inglés). El CRS nos dice cómo están referidos los datos a la superficie de la tierra.\nGeopandas provee métodos para trabajar con sistemas de referencia. Primero, si la fuente de donde leímos los datos, tiene una referencia, esta se va a conservar a través de la propiedad crs:\n\nagebs.crs\n\n<Derived Projected CRS: PROJCS[\"MEXICO_ITRF_2008_LCC\",GEOGCS[\"ITRF2008\",DA ...>\nName: MEXICO_ITRF_2008_LCC\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: unnamed\n- method: Lambert Conic Conformal (2SP)\nDatum: International Terrestrial Reference Frame 2008\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nPara reproyectar nuestros datos a un sistema diferente se requiere conocer el código SRID del nuevo sistema. En el sitio Spatial Reference podemos buscar los códigos de diferentes proyecciones y sistemas de referencia.\nPor ejemplo, para cambiar la proyección de nuestros datos al sistema UTM, hacemos:\n\nagebs.to_crs(32614).plot(figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nDebido a que el área de trabajo es relativamente pequeña, la forma de los polígonos observados es casi idéntica a la observada en los ejemplos anteriores. Sin embargo, la escala utilizada para la gráfica es ahora diferente. ___ ### Para practicar\nGenera un mapa de las AGEB’s de la Ciudad de México que posea las siguientes características: * Posea un título * No muestre los ejes * Posea un tamaño de 10in x 11in * Todos los polígonos tengan un relleno del color “#525252” y sean completamente opacos * Los bordes del polígono tengan un ancho de 0.3 y sean del color “#B9EBE3” ___"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#múltiples-capas",
    "href": "parte_1/05_intro_geopandas.html#múltiples-capas",
    "title": "5  Objetos geográficos",
    "section": "5.4 Múltiples Capas",
    "text": "5.4 Múltiples Capas\nPor lo pronto nuestros mapas se han limitado a una sola capa, sin embargo es bastante común querer sobreponer visualmente diferentes capas de información. Hacer esto con Geopandas es muy sencillo, simplemente necesitamos agragar los nuevos dibujos a los ejes ya existentes. Por ejemplo, podemos combinar en un sólo mapa las vialidades de la alcaldía Cuahutemoc con sus límites.\n\n# Primero se necesita importar el polígono de la alcaldía, pues no se ha utilizado en ejemplos anteriores\ncuauhtemoc = gpd.read_file(\"datos/cuauhtemoc.geojson\")\n# Se establece la figura y su eje con '.subplots()'\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade primero el polígono base en la fila\ncuauhtemoc.plot(ax = ax)\n# Y después se colocan las vialidades en la misma fila\nvias.plot(ax = ax)\n# Por último, y como se ha hecho en ejemplos anteriores, el comando para mostrar el resultado\nplt.show()\n\n\n\n\nSe alcanza a ver que ahí debajo del polígono están las líneas, pero claramente necesitamos trabajar más el mapa. Cambiemos los colores y la transparencia.\n\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Para el polígono, se utilizan los argumentos aplicables al polígono\ncuauhtemoc.plot(ax = ax, facecolor = 'grey', alpha = 0.8)\n# Y pra las líneas, se utilizan los argumentos usados para las líneas\nvias.plot(ax = ax, color = 'white', linewidth = 0.5)\nplt.show()"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#paletas-de-colores",
    "href": "parte_1/05_intro_geopandas.html#paletas-de-colores",
    "title": "5  Objetos geográficos",
    "section": "5.5 Paletas de Colores",
    "text": "5.5 Paletas de Colores\nLa elección de colores en un mapa está muy relacionada con la efectividad para comunicar. Aunque en algunos casos elegir estos colores a mano sea suficiente, a veces es conveniente recurrir a paletas ya previamente diseñadas (por ejemplo, para dseñar mapas accesibles a personas daltónicas).\nEn este caso vamos a usar la librería palettable que nos provee con diferentes paletas de colores.\nLlamemos entonces a una paleta de colores y guardemosla en una variable.\n\npaleta = pltt.wesanderson.Darjeeling2_5.hex_colors\n\nPara visualizar la apariencia de esta paleta, se recurre a la función palplot() de seaborn:\n\npalplot(paleta)\n\n\n\n\nSi se ve la variable por sí misma, podrá notarse que no se trata más que de una lista de colores bajo su Código Hexadecimal:\n\npaleta\n\n['#D5E3D8', '#618A98', '#F9DA95', '#AE4B16', '#787064']\n\n\nEsta paleta será la que le dará color a nuestro mapa. Como elemento adicional, se colocarán las colonias en las que se subdivide la alcaldía y en donde se han registrado 10 o más homicidios entre los años 2016 a 2018, por lo que primero es necesario importarlas:\n\nagebs_cuau = gpd.read_file(\"datos/agebs_cuauhtemoc.geojson\")\n\nPara esto, no se utilizará nada más que lo aprendido en secciones anteriores de la práctica, siendo la única diferencia que dentro de los argumentos de color correspondientes se colocará el Código Hexadecimal del color que se pretenda utilizar para esa capa:\n\n# Se define la figura con sus respectivas filas\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade el polígono base de la alcaldía\ncuauhtemoc.plot(ax = ax, facecolor = '#F9DA95', edgecolor= '#787064', linewidth = 3)\n# Se añaden las vías\nvias.plot(ax = ax, color = '#AE4B16', linewidth = 0.5)\n# Se aladen las AGEB's con el mayor número de homicidios\nagebs_cuau.plot(ax = ax, facecolor = '#D5E3D8', edgecolor = '#618A98', linewidth = 2, alpha = 0.8)\n# Se remueve el marco con los ejes de la fila\nax.set_axis_off()\nplt.show()"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#exportar-mapas",
    "href": "parte_1/05_intro_geopandas.html#exportar-mapas",
    "title": "5  Objetos geográficos",
    "section": "5.6 Exportar Mapas",
    "text": "5.6 Exportar Mapas\nUna vez producido el mapa final, puede que se busque el exportar la imagen de modo que pueda ser colocada en un reporte, artículo, sitio web, etc. Para exportar mapas en Python, basta con sustituir la función plt.show() por plt.savefig() al final de las líneas de código para especificar el dónde y cómo guardarla. Por ejemplo, para guardar el mapa anterior en un archivo de tipo .png en la carpeta data, donde se encuentra toda la información con la que se ha trabajado hasta ahora, simplemente basta con:\n\n# Se define la figura con sus respectivas filas\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade el polígono base de la alcaldía\ncuauhtemoc.plot(ax = ax, facecolor = '#F9DA95', edgecolor= '#787064', linewidth = 3)\n# Se añaden las vías\nvias.plot(ax = ax, color = '#AE4B16', linewidth = 0.5)\n# Se aladen las AGEB's con el mayor número de homicidios\nagebs_cuau.plot(ax = ax, facecolor = '#D5E3D8', edgecolor = '#618A98', linewidth = 2, alpha = 0.8)\n# Se remueve el marco con los ejes de la fila\nax.set_axis_off()\n\n# Se guarda el mapa como un archivo PNG en la carpeta 'data/'\nplt.savefig('datos/mapa_final.png')\n\n\n\n\nSi se revisa la carpeta, se encontrará la imagen .png con el mapa.\nLa función plt.savefig(), de la librería matplotlib.pyplot contiene un gran número de parámetros y opciones para trabajar. Dado que el tamaño del mapa generado no es muy grande, es posible incrementar la calidad de éste a través del argumento dpi, o Puntos Por Pulgada (dpi), el cual es una medida estándar de la resolución de las imágenes. Por ejemplo, para obtener una imagen de Alta Definición (HD), se puede cambiar el argumento a 1,080:\nImportante - Si el proceso tarda demasiado, cambiar el argumento por 500 también funciona, pues también arrojará una imagen de buena calidad y más fácil de generar.\n\n# Se define la figura con sus respectivas filas\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade el polígono base de la alcaldía\ncuauhtemoc.plot(ax = ax, facecolor = '#F9DA95', edgecolor= '#787064', linewidth = 3)\n# Se añaden las vías\nvias.plot(ax = ax, color = '#AE4B16', linewidth = 0.5)\n# Se aladen las AGEB's con el mayor número de homicidios\nagebs_cuau.plot(ax = ax, facecolor = '#D5E3D8', edgecolor = '#618A98', linewidth = 2, alpha = 0.8)\n# Se remueve el marco con los ejes de la fila\nax.set_axis_off()\n\n# Se guarda el mapa como un archivo PNG en la carpeta 'data/'\nplt.savefig('datos/mapa_final.png', dpi = 1080)"
  },
  {
    "objectID": "parte_1/06_manipulacion_espacial.html#manipulación-de-datos-espaciales-geodataframes",
    "href": "parte_1/06_manipulacion_espacial.html#manipulación-de-datos-espaciales-geodataframes",
    "title": "6  Trabajando con objetos espaciales",
    "section": "6.1 Manipulación de Datos Espaciales (GeoDataFrames)",
    "text": "6.1 Manipulación de Datos Espaciales (GeoDataFrames)\nUna vez teniendo el conocimiento de cómo visualizar la información espacial, es posible estudiar cómo puede ser combinada con las operaciones aprendidas en sesiones pasadas sobre la manipulación de datos tabulares no espaciales. En esencia, la clave es entender que un GeoDataFrame contiene la mayoría de su información espacial en una sola columna llamada geometry, teniendo el resto de la tabla la misma apariencia y comportamiento que un DataFrame no espacial, lo que les concede toda la flexibilidad y conveniencia estudiada en la manipulación, segmentación y transformación de datos tabulares; además de ello, la naturaleza de un GeoDataFrame también incorpora un conjunto de operaciones explícitamente espaciales para combinar y transformar los datos. En esta sección, ambos tipos de operaciones serán consideradas.\nPrimero, vale la pena repasar algunas de las técnicas aprendidas en sesiones pasadas sobre datos no espaciales, y entender cómo pueden ser combinados con el mapeo de sus contrapartes espaciales. Para esto, se utilizarán los datos de población abordados anteriormente:\n\nimport pandas as pd\n\n# Importar la tabla\npoblacion = pd.read_csv(f + 'poblacion_cdmx.csv' , index_col = 'ageb_urbana_cvegeo')\n\n# Visualizar la tabla\npoblacion.head()\n\n\n\n\n\n  \n    \n      \n      pob_0a2\n      pob_3a5\n      pob_6a11\n      pob_12a14\n      pob_15a17\n      pob_18a24\n      pob_25a29\n      pob_30a49\n      pob_50a59\n      pob_60ymas\n    \n    \n      ageb_urbana_cvegeo\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0900200010010\n      71\n      77\n      235\n      129\n      137\n      298\n      182\n      1055\n      293\n      822\n    \n    \n      0900200010025\n      206\n      241\n      466\n      212\n      261\n      622\n      426\n      1705\n      699\n      974\n    \n    \n      090020001003A\n      157\n      137\n      347\n      202\n      197\n      478\n      273\n      1395\n      370\n      902\n    \n    \n      0900200010044\n      148\n      162\n      391\n      178\n      203\n      459\n      365\n      1422\n      469\n      993\n    \n    \n      0900200010097\n      56\n      82\n      150\n      80\n      88\n      202\n      157\n      626\n      248\n      523\n    \n  \n\n\n\n\n\n6.1.1 Unión de Tablas\nEn la práctica se han importado dos tablas importantes: por una parte, toda la información espacial está contenida en la tabla agebs, mientras que los datos tabulares se encuentran contenidos en poblacion. Para trabajar con ambos, es necesario conectar las tablas; en idioma de pandas, esto recibe el nombre de join (Unión), siendo los elementos claves para la unión los índices de cada tabla, esto es, los nombres asignados a cada fila; esto es lo que se determina utilizando el argumento index_col al momento de importar un .csv.\nTanto en el caso de agebs como de poblacion, el índice corresponde a la Clave Geográfica (CVEGEO) de cada una de las AGEB’s, lo cual permite ejecutar el join de la siguiente manera:\n\nageb_pob = agebs.join(poblacion , on = 'ageb_urbana_cvegeo')\nageb_pob.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      pob_0a2\n      pob_3a5\n      pob_6a11\n      pob_12a14\n      pob_15a17\n      pob_18a24\n      pob_25a29\n      pob_30a49\n      pob_50a59\n      pob_60ymas\n    \n    \n      ageb_urbana_cvegeo\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0900700013628\n      MULTIPOLYGON (((-99.03887 19.39128, -99.03851 ...\n      40\n      45\n      104\n      48\n      49\n      126\n      96\n      345\n      144\n      133\n    \n    \n      0900300011533\n      MULTIPOLYGON (((-99.18010 19.30772, -99.17845 ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      0901500010235\n      MULTIPOLYGON (((-99.14495 19.45625, -99.14536 ...\n      29\n      37\n      87\n      57\n      47\n      152\n      111\n      396\n      178\n      229\n    \n    \n      0900200010097\n      MULTIPOLYGON (((-99.20573 19.50454, -99.20630 ...\n      56\n      82\n      150\n      80\n      88\n      202\n      157\n      626\n      248\n      523\n    \n    \n      0900200011184\n      MULTIPOLYGON (((-99.20723 19.50387, -99.20652 ...\n      38\n      54\n      84\n      50\n      65\n      144\n      112\n      416\n      187\n      239\n    \n  \n\n\n\n\nAnalizando un poco la lógica detrás de la operación .join(): * En primer lugar, se trata de una operación en la que se está “anexando” un conjunto de datos a uno ya existente. En este caso en particular, se anexan los datos de población contenidos en la tabla poblacion a la tabla espacial agebs. * La tabla que se está anexando (población) necesita tener sus índices bien definidos para que la operación pueda proceder correctamente. En este caso, los índices fueron asignados desde la importación de la tabla, por lo que se trata de un requerimiento ya cumplido. * La función .join() posee dos argumentos: el primero, referente a la tabla que se está anexando; el segundo, determinado por on, determina el nombre de la columna o del índice de la tabla principal que servirá como referencia para la unión.\n\n\n6.1.2 Manipulaciones No Espaciales\nUna vez unidos los datos espaciales con los no espaciales, pueden utilizarse las técnicas aprendidas en la manipulación y segmentación de tablas no espaciales para crear mapas más complejos. En particular, se utilizarán las funciones requeridas para seleccionar filas basándose en las características de su población, añadiendo la posibilidad de visualizarlas en un mapa.\nEn primer lugar, puede generarse nuevamente la columna que contiene la Población Total de cada AGEB replicando los comandos ya aprendidos:\n\nageb_pob['pob_total'] = poblacion.sum(axis = 1)\nageb_pob.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      pob_0a2\n      pob_3a5\n      pob_6a11\n      pob_12a14\n      pob_15a17\n      pob_18a24\n      pob_25a29\n      pob_30a49\n      pob_50a59\n      pob_60ymas\n      pob_total\n    \n    \n      ageb_urbana_cvegeo\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0900700013628\n      MULTIPOLYGON (((-99.03887 19.39128, -99.03851 ...\n      40\n      45\n      104\n      48\n      49\n      126\n      96\n      345\n      144\n      133\n      1130\n    \n    \n      0900300011533\n      MULTIPOLYGON (((-99.18010 19.30772, -99.17845 ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      0901500010235\n      MULTIPOLYGON (((-99.14495 19.45625, -99.14536 ...\n      29\n      37\n      87\n      57\n      47\n      152\n      111\n      396\n      178\n      229\n      1323\n    \n    \n      0900200010097\n      MULTIPOLYGON (((-99.20573 19.50454, -99.20630 ...\n      56\n      82\n      150\n      80\n      88\n      202\n      157\n      626\n      248\n      523\n      2212\n    \n    \n      0900200011184\n      MULTIPOLYGON (((-99.20723 19.50387, -99.20652 ...\n      38\n      54\n      84\n      50\n      65\n      144\n      112\n      416\n      187\n      239\n      1389\n    \n  \n\n\n\n\nPosteriormente, se seleccionan las 10 AGEB’s con la mayor cantidad de población, a través de la función .sort_values() y añadiendo el valor de la selección deseado a .head():\n\nagebs_mas_pobladas = ageb_pob.sort_values('pob_total', ascending = False).head(10)\n\nAhora, puede crearse un mapa de la Ciuda de México y sobreponer como una nueva capa estas áreas:\n\nfig, fila = plt.subplots(1, figsize=(10, 10))\nageb_pob.plot(facecolor='black', linewidth=0.025, ax=fila)              # Capa Base de AGEB's\nagebs_mas_pobladas.plot(alpha=1, facecolor='red', linewidth=0, ax=fila) # Capa AGEB's más pobladas\nfila.set_axis_off()\nfig.suptitle(\"AGEB's de la CDMX con Mayor Población\")\nplt.show()\n\n\n\n\n\n\n6.1.3 Manipulaciones Espaciales\nAdemás de las operaciones basadas únicamente en valores, como las realizadas anteriormente, es posible realizar sobre un GeoDataFrame una gran variedad de operaciones encontradas en los SIG. A continuación se detallarán algunas de las más comunes\n\n6.1.3.1 Cálculo de Centroides\nEn algunos casos, resulta útil simplificar un polígono en un sólo punto y, para ello, se calculan los Centroides (siendo algo como el análogo espacial de la media estadística). El siguiente comando dará como resultado un objeto del tipo GeoSeries (una sola columna con datos espaciales) con los Centroides de los polígonos contenidos en un GeoDataFrame:\n\ncentroides = ageb_pob.centroid\ncentroides.head()\n\n/tmp/ipykernel_498/4168445328.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  centroides = ageb_pob.centroid\n\n\nageb_urbana_cvegeo\n0900700013628    POINT (-99.03839 19.39066)\n0900300011533    POINT (-99.17801 19.30761)\n0901500010235    POINT (-99.14579 19.45542)\n0900200010097    POINT (-99.20697 19.50685)\n0900200011184    POINT (-99.20608 19.50239)\ndtype: geometry\n\n\nEs importante destacar que centroides no es una tabla completa, sino una sola columna, esto es, un objeto del tipo GeoSeries; esto brinda la posibilidad de graficarlo de forma similar a como se ha hecho anteriormente:\n\ncentroides.plot(figsize = (10,10))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f08bb39ce80>\n\n\n\n\n\nEn el caso de un objeto del tipo GeoSeries, ya no es necesario hacer referencia a la columna geometry para inspeccionar su contenido, ya que, al tratarse de una columna de datos espaciales, el objetivo es en sí la geometría.\n\n\n6.1.3.2 Point-In-Polygon (PiP)\nEl saber si un punto se encuentra dentro de un polígono es un ejercicio simple desde el punto de vista conceptual; sin embargo, al momento de trasladarlo a la programación, puede tratarse de una acción difícil de completar. La forma más sencilla de realizar esto en GeoPandas es a través del método .contains(), disponible para todo polígono.\n\npoligono = ageb_pob['geometry'][0]  # Se aisla el primer polígono dela tabla\npunto1 = centroides[0]              # Se aisla el primer punto de la serie\npunto2 = centroides[1]              # Se aisla el segundo punto de la serie\n\n\npoligono.contains(punto1)\n\nTrue\n\n\n\npoligono.contains(punto2)\n\nFalse\n\n\nEl método anterior permite realizar una verificación rápida y cualitativa de si un punto se encuentra dentro de un polígono; sin embargo, en muchos otros casos esto no resulta ser muy eficiente, por lo que se recurre a una operación conocida como Spatial Join; éstos serán estudiados más a fondo en futuras prácticas.\n\n\n6.1.3.3 Buffers\nLos Buffers son parte de las operaciones clásicas de un SIG, y consisten en trazar un área alrededor de una geometría en particular, dado un radio específico. Éstos resultan bastate útiles al momento de combinarlos, por ejemplo, con operaciones de Point-In-Polygon para calcular valores de accesibilidad, áreas de influencia, entre otros.\nPara crear un Buffer a través de GeoPandas, puede utilizarse el método .buffer(), al cual se le coloca como argumento el radio deseado. Es importante tomar en cuenta que el radio especificado necesita encontrarse en las mismas unidades que el Sistema de Coordenadas de Referencia (CRS) de la geometría con la que se esté trabajando. Por ejemplo, revisando la capa importada anteriormente de Estaciones del Metro:\n\nestaciones.crs\n\n<Derived Projected CRS: EPSG:32614>\nName: WGS 84 / UTM zone 14N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 102°W and 96°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Manitoba; Nunavut; Saskatchewan. Mexico. United States (USA).\n- bounds: (-102.0, 0.0, -96.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 14N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLa propiedad crs indica que se trata de la proyección con Código EPSG 32614, de la cual, al investigar sobre ella, se tiene que se trata de una proyección que trabaja en metros. Como tal, si se buscara generar un Buffer de 500m alrededor de cada estación, simplemente se tendría que:\n\nbuff = estaciones.buffer(500)\nbuff.head()\n\n0    POLYGON ((485905.843 2149860.572, 485903.435 2...\n1    POLYGON ((486208.110 2152724.378, 486205.702 2...\n2    POLYGON ((480784.558 2142470.874, 480782.151 2...\n3    POLYGON ((487661.939 2146081.726, 487659.532 2...\n4    POLYGON ((480876.875 2142406.938, 480874.468 2...\ndtype: geometry\n\n\nPara representar éstos en un mapa, se recurre a los métodos estudiados anteriormente:\n\nfig, fila = plt.subplots(1, figsize=(10, 10))\n\n# Graficar los Buffers\nbuff.plot(ax = fila , alpha = 0.5 , facecolor = 'red', linewidth = 0)\n\n# Graficar las Estaciones de Metro sobre las referencias\nestaciones.plot(ax = fila , color = 'green')\nplt.show()\n\n\n\n\n\n\n6.1.3.4 Ejercicio Opcional\nGenera un mapa de la Ciudad de México donde los polígonos de las AGEB’s sean de color negro, y sobre ellos y de color amarillo los Buffers a 250m de cada uno de sus centroides.\n\n\n\n6.1.4 Coropletas\nPara terminar la práctica de geovisualización vamos a hacer mapas de coropletas usando dos métodos diferentes. Primero vamos a usar directamente GeoPandas para generar los mapas y después vamos a usar ipyLeaflet para hacer una visualización interactiva.\nPrimero, recordemos que ya tenemos los datos de población unidos a las geometrías de las AGEBS\n\nageb_pob\n\n\n\n\n\n  \n    \n      \n      geometry\n      pob_0a2\n      pob_3a5\n      pob_6a11\n      pob_12a14\n      pob_15a17\n      pob_18a24\n      pob_25a29\n      pob_30a49\n      pob_50a59\n      pob_60ymas\n      pob_total\n    \n    \n      ageb_urbana_cvegeo\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0900700013628\n      MULTIPOLYGON (((-99.03887 19.39128, -99.03851 ...\n      40\n      45\n      104\n      48\n      49\n      126\n      96\n      345\n      144\n      133\n      1130\n    \n    \n      0900300011533\n      MULTIPOLYGON (((-99.18010 19.30772, -99.17845 ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      0901500010235\n      MULTIPOLYGON (((-99.14495 19.45625, -99.14536 ...\n      29\n      37\n      87\n      57\n      47\n      152\n      111\n      396\n      178\n      229\n      1323\n    \n    \n      0900200010097\n      MULTIPOLYGON (((-99.20573 19.50454, -99.20630 ...\n      56\n      82\n      150\n      80\n      88\n      202\n      157\n      626\n      248\n      523\n      2212\n    \n    \n      0900200011184\n      MULTIPOLYGON (((-99.20723 19.50387, -99.20652 ...\n      38\n      54\n      84\n      50\n      65\n      144\n      112\n      416\n      187\n      239\n      1389\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      0901700011469\n      MULTIPOLYGON (((-99.06175 19.42993, -99.06174 ...\n      88\n      79\n      165\n      64\n      84\n      199\n      132\n      367\n      117\n      61\n      1356\n    \n    \n      0901700011473\n      MULTIPOLYGON (((-99.06098 19.42894, -99.06098 ...\n      81\n      81\n      127\n      71\n      61\n      175\n      100\n      332\n      98\n      53\n      1179\n    \n    \n      0901700011488\n      MULTIPOLYGON (((-99.05931 19.42789, -99.05934 ...\n      80\n      83\n      133\n      62\n      72\n      185\n      128\n      317\n      134\n      51\n      1245\n    \n    \n      0901700011492\n      MULTIPOLYGON (((-99.05931 19.42789, -99.05794 ...\n      67\n      52\n      122\n      54\n      59\n      139\n      94\n      271\n      66\n      59\n      983\n    \n    \n      0901700011524\n      MULTIPOLYGON (((-99.05709 19.42377, -99.05723 ...\n      191\n      211\n      434\n      235\n      233\n      559\n      343\n      1388\n      419\n      537\n      4550\n    \n  \n\n2432 rows × 12 columns\n\n\n\nHacer un primer mapa sencillo usando GeoPandas es tan sencillo como pasarle el nombre de la columna que queremos usar para colorear el mapa\n\nageb_pob.plot('pob_total', figsize=(10,10))\n\n<AxesSubplot:>\n\n\n\n\n\nIncluir una leyenda también es muy sencillo\n\nageb_pob.plot('pob_total', figsize=(10,10), legend=True)\n\n<AxesSubplot:>\n\n\n\n\n\nY cambiar la paleta de colores\n\nageb_pob.plot('pob_total', figsize=(10,10), legend=True, cmap='OrRd')\n\n<AxesSubplot:>\n\n\n\n\n\nEn la documentación de GeoDtaFrame.plot() pueden ver la lista completa de opciones.\nEn estos primeros mapas que hemos hecho usamos una escala continua para representar la variable de interés. Otra forma de representar la variación espacial es utilizando un esquema de clasificación discreto sobre nuestra variable de interés, por ejemplo cuantiles, intervalos iguales, etcétera. Para esto, GeoDtaFrame.plot() admite pasarle el parámetro scheme que toma cualquier esquema de clasificación admitido por mapclassify.\n\nageb_pob.plot('pob_total', figsize=(10,10), legend=True, cmap='OrRd', scheme='quantiles')\n\n<AxesSubplot:>\n\n\n\n\n\n\n6.1.4.1 Ejercicio\nPrueben diferentes esquemas de clasificación (aquí pueden encontrar la lista de esquemas disponibles) y discutanb sobre qué esquema representa mejor la variación espacial de los datos\n\n\n\n6.1.5 Mejorando el estilo\nLos mapas que hemos hecho son relativamente sencillos, para darles una mejor presentación podemos tomar algunas cosas que ya hemos aprendido, por ejemplo eliminar los ejes y ponerles un título:\n\nfig , fila = plt.subplots(1, figsize=(10,10))\nageb_pob.plot('pob_total', figsize=(10,10), legend=True, cmap='OrRd', scheme='quantiles', ax=fila)\nfig.suptitle(\"Población por AGEB\")\nfila.set_axis_off()  \nplt.show()\n\n\n\n\nTambién es posible agregar un mapa base utilizando la librería contextily\n\nimport contextily as ctx\n\n\nfig , fila = plt.subplots(1, figsize=(10,10))\nageb_pob.plot('pob_total', figsize=(10,10), legend=True, cmap='OrRd', scheme='quantiles', ax=fila, alpha=0.8)\nfig.suptitle(\"Población por AGEB\")\nfila.set_axis_off()\nctx.add_basemap(fila, source=ctx.providers.OpenStreetMap.Mapnik, crs=ageb_pob.crs.to_string())\nplt.show()\n\n\n\n\n\n6.1.5.1 Ejercicio\nPrueben usando diferentes mapas base, y cambiando el fondo de la imágen.\n\n\n\n6.1.6 Varios mapas en la misma figura\nA veces queremos hacer una imagen que nos permita comparar rápidamente una variable, para eso es conveniente poner juntos dos mapas. Pensemos en este momento en comparar la población por AGEB con la densidad de población por AGEB. Obviamente el pri er paso es calcular la densidad de población, para esto vamos a usar la propiedad área de los GeoDataFrames. Fíjense como reproyectamos antes de calcular el area para obtener un valor en metros cuadrados\n\nageb_pob.to_crs(32614).area\n\nageb_urbana_cvegeo\n0900700013628     37323.787352\n0900300011533     91454.392482\n0901500010235     31110.848381\n0900200010097    178129.071473\n0900200011184     48626.677816\n                     ...      \n0901700011469     22976.721781\n0901700011473     19149.811749\n0901700011488     22732.498079\n0901700011492     25787.042294\n0901700011524    223050.891018\nLength: 2432, dtype: float64\n\n\n\nageb_pob['densidad_pob'] = ageb_pob['pob_total'] / ageb_pob.to_crs(32614).area\nageb_pob.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      pob_0a2\n      pob_3a5\n      pob_6a11\n      pob_12a14\n      pob_15a17\n      pob_18a24\n      pob_25a29\n      pob_30a49\n      pob_50a59\n      pob_60ymas\n      pob_total\n      densidad_pob\n    \n    \n      ageb_urbana_cvegeo\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0900700013628\n      MULTIPOLYGON (((-99.03887 19.39128, -99.03851 ...\n      40\n      45\n      104\n      48\n      49\n      126\n      96\n      345\n      144\n      133\n      1130\n      0.030276\n    \n    \n      0900300011533\n      MULTIPOLYGON (((-99.18010 19.30772, -99.17845 ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0.000000\n    \n    \n      0901500010235\n      MULTIPOLYGON (((-99.14495 19.45625, -99.14536 ...\n      29\n      37\n      87\n      57\n      47\n      152\n      111\n      396\n      178\n      229\n      1323\n      0.042525\n    \n    \n      0900200010097\n      MULTIPOLYGON (((-99.20573 19.50454, -99.20630 ...\n      56\n      82\n      150\n      80\n      88\n      202\n      157\n      626\n      248\n      523\n      2212\n      0.012418\n    \n    \n      0900200011184\n      MULTIPOLYGON (((-99.20723 19.50387, -99.20652 ...\n      38\n      54\n      84\n      50\n      65\n      144\n      112\n      416\n      187\n      239\n      1389\n      0.028565\n    \n  \n\n\n\n\nAhora sí podemos hacer una figura que incluya dos mapas. Hasta ahora siempre hemos usado fig , fila = plt.subplots(1, figsize=(10,10)) para hacer un subplot con una sóla gráfica, lo que vamos a hacer es usarlo ahora para obtener dos ejes en los que graficar.\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12)) # Pedimos subfiguras con un renglón y dos columnas\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))# Pedimos subfiguras con un renglón y dos columnas\nageb_pob.plot('pob_total', legend=True, cmap='OrRd', scheme='quantiles', ax=ax1, alpha=0.8)# Graficamos en el primer ax\nax1.set_title(\"Población por AGEB\") # Este título está en el nivel ax, no en el figure\nax1.set_axis_off()\nctx.add_basemap(ax1, source=ctx.providers.OpenStreetMap.Mapnik, crs=ageb_pob.crs.to_string())\nageb_pob.plot('densidad_pob', legend=True, cmap='OrRd', scheme='quantiles', ax=ax2, alpha=0.8)# Graficamos en el segndo ax\nax2.set_title(\"Densidad de Población por AGEB\") # Este título está en el nivel ax, no en el figure\nax2.set_axis_off()\nctx.add_basemap(ax2, source=ctx.providers.OpenStreetMap.Mapnik, crs=ageb_pob.crs.to_string())\nplt.tight_layout() # Para disminuir el espacio entre gráficas\n\n\n\n\nComo pueden ver es relatívamente fácil poner dos gráficas, sólo hay que especificar cómo las queremos organizar y entender que subplots nos va a regresar un array de ejes. Para que quede aún más claro, vamos a hacer ahora cuatro mapas, comparando los porcentajes de poblaión para distintos grupos de edad. Primero vamos a calcular las variables que vamos a mapear\n\nageb_pob['prop_0a2'] = ageb_pob['pob_0a2'] / ageb_pob['pob_total']\nageb_pob['prop_12a14'] = ageb_pob['pob_12a14'] / ageb_pob['pob_total']\nageb_pob['prop_18a24'] = ageb_pob['pob_18a24'] / ageb_pob['pob_total']\nageb_pob['prop_60ymas'] = ageb_pob['pob_60ymas'] / ageb_pob['pob_total']\nageb_pob.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      pob_0a2\n      pob_3a5\n      pob_6a11\n      pob_12a14\n      pob_15a17\n      pob_18a24\n      pob_25a29\n      pob_30a49\n      pob_50a59\n      pob_60ymas\n      pob_total\n      densidad_pob\n      prop_0a2\n      prop_12a14\n      prop_18a24\n      prop_60ymas\n    \n    \n      ageb_urbana_cvegeo\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0900700013628\n      MULTIPOLYGON (((-99.03887 19.39128, -99.03851 ...\n      40\n      45\n      104\n      48\n      49\n      126\n      96\n      345\n      144\n      133\n      1130\n      0.030276\n      0.035398\n      0.042478\n      0.111504\n      0.117699\n    \n    \n      0900300011533\n      MULTIPOLYGON (((-99.18010 19.30772, -99.17845 ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0.000000\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      0901500010235\n      MULTIPOLYGON (((-99.14495 19.45625, -99.14536 ...\n      29\n      37\n      87\n      57\n      47\n      152\n      111\n      396\n      178\n      229\n      1323\n      0.042525\n      0.021920\n      0.043084\n      0.114890\n      0.173091\n    \n    \n      0900200010097\n      MULTIPOLYGON (((-99.20573 19.50454, -99.20630 ...\n      56\n      82\n      150\n      80\n      88\n      202\n      157\n      626\n      248\n      523\n      2212\n      0.012418\n      0.025316\n      0.036166\n      0.091320\n      0.236438\n    \n    \n      0900200011184\n      MULTIPOLYGON (((-99.20723 19.50387, -99.20652 ...\n      38\n      54\n      84\n      50\n      65\n      144\n      112\n      416\n      187\n      239\n      1389\n      0.028565\n      0.027358\n      0.035997\n      0.103672\n      0.172066\n    \n  \n\n\n\n\nAhora vamos a hacer un layout con cuatro subplots en dos renglones y dos columnas\n\nfig, axes = plt.subplots(2,2, figsize=(24,24))# Pedimos subfiguras con un renglón y dos columnas\n\n\n\n\nFíjense lo que tenemos ahora en el objeto axes\n\naxes\n\narray([[<AxesSubplot:>, <AxesSubplot:>],\n       [<AxesSubplot:>, <AxesSubplot:>]], dtype=object)\n\n\nEs un array de 2x2 que tiene la misma forma de la figura. Para manejarlo más fácil, vamos a aplanar el array\n\naxes.ravel()\n\narray([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>],\n      dtype=object)\n\n\nEso ya iene una sola dimensión y lo podemos usar de la misma forma que antes\n\nfig, axes = plt.subplots(2,2, figsize=(24,12))# Pedimos subfiguras con un renglón y dos columnas\naxes = axes.ravel() # aplanamos el array\nageb_pob.plot('prop_0a2', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[0], alpha=0.8)# Graficamos en el primer ax\naxes[0].set_title(\"Proporción de 0 a 2\") # Este título está en el nivel ax, no en el figure\naxes[0].set_axis_off()\nctx.add_basemap(axes[0], source=ctx.providers.OpenStreetMap.Mapnik, crs=ageb_pob.crs.to_string())\nageb_pob.plot('prop_12a14', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[1], alpha=0.8)# Graficamos en el segndo ax\naxes[1].set_title(\"Proporción de 12 a 14\") # Este título está en el nivel ax, no en el figure\naxes[1].set_axis_off()\nctx.add_basemap(axes[1], source=ctx.providers.OpenStreetMap.Mapnik, crs=ageb_pob.crs.to_string())\nageb_pob.plot('prop_18a24', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[2], alpha=0.8)# Graficamos en el segndo ax\naxes[2].set_title(\"Proporción de 18 a 24\") # Este título está en el nivel ax, no en el figure\naxes[2].set_axis_off()\nctx.add_basemap(axes[2], source=ctx.providers.OpenStreetMap.Mapnik, crs=ageb_pob.crs.to_string())\nageb_pob.plot('prop_60ymas', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[3], alpha=0.8)# Graficamos en el segndo ax\naxes[3].set_title(\"Proporción de 60 y más\") # Este título está en el nivel ax, no en el figure\naxes[3].set_axis_off()\nctx.add_basemap(axes[3], source=ctx.providers.OpenStreetMap.Mapnik, crs=ageb_pob.crs.to_string())\nplt.tight_layout() # Para disminuir el espacio entre gráficas\nplt.show()"
  },
  {
    "objectID": "parte_2.html",
    "href": "parte_2.html",
    "title": "Geoinformática en R",
    "section": "",
    "text": "Esta parte del libro cubre el manejo de datos espaciales utilizando R\nEN CONSTRUCCIÓN"
  },
  {
    "objectID": "parte_2/intro_R.html",
    "href": "parte_2/intro_R.html",
    "title": "7  Introducción a R",
    "section": "",
    "text": "EN CONSTRUCCIÓN"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]