[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geoinformática",
    "section": "",
    "text": "Prefacio\nPara nosotros en CentroGeo, la computación no es sólo una herramienta para ayudarnos a resolver diferentes problemas geoespaciales; la computación es una parte integral del proceso de análisis y una forma de pensar en geografía. Ser capaz de programar nos permite liberarnos de los algoritmos, técnicas y configuraciones que se incluyen en el software (comercial o abierto) para el análisis de datos geográficos y pensar los problemas de forma diferente. Abrir la posibilidad de automatizar los procesos de análisis no sólo hace más eficiente nuestro trabajo, sino que también nos permite pensar en los problemas de forma diferente, de forma computacional. Con el fin de ayudar a nuestros estudiantes (y a estudiantes o profesionales de otras instituciones) a adquirir las herramientas técnicas básicas para poder llevar a cabo tareras de análisis de datos geoespaciales en Python o en R hemos creado este libro.\nEste libro es (por lo pronto, pretende ser) una compilación de materiales educativos sobre Geoinformática. Busca funcionar como un apoyo para profesores interesados en impartir cursos relacionados con el uso de herramientas de programación para el análisis de datos geográficos o bien, para estudiantes independientes que busquen complementar su formación de manera autodidacta.\nEl libro y el material incluido se distribuye bajo una licencia Creative Commons, de forma que todo mundo es libre de utilizarlo y modificarlo de acuerdo a sus propiuas necesidades, siempre citando la fuente original.\nEste libro fue creado con Quarto.\nPara aprender más sobre quarto, visita: https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#organización-del-libro",
    "href": "intro.html#organización-del-libro",
    "title": "Introducción",
    "section": "Organización del libro",
    "text": "Organización del libro\nEl libro está (estará) organizado en dos grandes secciones: una dedicada a Python y otra a R. Está organizado como un conjunto de talleres. En cada taller se revisarán algunas ideas detrás del análisis de datos geoespaciales con énfasis en las herramientas y técnicas computacionales. Cada taller contiene todo el código necesario y las explicaciones básicas."
  },
  {
    "objectID": "parte_1.html",
    "href": "parte_1.html",
    "title": "Geoinformática: las herramientas básicas",
    "section": "",
    "text": "En esta parte del libro vamos a tratar de cubrir los fundamentos técnicos del procesaminto, análisis y visualización de datos geoespaciales con Python.\nPara seguir los talleres vas a necesitar varios conjuntos de datos que puedes descargar de aqui.\nEl libro está desarrollado a partir de Notebooks de Jupyter, de forma que lo más natural es que vayas siguiendo el desarrollo del libro utilizando estos notebooks.\nLa forma más sencilla de instalar Jupyter y las librerías que estaremos utilizando es utilizando el gestor de paquetes conda, que nos permite instalar fácilmente paquetes de Python sin preocuparnos por dependencias del sistema.\nExisten varis formas de instalar y trabajar con conda. Para usuarios de Windows quizá lo más sencillo sea instalar el paquete de cómputo científico Anaconda. Anaconda contiene, además del gestor de paquetes conda, muchas librerías ya preinstaladas por lo que puede resultar un poco excesivo en tamaño.\nPara trabajar de mejor forma en Python es recomendable crear environments de trabajo. Un environment es algo así como una instalación independiente de Python que contiene todo lo necesario para el desarrollo de un proyecto específico. A continuación les dejo un par de tutoriales en video para aprender a trabajar con environments de conda:\nAnaconda Beginners Guide for Linux and Windows - Python Working Environments Tutorial\nMaster the basics of Conda environments in Python\nFinalmente, conda viene configurado por defecto para utilizar los repositorios de Anaconda, Inc.. La epmpresa provee acceso a sus repositorios sin ningún costo, sin embargo en este repositorio no siempre se encuantran las versiones más actuañlizadoas y completas que vamos a necesitar. Para evitar dificultades les recomiendo utilizar los repositorios de conda-forge, acá les dejo un tutorial:\nTutorial conda-forge"
  },
  {
    "objectID": "parte_1/01_transformacion.html#conjunto-de-datos",
    "href": "parte_1/01_transformacion.html#conjunto-de-datos",
    "title": "1  Transformación de datos",
    "section": "1.1 Conjunto de Datos",
    "text": "1.1 Conjunto de Datos\nVamos a utlizar los datos del Censo de Poblacioń y Vivienda 2020 de INEGI. Trabajaremos con los datos a nivel AGEB para la Ciudad de México. Una AGEB se define como un Área Geográfica ocupada por un conjunto de manzanas perfectamente delimitadas por calles, avenidas, andadores o cualquier otro rasgo de fácil identificación en el terreno y cuyo uso de suelo es principamete habitacional, industrial, de servicios, etc.. Las AGEB’s son la unidad básica de representatividad del Marco Geoestadístico Nacional, son lo suficientemente pequeñas para representar la variabilidad espacial, pero lo suficientemente grandes para mantener la privacidad de la población y disminuir efectos de ruido estadístico.\nLos datos son publicados por INEGI en un archivo en formato csv que contiene diferentes agregaciones geográficas en el mismo archivo. Para entenderlo bien, vamos a abrirlo:\n\n\n\n\n\n\nNote\n\n\n\nEl archivo con los datos lo encuentras en la caropeta de datos del libro con el nombre conjunto_de_datos_ageb_urbana_09_cpv2020.zip\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDentro de este libro, la convención es que los datos están guardados en la carpeta datos/ relativa al notebook que se esté ejecutando.\n\n\n\ndb = pd.read_csv('datos/conjunto_de_datos_ageb_urbana_09_cpv2020.zip',\n                 dtype={'ENTIDAD': object,\n                        'MUN':object,\n                        'LOC':object,\n                        'AGEB':object})\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      0\n      09\n      Ciudad de México\n      000\n      Total de la entidad Ciudad de México\n      0000\n      Total de la entidad\n      0000\n      0\n      9209944\n      4805017\n      ...\n      1898265\n      2536523\n      2084156\n      1290811\n      957162\n      568827\n      46172\n      77272\n      561128\n      10528\n    \n    \n      1\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0000\n      Total del municipio\n      0000\n      0\n      432205\n      227255\n      ...\n      96128\n      123961\n      105899\n      66399\n      50965\n      31801\n      1661\n      2869\n      22687\n      322\n    \n    \n      2\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total de la localidad urbana\n      0000\n      0\n      432205\n      227255\n      ...\n      96128\n      123961\n      105899\n      66399\n      50965\n      31801\n      1661\n      2869\n      22687\n      322\n    \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183\n      1695\n      ...\n      741\n      772\n      692\n      313\n      221\n      145\n      8\n      14\n      148\n      5\n    \n    \n      4\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Azcapotzalco\n      0010\n      1\n      159\n      86\n      ...\n      45\n      42\n      39\n      18\n      13\n      6\n      *\n      0\n      9\n      0\n    \n  \n\n5 rows × 230 columns\n\n\n\nLa librería Pandas es la que provee la funcionalidad para trabajar con datos tabulares en Python. La estructura fundamental de Pandas es el DataFrame, podemos pensar en los DataFrames como hojas de Excel, con columnas nombradas que funcionan como indices para las variables y filas para las observaciones.\nPara leer el archivo utilizamos el método read_csv() de los DataFrames de Pandas. El parámetro dtype que le pasamos a la función nos asegura que ciertas columnas se lean con un tipo de datos especial, en este caso como object, para asegurarnos que no se lean como números y perdamos identificadores, vamos a regresar a esto más adelante.\nLa columna que nos interesa ahorita es NOM_LOC, esta nos ayuda a distinguiir los datos que vienen en cada fila: las filas etiquetadas con Total AGEB urbana contienen los conteos para cada AGEB de todas las variables, entonces, nuestra primera tarea es filtrar la base y quedarnos sólo con las columnas que en la columna NOM_LOC dice Total AGEB urbana.\n\ndb = db.loc[db['NOM_LOC'] == 'Total AGEB urbana']\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183\n      1695\n      ...\n      741\n      772\n      692\n      313\n      221\n      145\n      8\n      14\n      148\n      5\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593\n      2915\n      ...\n      1373\n      1510\n      1203\n      478\n      349\n      238\n      28\n      68\n      393\n      14\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235\n      2232\n      ...\n      965\n      1049\n      878\n      361\n      339\n      247\n      5\n      12\n      250\n      *\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768\n      2551\n      ...\n      1124\n      1237\n      1076\n      481\n      452\n      294\n      10\n      17\n      254\n      *\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176\n      1115\n      ...\n      517\n      562\n      507\n      276\n      260\n      153\n      4\n      3\n      70\n      0\n    \n  \n\n5 rows × 230 columns\n\n\n\nLo que hicimos aquí fue utlizar el selector loc de pandas para seleccionar las filas que queremos, pasándole el filtro que nos interesa, en este caso db['NOM_LOC'] == 'Total AGEB urbana'"
  },
  {
    "objectID": "parte_1/01_transformacion.html#limpieza-de-los-datos",
    "href": "parte_1/01_transformacion.html#limpieza-de-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.2 Limpieza de los datos",
    "text": "1.2 Limpieza de los datos\nHasta aquí lo que tenemos es un DataFrame con todas las variables del censo agregadas por AGEB. Ahora, para poder realizar análisis a partir de esta base de datos, necesitamos asegurarnos de que los datos son del tipo correcto, es decir, si vamos a hacer cuentas, los datos deben ser de tipo float o int. Utlicemos entonces la propiedad db.dtypes para preguntar los tipos de datos.\n\ndb.dtypes\n\nENTIDAD        object\nNOM_ENT        object\nMUN            object\nNOM_MUN        object\nLOC            object\n                ...  \nVPH_CVJ        object\nVPH_SINRTV     object\nVPH_SINLTC     object\nVPH_SINCINT    object\nVPH_SINTIC     object\nLength: 230, dtype: object\n\n\nComo podemos ver, no sólo las columnas que pedimos que leyera como object las leyó así, también las demás columnas. Esto se puede deber a que tienen codificados valores faltantes con caracteres especiales, por lo que pandas no pudo convertirlos automáticamente en números.\nPara entender esto un poco mejor, vamos a leer el diccionario de datos del censo.\n\n\n\n\n\n\nNote\n\n\n\nTambién pueden explorar el archivo en excel, para verlo con más calma\n\n\n\ndiccionario = pd.read_csv('datos/diccionario_datos_ageb_urbana_09_cpv2020.csv', skiprows=3)\ndiccionario\n\n\n\n\n\n  \n    \n      \n      Núm.\n      Indicador\n      Descripción\n      Mnemónico\n      Rangos\n      Longitud\n    \n  \n  \n    \n      0\n      1\n      Clave de entidad federativa\n      Código que identifica a la entidad federativa....\n      ENTIDAD\n      00…32\n      2\n    \n    \n      1\n      2\n      Entidad federativa\n      Nombre oficial de la entidad federativa.\n      NOM_ENT\n      Alfanumérico\n      50\n    \n    \n      2\n      3\n      Clave de municipio o demarcación territorial\n      Código que identifica al municipio o demarcaci...\n      MUN\n      000…570\n      3\n    \n    \n      3\n      4\n      Municipio o demarcación territorial\n      Nombre oficial del municipio o demarcación ter...\n      NOM_MUN\n      Alfanumérico\n      50\n    \n    \n      4\n      5\n      Clave de localidad\n      Código que identifica a la localidad al interi...\n      LOC\n      0000…9999\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      225\n      218\n      Viviendas particulares habitadas que disponen ...\n      Viviendas particulares habitadas que tienen co...\n      VPH_CVJ\n      0…999999999\n      9\n    \n    \n      226\n      219\n      Viviendas particulares habitadas sin radio ni ...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINRTV\n      0…999999999\n      9\n    \n    \n      227\n      220\n      Viviendas particulares habitadas sin línea tel...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINLTC\n      0…999999999\n      9\n    \n    \n      228\n      221\n      Viviendas particulares habitadas sin computado...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINCINT\n      0…999999999\n      9\n    \n    \n      229\n      222\n      Viviendas particulares habitadas sin tecnologí...\n      Viviendas particulares habitadas que no cuenta...\n      VPH_SINTIC\n      0…999999999\n      9\n    \n  \n\n230 rows × 6 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFíjense como pasamos skiprows=3 para leer el diccionario del censo. Esto le dice a pandas que el header (los nombres de las columnas), vienen en el cuarto renglón.\n\n\nA partir de este diccionario podemos ver que hay varias formas de codificar valores faltantes: ‘999999999’, ‘99999999’, ’*’ y ‘N/D’.\nPara poder convertir todas estas columnas en numéricas tenemos que reemplazar todos esos valores por la forma en la que se expresan los datos faltantes en Pandas, utilizando el valor Not a Number de numpy. Para hacer este reemplazo vamos a usar la función replace de Pandas, que toma como argumento el valor que queremos reemplazar y el valor por el cual lo queremos reemplazar:\n\ndb = (db  \n      .replace('999999999', np.nan)\n      .replace('99999999', np.nan)\n      .replace('*', np.nan)\n      .replace('N/D', np.nan))\n\n¡Esta fue una instrucción complicada!\nPero no es realmente difícil. Como hemos visto hasta aquí, los métodos de los DataFrames en general regresan otros DataFrames con el resultado de la operación, esto nos permite encadenar métodos, de forma que cuando hacemos db..replace('999999999', np.nan)..replace('99999999', np.nan), el segundo replace opera sobre el resultado del primero y así sucesivamente. Este encadenamiento de métodos nos ayuda a escribir código más fácil de leer.\nAhora ya tenemos todos los valores faltantes codificados adecuadamente, sin embargo aún nos falta convertirlos a números ¿verdad?\n\ndb.dtypes\n\nENTIDAD        object\nNOM_ENT        object\nMUN            object\nNOM_MUN        object\nLOC            object\n                ...  \nVPH_CVJ        object\nVPH_SINRTV     object\nVPH_SINLTC     object\nVPH_SINCINT    object\nVPH_SINTIC     object\nLength: 230, dtype: object\n\n\nLa forma normal de cambiar el tipo de datos de una columna es utilizar el método astype\n\ndb['VPH_CVJ'].astype('float').dtypes\n\ndtype('float64')\n\n\n\n\n\n\n\n\nNote\n\n\n\nAquí no estamos asignando el resultado de la operación a ninguna variable, el resultado de esta operación no modifica el valor de los datos.\n\n\nAsí podríamos ir cambiando columna por columna, pero como estamos programando ¡nos gusta hacer las cosas en bruto!\nEn el diccionario de datos tenemos los nombres de todas las variables, entonces podemos utilizar estos nombres para seleccionar todas las columnas que contienen datos numéricos y cambiar su tipo en el DataFrame. Fíjense que las primeras 8 filas del diccionario contienen los identificadores geográficos:\n\ndiccionario.head(8)\n\n\n\n\n\n  \n    \n      \n      Núm.\n      Indicador\n      Descripción\n      Mnemónico\n      Rangos\n      Longitud\n    \n  \n  \n    \n      0\n      1\n      Clave de entidad federativa\n      Código que identifica a la entidad federativa....\n      ENTIDAD\n      00…32\n      2\n    \n    \n      1\n      2\n      Entidad federativa\n      Nombre oficial de la entidad federativa.\n      NOM_ENT\n      Alfanumérico\n      50\n    \n    \n      2\n      3\n      Clave de municipio o demarcación territorial\n      Código que identifica al municipio o demarcaci...\n      MUN\n      000…570\n      3\n    \n    \n      3\n      4\n      Municipio o demarcación territorial\n      Nombre oficial del municipio o demarcación ter...\n      NOM_MUN\n      Alfanumérico\n      50\n    \n    \n      4\n      5\n      Clave de localidad\n      Código que identifica a la localidad al interi...\n      LOC\n      0000…9999\n      4\n    \n    \n      5\n      6\n      Localidad\n      Nombre con el que se reconoce a la localidad d...\n      NOM_LOC\n      Alfanumérico\n      70\n    \n    \n      6\n      7\n      Clave del AGEB\n      Clave que identifica al AGEB urbana, al interi...\n      AGEB\n      001...999; 0...9 o A-P\n      4\n    \n    \n      7\n      8\n      Clave de manzana\n      Clave que identifica a la manzana, al interior...\n      MZA\n      001...999\n      3\n    \n  \n\n\n\n\nLas demás filas contienen los nombres (y descripciones) de las variables del Censo.\n\ncampos_datos = diccionario.loc[8:,]['Mnemónico']\ncampos_datos\n\n8           POBTOT\n9           POBFEM\n10          POBMAS\n11           P_0A2\n12         P_0A2_F\n          ...     \n225        VPH_CVJ\n226     VPH_SINRTV\n227     VPH_SINLTC\n228    VPH_SINCINT\n229     VPH_SINTIC\nName: Mnemónico, Length: 222, dtype: object\n\n\nAquí utilizamos una vez más el método loc para seleccionar filas en nuestros datos. En esta ocasión seleccionamos las filas por índice (en este momento nuestro índice es simplemente el número de fila, más adelante usaremos índices diferentes), la selección loc[8:,] simplemente quiere decir todas las columnas para las filas de la 9 en adelante.\nTambién estamos seleccionando una única columna al hacer ['Mnemónico'], el resultado de esta selección ya no es un DataFrame, es una Serie. Las series son las estructuras que usa Pandas para guardar una sóla columna (o fila).\nLas Series se pueden utilizar (igual que las listas) para seleccionar columnas de un DataFrame, entoinces, ahora sí podemos cambiar todos los tipos de datos de una sola vez.\n\ndb[campos_datos] = db[campos_datos].astype('float')\ndb.dtypes\n\nENTIDAD         object\nNOM_ENT         object\nMUN             object\nNOM_MUN         object\nLOC             object\n                ...   \nVPH_CVJ        float64\nVPH_SINRTV     float64\nVPH_SINLTC     float64\nVPH_SINCINT    float64\nVPH_SINTIC     float64\nLength: 230, dtype: object"
  },
  {
    "objectID": "parte_1/01_transformacion.html#descripciones-de-los-datos",
    "href": "parte_1/01_transformacion.html#descripciones-de-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.3 Descripciones de los datos",
    "text": "1.3 Descripciones de los datos\nPandas nos provee una serie de métodos para obtener descripciones generales de la tabla. Podemos usar el método info para obtener una descripción general de la estructura de la tabla y el espacio que ocupa en la memoria:\n\ndb.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2433 entries, 3 to 68915\nColumns: 230 entries, ENTIDAD to VPH_SINTIC\ndtypes: float64(222), int64(1), object(7)\nmemory usage: 4.3+ MB\n\n\nPara obtener las estadísticas descriptivas podemos usar el método describe:\n\ndb.describe()\n\n\n\n\n\n  \n    \n      \n      MZA\n      POBTOT\n      POBFEM\n      POBMAS\n      P_0A2\n      P_0A2_F\n      P_0A2_M\n      P_3YMAS\n      P_3YMAS_F\n      P_3YMAS_M\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      count\n      2433.0\n      2433.000000\n      2422.000000\n      2423.00000\n      2406.000000\n      2392.000000\n      2390.000000\n      2423.000000\n      2422.000000\n      2423.000000\n      ...\n      2416.000000\n      2420.000000\n      2418.000000\n      2415.000000\n      2415.000000\n      2410.000000\n      2251.000000\n      2235.000000\n      2405.000000\n      1801.000000\n    \n    \n      mean\n      0.0\n      3758.993835\n      1970.647812\n      1804.64837\n      109.901912\n      54.471990\n      56.089121\n      3661.372678\n      1914.832370\n      1747.329757\n      ...\n      783.982616\n      1041.995455\n      859.506617\n      533.200000\n      395.840580\n      235.558506\n      20.068858\n      33.803579\n      229.281081\n      5.181011\n    \n    \n      std\n      0.0\n      2433.068753\n      1254.533102\n      1186.95856\n      85.636899\n      42.286817\n      43.908616\n      2347.050678\n      1215.700184\n      1147.281855\n      ...\n      525.413812\n      690.331581\n      601.110222\n      426.577764\n      390.905691\n      204.624708\n      16.611861\n      30.598161\n      191.422212\n      6.154989\n    \n    \n      min\n      0.0\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.0\n      2045.000000\n      1083.500000\n      974.00000\n      46.250000\n      23.000000\n      24.000000\n      2018.000000\n      1053.000000\n      942.500000\n      ...\n      456.750000\n      590.000000\n      488.000000\n      271.000000\n      173.000000\n      118.250000\n      8.000000\n      10.000000\n      79.000000\n      0.000000\n    \n    \n      50%\n      0.0\n      3396.000000\n      1783.500000\n      1616.00000\n      91.000000\n      45.000000\n      46.000000\n      3304.000000\n      1730.500000\n      1566.000000\n      ...\n      698.500000\n      921.500000\n      749.000000\n      442.000000\n      300.000000\n      189.000000\n      16.000000\n      25.000000\n      185.000000\n      4.000000\n    \n    \n      75%\n      0.0\n      4992.000000\n      2617.500000\n      2391.00000\n      152.000000\n      75.000000\n      77.000000\n      4852.000000\n      2539.000000\n      2315.000000\n      ...\n      992.500000\n      1348.250000\n      1083.000000\n      671.000000\n      476.000000\n      288.750000\n      27.000000\n      50.000000\n      336.000000\n      7.000000\n    \n    \n      max\n      0.0\n      21198.000000\n      11128.000000\n      10616.00000\n      709.000000\n      350.000000\n      393.000000\n      20530.000000\n      10774.000000\n      10551.000000\n      ...\n      6196.000000\n      7867.000000\n      7512.000000\n      5717.000000\n      5903.000000\n      3056.000000\n      149.000000\n      290.000000\n      1488.000000\n      66.000000\n    \n  \n\n8 rows × 223 columns"
  },
  {
    "objectID": "parte_1/01_transformacion.html#creación-de-variables",
    "href": "parte_1/01_transformacion.html#creación-de-variables",
    "title": "1  Transformación de datos",
    "section": "1.4 Creación de variables",
    "text": "1.4 Creación de variables\nMuchas veces vamos a querer crear nuevas columnas a partir de las ya existentes. Por ejemplo, podemos estar interesados en el porcentaje de población femenina en cada AGEB.\n\npct_fem = db['POBFEM'] / db['POBTOT']\npct_fem.head()\n\n3      0.532516\n30     0.521187\n82     0.527037\n116    0.535025\n163    0.512408\ndtype: float64\n\n\nFíjense cómo usamos / para dividir dos columnas. El resultado de la operación lo guardamos en la variable pct_fem ¿De qué tipo será esta variable?\n\npct_fem.info()\n\n<class 'pandas.core.series.Series'>\nInt64Index: 2433 entries, 3 to 68915\nSeries name: None\nNon-Null Count  Dtype  \n--------------  -----  \n2405 non-null   float64\ndtypes: float64(1)\nmemory usage: 38.0 KB\n\n\nEs una serie, es decir una columna en nuestro caso. Como esta columna comparte el mismo índice que los datos originales (es resultado de una operación renglón por renglón), entonces la podemos agregar al DataFrame original facilmente:\n\ndb['pct_fem'] = pct_fem\ndb['pct_fem'].head()\n\n/tmp/ipykernel_5237/2610780181.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  db['pct_fem'] = pct_fem\n\n\n3      0.532516\n30     0.521187\n82     0.527037\n116    0.535025\n163    0.512408\nName: pct_fem, dtype: float64\n\n\n\n1.4.1 Modificar valores\nDe la misma forma que podemos agregar columnas (o filas) a nuestro DataFrame, podemos también modificar los valores existentes. Para explorar esto, vamos a crear una nueva columna y llenarla con valores nulos:\n\n# Nueva columna llena de sólamente el número 1\ndb['Nueva'] = None\ndb['Nueva'].head()\n\n/tmp/ipykernel_5237/463547730.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  db['Nueva'] = None\n\n\n3      None\n30     None\n82     None\n116    None\n163    None\nName: Nueva, dtype: object\n\n\nPodemos fácilmente cambiar los valores de todas las filas:\n\ndb['Nueva'] = 1\ndb['Nueva'].head()\n\n3      1\n30     1\n82     1\n116    1\n163    1\nName: Nueva, dtype: int64\n\n\nO también cambiar el valor sólo para una fila específica:\n\ndb.loc[3, 'Nueva'] = 10\ndb['Nueva'].head()\n\n3      10\n30      1\n82      1\n116     1\n163     1\nName: Nueva, dtype: int64\n\n\n\n\n1.4.2 Eliminar columnas\nEliminar columnas es igualmente fácil usando el método drop:\n\ndb = db.drop(columns=['Nueva'])\n'Nueva' in db.columns\n\nFalse\n\n\n¡Fíjense como preguntamos al final si ya habíamos eliminado la columna!\n\n\n1.4.3 Buscando datos\nMuchas veces queremos encontrar observaciones que cumplan con uno o más criterios. Una vez más, el método loc es nuestro amigop para seleccionar datos. Supongamos que queremos encontrar aquelas AGEBs que tengan una población de ‘65 años o más’ mayor a 1,000 personas.\n\ndb_seleccion = db.loc[db['POB65_MAS'] > 1000, :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      444\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0186\n      0\n      11139.0\n      5776.0\n      ...\n      3299.0\n      2878.0\n      1731.0\n      1407.0\n      994.0\n      54.0\n      47.0\n      470.0\n      4.0\n      0.518538\n    \n    \n      3617\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0107\n      0\n      6992.0\n      3673.0\n      ...\n      2205.0\n      2022.0\n      1478.0\n      1117.0\n      650.0\n      21.0\n      26.0\n      256.0\n      4.0\n      0.525315\n    \n    \n      4075\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0287\n      0\n      8213.0\n      4526.0\n      ...\n      2373.0\n      2226.0\n      1503.0\n      1309.0\n      616.0\n      43.0\n      40.0\n      241.0\n      6.0\n      0.551078\n    \n    \n      4886\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0573\n      0\n      12827.0\n      6653.0\n      ...\n      3437.0\n      2878.0\n      1727.0\n      1409.0\n      863.0\n      59.0\n      82.0\n      669.0\n      6.0\n      0.518672\n    \n  \n\n5 rows × 231 columns\n\n\n\nSimplemente pasamos la condición que nos interesa al selector y listo.\nLos criterios de búsquera pueden ser tan sofisticados como se requiera, por ejemplo, podemos seleccionar los AGEBs en los cuales la población de 0 a 14 años sea menor a un cuarto de la población total:\n\ndb_seleccion = db.loc[(db['POB0_14'] / db['POBTOT']) < 0.25, :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183.0\n      1695.0\n      ...\n      772.0\n      692.0\n      313.0\n      221.0\n      145.0\n      8.0\n      14.0\n      148.0\n      5.0\n      0.532516\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235.0\n      2232.0\n      ...\n      1049.0\n      878.0\n      361.0\n      339.0\n      247.0\n      5.0\n      12.0\n      250.0\n      NaN\n      0.527037\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768.0\n      2551.0\n      ...\n      1237.0\n      1076.0\n      481.0\n      452.0\n      294.0\n      10.0\n      17.0\n      254.0\n      NaN\n      0.535025\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176.0\n      1115.0\n      ...\n      562.0\n      507.0\n      276.0\n      260.0\n      153.0\n      4.0\n      3.0\n      70.0\n      0.0\n      0.512408\n    \n  \n\n5 rows × 231 columns\n\n\n\nPodemos hacer combinaciones arbitrarias de selectores utilizando los operadores lógicos & (and) y | (or). Por ejemplo, podemos combinar nuestras selecciones anteriores para encontrar las AGEBs con menos de 50% de mujeres y población de 0 a 14 años sea menor a un cuarto de la población total\n\ndb_seleccion = db.loc[(db['pct_fem'] < 0.5) & \n                      ((db['POB0_14'] / db['POBTOT']) < 0.25), :]\ndb_seleccion.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      2342\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0877\n      0\n      821.0\n      403.0\n      ...\n      174.0\n      135.0\n      56.0\n      44.0\n      34.0\n      3.0\n      23.0\n      70.0\n      NaN\n      0.490865\n    \n    \n      3292\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      1165\n      0\n      400.0\n      199.0\n      ...\n      91.0\n      51.0\n      26.0\n      26.0\n      15.0\n      3.0\n      8.0\n      49.0\n      NaN\n      0.497500\n    \n    \n      5321\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      0770\n      0\n      326.0\n      160.0\n      ...\n      137.0\n      136.0\n      88.0\n      96.0\n      50.0\n      0.0\n      0.0\n      4.0\n      0.0\n      0.490798\n    \n    \n      6016\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1092\n      0\n      5787.0\n      2887.0\n      ...\n      1792.0\n      1744.0\n      1363.0\n      1206.0\n      614.0\n      11.0\n      5.0\n      55.0\n      NaN\n      0.498877\n    \n    \n      7919\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1660\n      0\n      3328.0\n      1653.0\n      ...\n      895.0\n      823.0\n      552.0\n      379.0\n      259.0\n      10.0\n      10.0\n      88.0\n      0.0\n      0.496695\n    \n  \n\n5 rows × 231 columns"
  },
  {
    "objectID": "parte_1/01_transformacion.html#ordenar-valores",
    "href": "parte_1/01_transformacion.html#ordenar-valores",
    "title": "1  Transformación de datos",
    "section": "1.5 Ordenar valores",
    "text": "1.5 Ordenar valores\nFinalmente, vamos a ver cómo ordenar los datos de acuerdo a los valores de un campo. Pensemos que queremos ver las 10 AGEBS más pobladas de la ciudad.\n\ndb.sort_values('POBTOT', ascending = False).head(10)\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      39932\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      Total AGEB urbana\n      0135\n      0\n      21198.0\n      11128.0\n      ...\n      7867.0\n      7512.0\n      5573.0\n      5568.0\n      3056.0\n      60.0\n      39.0\n      346.0\n      3.0\n      0.524955\n    \n    \n      63316\n      09\n      Ciudad de México\n      016\n      Miguel Hidalgo\n      0001\n      Total AGEB urbana\n      0444\n      0\n      18174.0\n      8931.0\n      ...\n      7294.0\n      7187.0\n      5717.0\n      5903.0\n      2640.0\n      149.0\n      9.0\n      144.0\n      NaN\n      0.491416\n    \n    \n      65102\n      09\n      Ciudad de México\n      016\n      Miguel Hidalgo\n      0001\n      Total AGEB urbana\n      1349\n      0\n      15549.0\n      8211.0\n      ...\n      4279.0\n      3756.0\n      2213.0\n      1482.0\n      944.0\n      70.0\n      208.0\n      831.0\n      21.0\n      0.528073\n    \n    \n      9394\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0020\n      Total AGEB urbana\n      0316\n      0\n      15087.0\n      7701.0\n      ...\n      3434.0\n      2289.0\n      1277.0\n      738.0\n      512.0\n      110.0\n      205.0\n      1301.0\n      23.0\n      0.510439\n    \n    \n      9090\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0001\n      Total AGEB urbana\n      0369\n      0\n      14609.0\n      7459.0\n      ...\n      5989.0\n      5970.0\n      5155.0\n      4826.0\n      2486.0\n      23.0\n      9.0\n      19.0\n      0.0\n      0.510576\n    \n    \n      9190\n      09\n      Ciudad de México\n      004\n      Cuajimalpa de Morelos\n      0001\n      Total AGEB urbana\n      0373\n      0\n      14170.0\n      7457.0\n      ...\n      3293.0\n      2650.0\n      1743.0\n      1430.0\n      889.0\n      59.0\n      130.0\n      782.0\n      13.0\n      0.526253\n    \n    \n      6211\n      09\n      Ciudad de México\n      003\n      Coyoacán\n      0001\n      Total AGEB urbana\n      1162\n      0\n      14061.0\n      7267.0\n      ...\n      3416.0\n      2780.0\n      1316.0\n      971.0\n      689.0\n      67.0\n      120.0\n      837.0\n      23.0\n      0.516820\n    \n    \n      52537\n      09\n      Ciudad de México\n      012\n      Tlalpan\n      0001\n      Total AGEB urbana\n      2121\n      0\n      13974.0\n      7345.0\n      ...\n      3954.0\n      3518.0\n      2477.0\n      2100.0\n      1259.0\n      34.0\n      61.0\n      476.0\n      8.0\n      0.525619\n    \n    \n      26177\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      Total AGEB urbana\n      1994\n      0\n      13946.0\n      3330.0\n      ...\n      1069.0\n      767.0\n      327.0\n      231.0\n      180.0\n      18.0\n      25.0\n      329.0\n      4.0\n      0.238778\n    \n    \n      42074\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      Total AGEB urbana\n      1171\n      0\n      13918.0\n      7438.0\n      ...\n      4387.0\n      3856.0\n      2895.0\n      2351.0\n      1244.0\n      51.0\n      96.0\n      655.0\n      12.0\n      0.534416\n    \n  \n\n10 rows × 231 columns\n\n\n\nEl método sort_values nos permite ordenar los datos de acuerdo al valor (o criterio) que queramos. El argumento ascending = False indica que los queremos ordenar de forma descendente."
  },
  {
    "objectID": "parte_1/01_transformacion.html#exploración-visual",
    "href": "parte_1/01_transformacion.html#exploración-visual",
    "title": "1  Transformación de datos",
    "section": "1.6 Exploración Visual",
    "text": "1.6 Exploración Visual\nYa que nos empezamos a familiarizar con el manejo de datos usando Pandas, podemos empezar a hacer cosas más divertidas, por ejemplo, explorar visualmente los datos.\nLa librería seaborn nos ofrece una serie de herramientas para la exploración visual de los datos. Podemos comenzar con un histograma para ver la distribución de los valores de una columna.\n\n_ = sns.histplot(db['POBTOT'], kde = False)\n\n\n\n\nLa función histplot de seaborn nos regresa el histograma, el argumento kde=False le dice que no queremos que ajuste una distribución empírica.\n\n\n\n\n\n\nNote\n\n\n\nCuando hicimos _ = sns.histplot(db['POBTOT'], kde = False) estamos asignando el resultado a la variable _, esto se hace comunmente cuando no queremos ya hacer nada más con ese resultado. Más adelante haremos operaciones sobre las gráficas.\n\n\n\n1.6.0.1 Densidad de Kernel\nOtra forma de representar la distribución de una variable es ajustando una densidad de kernel, que estima una distribución (empírica) de probabilidad a partir de nuestras observaciones.\n\n_ = sns.kdeplot(db['POBTOT'], fill = True)\n\n\n\n\nOtra visualización muy útil es la de la distribución conjunta de dos variables. Por ejemplo, supongamos que queremos comparar las distribuciones de la población masculina y femenina.\n\n_ = sns.jointplot(data=db, x='POBFEM', y='POBMAS')\n\n\n\n\nLa relación, como es de esperarse, es casi perfectamente lineal, pero ver las distribuciones conjuntas nos permite identificar algunas AGEBS con poblaciones masculinas desproporcionadamente grandes ¿Qué serán?.\nMuchas veces queremos visualizar la distribución conjunta de varias variables al mismo tiempo. Por ejemplo cuando queremos hacer ejercicios de regresión queremos explorar la correlación entre las covariables. Una forma de visualizar rápidamente estas distribuciones conjuntas es con un PairGrid. Utlicemos uno sencillo para ver las distribuciones de algunas variables.\n\nvars = ['P_0A2', 'P_15A17', 'PNACOE', 'P3YM_HLI']\ng = sns.PairGrid(db[vars])\ng = g.map(sns.scatterplot)\n\n\n\n\nLa función PairPlot sólo nos prepara la malla (un cuadrado del número de variables de los datos) y con el map llenamos esa malla con la gráfica que queramos, en nuestro caso un diagrama de dispersión.\nEn este caso la diagonal no es muy informativa, es un diagrama de dispersión de una variable consigo misma. PairPlot es muy flexible y nos permite mapear diferentes funciones para la diagonal y los demás elementos, por ejemplo:\n\ng = sns.PairGrid(db[vars])\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n\n<seaborn.axisgrid.PairGrid at 0x7f70a19c4040>"
  },
  {
    "objectID": "parte_1/01_transformacion.html#organizando-los-datos",
    "href": "parte_1/01_transformacion.html#organizando-los-datos",
    "title": "1  Transformación de datos",
    "section": "1.7 Organizando los datos",
    "text": "1.7 Organizando los datos\nMuchos flujos de análisis requieren organizar los datos en una estructura particular conocida como Tidy Data (algo así como datos ordenados). La idea es tener una estructura estandarizada con principios comunes de manipulación que sirva como entrada a diferentes tipos de análisis.\nLas tres características fundamentales de un conjunto de datos bien ordenado de acuerdo a los principios tidy son:\n\nCada variable en una columna\nCada observación en una fila\nCada unidad de observación en una tabla\n\nPara mayor información sobre el concepto de Tidy Data, puede consultarse el Artículo Académico original (de Acceso Libre), así como el Repositorio Púlico asociado a él.\nTratemos de aplicar el concepto de Tidy Data a los datos de la práctica. Primero, recordando su estructura:\n\ndb.head()\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      pct_fem\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183.0\n      1695.0\n      ...\n      772.0\n      692.0\n      313.0\n      221.0\n      145.0\n      8.0\n      14.0\n      148.0\n      5.0\n      0.532516\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0.521187\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235.0\n      2232.0\n      ...\n      1049.0\n      878.0\n      361.0\n      339.0\n      247.0\n      5.0\n      12.0\n      250.0\n      NaN\n      0.527037\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768.0\n      2551.0\n      ...\n      1237.0\n      1076.0\n      481.0\n      452.0\n      294.0\n      10.0\n      17.0\n      254.0\n      NaN\n      0.535025\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176.0\n      1115.0\n      ...\n      562.0\n      507.0\n      276.0\n      260.0\n      153.0\n      4.0\n      3.0\n      70.0\n      0.0\n      0.512408\n    \n  \n\n5 rows × 231 columns\n\n\n\nEsta base de datos no cumple con las características tidy. En efecto, tenemos las variables en columnas (sin contar los identificadores), pero:\n\nTenemos dos tipos de unidades: personas y viviendas. El principio tidy nos indica que necesitamos dos tablas para representar los datos.\nPara cada tipoi de unidad tenemos en la misma fila tantas observaciones como variables (del mismo tipo). Por ejemplo, el valor de la población para cada grupo de edad en cada AGEB es una observación.\n\nEntonces, vamos a trabajar en acomodar la tabla a los principios tidy. Para comenzar, trabajemos sólo con las variables que representan segmentos de edad de la población. Seleccionar sólo estas columnas puede ser engorroso, pero si nos fijamos en el diccionario, podemos observar que todas las variables que nos interesan empiezan con ‘P_’ Podemos usar esta observación para seleccionar, a partir de la lista de columnas, sólo las que nos interesan:\n\ncols_pob = [c for c in db.columns if c.startswith('P_')]\nprint(cols_pob)               \n\n['P_0A2', 'P_0A2_F', 'P_0A2_M', 'P_3YMAS', 'P_3YMAS_F', 'P_3YMAS_M', 'P_5YMAS', 'P_5YMAS_F', 'P_5YMAS_M', 'P_12YMAS', 'P_12YMAS_F', 'P_12YMAS_M', 'P_15YMAS', 'P_15YMAS_F', 'P_15YMAS_M', 'P_18YMAS', 'P_18YMAS_F', 'P_18YMAS_M', 'P_3A5', 'P_3A5_F', 'P_3A5_M', 'P_6A11', 'P_6A11_F', 'P_6A11_M', 'P_8A14', 'P_8A14_F', 'P_8A14_M', 'P_12A14', 'P_12A14_F', 'P_12A14_M', 'P_15A17', 'P_15A17_F', 'P_15A17_M', 'P_18A24', 'P_18A24_F', 'P_18A24_M', 'P_15A49_F', 'P_60YMAS', 'P_60YMAS_F', 'P_60YMAS_M']\n\n\nAhora, vamos a construir un identificador único de AGEB para cada fila concatenando los identificadores de entidad, municipio, localidad y ageb:\n\ndb['AGEB_cvgeo'] = db['ENTIDAD'] + db['MUN'] + db['LOC'] + db['AGEB']\ndb['AGEB_cvgeo'].head()\n\n3      0900200010010\n30     0900200010025\n82     090020001003A\n116    0900200010044\n163    0900200010097\nName: AGEB_cvgeo, dtype: object\n\n\nYa con este identificador, podemos eliminar de la tabla los identificadores que usamos para construirlo\n\ndb = db.drop(columns=['ENTIDAD', 'MUN', 'LOC', 'AGEB'])\n\nCopiamos las columnas que nos interesan a una nueva tabla\n\nrangos = db[['AGEB_cvgeo'] + cols_pob]\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      P_0A2\n      P_0A2_F\n      P_0A2_M\n      P_3YMAS\n      P_3YMAS_F\n      P_3YMAS_M\n      P_5YMAS\n      P_5YMAS_F\n      P_5YMAS_M\n      ...\n      P_15A17\n      P_15A17_F\n      P_15A17_M\n      P_18A24\n      P_18A24_F\n      P_18A24_M\n      P_15A49_F\n      P_60YMAS\n      P_60YMAS_F\n      P_60YMAS_M\n    \n  \n  \n    \n      3\n      0900200010010\n      60.0\n      32.0\n      28.0\n      3123.0\n      1663.0\n      1460.0\n      3074.0\n      1639.0\n      1435.0\n      ...\n      111.0\n      61.0\n      50.0\n      303.0\n      149.0\n      154.0\n      726.0\n      816.0\n      470.0\n      346.0\n    \n    \n      30\n      0900200010025\n      122.0\n      58.0\n      64.0\n      5470.0\n      2856.0\n      2614.0\n      5363.0\n      2805.0\n      2558.0\n      ...\n      214.0\n      97.0\n      117.0\n      521.0\n      263.0\n      258.0\n      1436.0\n      1293.0\n      732.0\n      561.0\n    \n    \n      82\n      090020001003A\n      88.0\n      49.0\n      39.0\n      4147.0\n      2183.0\n      1964.0\n      4065.0\n      2138.0\n      1927.0\n      ...\n      180.0\n      74.0\n      106.0\n      425.0\n      226.0\n      199.0\n      1067.0\n      931.0\n      546.0\n      385.0\n    \n    \n      116\n      0900200010044\n      110.0\n      49.0\n      61.0\n      4658.0\n      2502.0\n      2156.0\n      4560.0\n      2445.0\n      2115.0\n      ...\n      175.0\n      87.0\n      88.0\n      487.0\n      241.0\n      246.0\n      1215.0\n      1132.0\n      672.0\n      460.0\n    \n    \n      163\n      0900200010097\n      40.0\n      16.0\n      24.0\n      2136.0\n      1099.0\n      1037.0\n      2100.0\n      1076.0\n      1024.0\n      ...\n      90.0\n      45.0\n      45.0\n      204.0\n      96.0\n      108.0\n      508.0\n      562.0\n      311.0\n      251.0\n    \n  \n\n5 rows × 41 columns\n\n\n\nAhora vamos a reorganizar la tabla de forma que cada grupo de edad corresponda a una fila en lugar de una columna, de esta forma tenemos las observaciones en filas, de acuerdo al principio tidy.\nPara lograr esto lo que tenemos que hacer es la operación inversa de un pivote, es decir, un stack. El método stack hace justo lo que necesitamos, sólo tenemos que especificar el índice (lo que distingue a cada observación) que queremos utilizar para cada fila, en este caso AGEB_cvgeo.\n\nrangos = rangos.set_index('AGEB_cvgeo').stack()\nrangos\n\nAGEB_cvgeo               \n0900200010010  P_0A2           60.0\n               P_0A2_F         32.0\n               P_0A2_M         28.0\n               P_3YMAS       3123.0\n               P_3YMAS_F     1663.0\n                              ...  \n0901700011524  P_18A24_M      230.0\n               P_15A49_F     1111.0\n               P_60YMAS       706.0\n               P_60YMAS_F     394.0\n               P_60YMAS_M     312.0\nLength: 96555, dtype: float64\n\n\nPerfecto, eso se parece bastante a lo que buscamos, sólo que en lugar de un DataFrame lo que tenemos es una Serie. Fíjense que para cada valor del índice (AGEB_cvgeo), tenemos todos los valores de los grupos de población.\nPara convertir esto en un DataFrame lo más sencillo es quitar el índice que creamos con la función reset_index:\n\nrangos = rangos.reset_index()\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      level_1\n      0\n    \n  \n  \n    \n      0\n      0900200010010\n      P_0A2\n      60.0\n    \n    \n      1\n      0900200010010\n      P_0A2_F\n      32.0\n    \n    \n      2\n      0900200010010\n      P_0A2_M\n      28.0\n    \n    \n      3\n      0900200010010\n      P_3YMAS\n      3123.0\n    \n    \n      4\n      0900200010010\n      P_3YMAS_F\n      1663.0\n    \n  \n\n\n\n\nAhora tenemos un DataFrame en el que el valor de la columna AGEB_cvgeo viene repetido para cada observación. Ya sólo necesitamos renombrar las columnas restantes para que nos indiquen más claramente su contenido:\n\nrangos = rangos.rename(columns = {'level_1':'Grupo', 0:'Población'})\nrangos.head()\n\n\n\n\n\n  \n    \n      \n      AGEB_cvgeo\n      Grupo\n      Población\n    \n  \n  \n    \n      0\n      0900200010010\n      P_0A2\n      60.0\n    \n    \n      1\n      0900200010010\n      P_0A2_F\n      32.0\n    \n    \n      2\n      0900200010010\n      P_0A2_M\n      28.0\n    \n    \n      3\n      0900200010010\n      P_3YMAS\n      3123.0\n    \n    \n      4\n      0900200010010\n      P_3YMAS_F\n      1663.0\n    \n  \n\n\n\n\n!Ahora tenemos nuestra tabla acomodada a los principios tidy!"
  },
  {
    "objectID": "parte_1/01_transformacion.html#agrupamiento-transformación-y-agregación",
    "href": "parte_1/01_transformacion.html#agrupamiento-transformación-y-agregación",
    "title": "1  Transformación de datos",
    "section": "1.8 Agrupamiento, Transformación y Agregación",
    "text": "1.8 Agrupamiento, Transformación y Agregación\nUna ventaja de tener los datos estructurados de acuerdo a los principios tidy es la facilidad con la que podemos realizar procesos de transformación más sofisticados como agrupaciones y sumarios. Las agrupaciones consisten en agrupar observaciones en una tabla de acuerdo a sus valores (o expresiones) en una columna, a los datos agrupados se le pueden aplicar operaciones de agregación más o menos arbitrarias.\nDigamos, por ejemplo, que queremos obtener los totales de población para cada grupo etario a través de todas las AGEBs. Para hacer esto tenemos que agrupar las observaciones por cada Grupo y después obtener el valor agregado por la suma. Vamos por partes.\n\ngrupos = rangos.groupby('Grupo')\ngrupos\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f70945509d0>\n\n\nLa función groupby nos permite agrupar los datos de acuerdo a una (o más) columnas. El resultado, como pueden ver, no es un DataFrame sino un objeto de la clase especial pandas.core.groupby.generic.DataFrameGroupBy. Esta clase sirve para representar DataFrames agregados, estos objetos nos permiten obtener de forma fácil los valores que corresponden a diferentes funciones de agregación. Por ejemplo, para obtener el total de posblación por cada grupo, podemos agregar nuestro objeto con la función sum:\n\ngrupos.sum(numeric_only=True)\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      Grupo\n      \n    \n  \n  \n    \n      P_0A2\n      264424.0\n    \n    \n      P_0A2_F\n      130297.0\n    \n    \n      P_0A2_M\n      134053.0\n    \n    \n      P_12A14\n      364225.0\n    \n    \n      P_12A14_F\n      179955.0\n    \n    \n      P_12A14_M\n      184240.0\n    \n    \n      P_12YMAS\n      7864313.0\n    \n    \n      P_12YMAS_F\n      4141887.0\n    \n    \n      P_12YMAS_M\n      3722424.0\n    \n    \n      P_15A17\n      377178.0\n    \n    \n      P_15A17_F\n      185144.0\n    \n    \n      P_15A17_M\n      191984.0\n    \n    \n      P_15A49_F\n      2490275.0\n    \n    \n      P_15YMAS\n      7500071.0\n    \n    \n      P_15YMAS_F\n      3961914.0\n    \n    \n      P_15YMAS_M\n      3538155.0\n    \n    \n      P_18A24\n      975897.0\n    \n    \n      P_18A24_F\n      483893.0\n    \n    \n      P_18A24_M\n      491985.0\n    \n    \n      P_18YMAS\n      7122878.0\n    \n    \n      P_18YMAS_F\n      3776738.0\n    \n    \n      P_18YMAS_M\n      3346138.0\n    \n    \n      P_3A5\n      321650.0\n    \n    \n      P_3A5_F\n      158674.0\n    \n    \n      P_3A5_M\n      162933.0\n    \n    \n      P_3YMAS\n      8871506.0\n    \n    \n      P_3YMAS_F\n      4637724.0\n    \n    \n      P_3YMAS_M\n      4233780.0\n    \n    \n      P_5YMAS\n      8660874.0\n    \n    \n      P_5YMAS_F\n      4533469.0\n    \n    \n      P_5YMAS_M\n      4127403.0\n    \n    \n      P_60YMAS\n      1487004.0\n    \n    \n      P_60YMAS_F\n      850901.0\n    \n    \n      P_60YMAS_M\n      636074.0\n    \n    \n      P_6A11\n      685511.0\n    \n    \n      P_6A11_F\n      337113.0\n    \n    \n      P_6A11_M\n      348375.0\n    \n    \n      P_8A14\n      829786.0\n    \n    \n      P_8A14_F\n      408494.0\n    \n    \n      P_8A14_M\n      421280.0\n    \n  \n\n\n\n\nComo ve, al usar un agregador sobre el objeto agrupado obtenemos un DataFrame con los valores que corresponden a la agregación que utilizamos.\n\n\n\n\n\n\nNote\n\n\n\nEl parámetro numeric_only=True le dice al agregador que sólo calcule el resultado para las columnas de tipo numérico.\n\n\nEn este caso la función que usamos para agregar los datos es la suma, sin embargo es posible utilizar cualquier función que opere sobre grupos de observaciones, por ejemplo, el promedio:\n\ngrupos.mean(numeric_only=True)\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      Grupo\n      \n    \n  \n  \n    \n      P_0A2\n      109.901912\n    \n    \n      P_0A2_F\n      54.471990\n    \n    \n      P_0A2_M\n      56.089121\n    \n    \n      P_12A14\n      151.130705\n    \n    \n      P_12A14_F\n      74.701121\n    \n    \n      P_12A14_M\n      76.702748\n    \n    \n      P_12YMAS\n      3245.692530\n    \n    \n      P_12YMAS_F\n      1710.110239\n    \n    \n      P_12YMAS_M\n      1536.287247\n    \n    \n      P_15A17\n      156.310816\n    \n    \n      P_15A17_F\n      77.014975\n    \n    \n      P_15A17_M\n      79.993333\n    \n    \n      P_15A49_F\n      1028.614209\n    \n    \n      P_15YMAS\n      3095.365662\n    \n    \n      P_15YMAS_F\n      1635.802642\n    \n    \n      P_15YMAS_M\n      1460.237309\n    \n    \n      P_18A24\n      403.596774\n    \n    \n      P_18A24_F\n      200.038446\n    \n    \n      P_18A24_M\n      204.143154\n    \n    \n      P_18YMAS\n      2939.693768\n    \n    \n      P_18YMAS_F\n      1559.346821\n    \n    \n      P_18YMAS_M\n      1380.989682\n    \n    \n      P_3A5\n      133.298798\n    \n    \n      P_3A5_F\n      65.949293\n    \n    \n      P_3A5_M\n      67.775790\n    \n    \n      P_3YMAS\n      3661.372678\n    \n    \n      P_3YMAS_F\n      1914.832370\n    \n    \n      P_3YMAS_M\n      1747.329757\n    \n    \n      P_5YMAS\n      3574.442427\n    \n    \n      P_5YMAS_F\n      1871.787366\n    \n    \n      P_5YMAS_M\n      1703.426744\n    \n    \n      P_60YMAS\n      615.481788\n    \n    \n      P_60YMAS_F\n      353.511010\n    \n    \n      P_60YMAS_M\n      264.040681\n    \n    \n      P_6A11\n      284.326421\n    \n    \n      P_6A11_F\n      140.055256\n    \n    \n      P_6A11_M\n      144.734109\n    \n    \n      P_8A14\n      343.739022\n    \n    \n      P_8A14_F\n      169.218724\n    \n    \n      P_8A14_M\n      174.587650\n    \n  \n\n\n\n\nLas funciones que usamos para agregar (sum y mean) son funciones de numpyy podemos utiliizar cualquier función de agregación. También es posible calcular diferentes agregaciones al mismo tiempo:\n\ngrupos.aggregate([np.sum, np.mean, np.std])\n\n/tmp/ipykernel_5237/732611272.py:1: FutureWarning: ['AGEB_cvgeo'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n  grupos.aggregate([np.sum, np.mean, np.std])\n\n\n\n\n\n\n  \n    \n      \n      Población\n    \n    \n      \n      sum\n      mean\n      std\n    \n    \n      Grupo\n      \n      \n      \n    \n  \n  \n    \n      P_0A2\n      264424.0\n      109.901912\n      85.636899\n    \n    \n      P_0A2_F\n      130297.0\n      54.471990\n      42.286817\n    \n    \n      P_0A2_M\n      134053.0\n      56.089121\n      43.908616\n    \n    \n      P_12A14\n      364225.0\n      151.130705\n      111.565262\n    \n    \n      P_12A14_F\n      179955.0\n      74.701121\n      55.572013\n    \n    \n      P_12A14_M\n      184240.0\n      76.702748\n      56.746606\n    \n    \n      P_12YMAS\n      7864313.0\n      3245.692530\n      2056.644056\n    \n    \n      P_12YMAS_F\n      4141887.0\n      1710.110239\n      1073.566831\n    \n    \n      P_12YMAS_M\n      3722424.0\n      1536.287247\n      1001.153466\n    \n    \n      P_15A17\n      377178.0\n      156.310816\n      113.532155\n    \n    \n      P_15A17_F\n      185144.0\n      77.014975\n      56.107357\n    \n    \n      P_15A17_M\n      191984.0\n      79.993333\n      57.996607\n    \n    \n      P_15A49_F\n      2490275.0\n      1028.614209\n      692.206450\n    \n    \n      P_15YMAS\n      7500071.0\n      3095.365662\n      1955.668987\n    \n    \n      P_15YMAS_F\n      3961914.0\n      1635.802642\n      1023.764553\n    \n    \n      P_15YMAS_M\n      3538155.0\n      1460.237309\n      950.879515\n    \n    \n      P_18A24\n      975897.0\n      403.596774\n      279.378732\n    \n    \n      P_18A24_F\n      483893.0\n      200.038446\n      138.590941\n    \n    \n      P_18A24_M\n      491985.0\n      204.143154\n      143.125222\n    \n    \n      P_18YMAS\n      7122878.0\n      2939.693768\n      1853.201763\n    \n    \n      P_18YMAS_F\n      3776738.0\n      1559.346821\n      973.233847\n    \n    \n      P_18YMAS_M\n      3346138.0\n      1380.989682\n      899.958704\n    \n    \n      P_3A5\n      321650.0\n      133.298798\n      101.904268\n    \n    \n      P_3A5_F\n      158674.0\n      65.949293\n      50.305709\n    \n    \n      P_3A5_M\n      162933.0\n      67.775790\n      52.251282\n    \n    \n      P_3YMAS\n      8871506.0\n      3661.372678\n      2347.050678\n    \n    \n      P_3YMAS_F\n      4637724.0\n      1914.832370\n      1215.700184\n    \n    \n      P_3YMAS_M\n      4233780.0\n      1747.329757\n      1147.281855\n    \n    \n      P_5YMAS\n      8660874.0\n      3574.442427\n      2284.544513\n    \n    \n      P_5YMAS_F\n      4533469.0\n      1871.787366\n      1185.089392\n    \n    \n      P_5YMAS_M\n      4127403.0\n      1703.426744\n      1115.802146\n    \n    \n      P_60YMAS\n      1487004.0\n      615.481788\n      358.110680\n    \n    \n      P_60YMAS_F\n      850901.0\n      353.511010\n      206.712937\n    \n    \n      P_60YMAS_M\n      636074.0\n      264.040681\n      152.406790\n    \n    \n      P_6A11\n      685511.0\n      284.326421\n      213.690386\n    \n    \n      P_6A11_F\n      337113.0\n      140.055256\n      105.214351\n    \n    \n      P_6A11_M\n      348375.0\n      144.734109\n      109.164209\n    \n    \n      P_8A14\n      829786.0\n      343.739022\n      255.780534\n    \n    \n      P_8A14_F\n      408494.0\n      169.218724\n      126.379228\n    \n    \n      P_8A14_M\n      421280.0\n      174.587650\n      130.222008"
  },
  {
    "objectID": "parte_1/01_transformacion.html#para-practicar",
    "href": "parte_1/01_transformacion.html#para-practicar",
    "title": "1  Transformación de datos",
    "section": "1.9 Para Practicar",
    "text": "1.9 Para Practicar\nLa organización Wikileaks posee una Base de Datos pública en la cual se contiene, entre otras cosas, el número de casualidades existentes durante los primeros años de la Guerra de Afganistán, la cual puede ser consultada a través de la siguiente liga:\n\nhttps://docs.google.com/spreadsheets/d/1EAx8_ksSCmoWW_SlhFyq2QrRn0FNNhcg1TtDFJzZRgc/edit?hl=en#gid=1\n\n\n\n\nWikileaks\n\n\nA partir de los datos, realiza los siguientes ejercidios: * Descarga la tabla como un archivo de tipo .csv (Archivo –> Descargar como –> .csv, hoja actual). * Importa los datos a un DataFrame de Pandas. * Explora los datos generando estadísticas descriptivas y algunas gráficas. * Examina qué tanto se ajusta a los principios del Tidy Data y ajústalo según creas conveniente * Obten una cuenta total de las bajas por mes y genera una gráfica con dicho conteo."
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#exploración-del-contenido",
    "href": "parte_1/02_ejemplo_transformacion.html#exploración-del-contenido",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.1 Exploración del contenido",
    "text": "2.1 Exploración del contenido\nLo primero que vamos a hacer es explorar los datos publicados por la Secretaría de Salud para entender cómo están organizados. En la carpeta de datos del libro puedes encontrar un ejemplo de la base de datos para el 9 de enero de 2023 bajo el nombre datos_abiertos_covid19.zip.\nPara leer los datos vamos a utilizar la función read_csv(), esta función (como pueden ver) acepta que el csv venga comprimido en un zip.\n\ndf = pd.read_csv('datos/datos_abiertos_covid19.zip', dtype=object, encoding='latin-1')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      0\n      2023-01-03\n      01e27d\n      2\n      9\n      25\n      2\n      25\n      25\n      001\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      2\n      2023-01-03\n      06fce8\n      1\n      12\n      07\n      1\n      07\n      07\n      059\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      3\n      2023-01-03\n      1a4a8d\n      1\n      12\n      23\n      2\n      27\n      23\n      008\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nCada renglón en la base de datos corresponde a un caso en seguimiento, el resultado de cada caso se puede actualizar en sucesivas publicaciones de la base de datos. Las columnas describen un conjunto de variables asociadas al seguimiento de cada uno de los casos. Las dos primeras columnas corresponden a la fecha en la que se actualizó el caso y a un id único para cada caso respectivamente, en este taller no vamos a usar esas dos columnas.\nLuego vienen un conjunto de columnas que describen la unidad médica de reporte y, después, las columnas que nos interesan más, que son las que describen al paciente.\nPara entender un poco mejor los datos, conviene leer el archívo de catálogo. Lo pueden descargar del sitio de datos abiertos o bien usar el que viene en la carpeta de datos del libro bajo el nombre 201128 Catalogos.xlsx. Como el catálogo es un archivo de excel con varias hojas, lo vamos a leer usando openpyxl que nos va a devolver un diccionario de DataFrames que relacionan el nombre de la hoja con los datos que contiene.\n\ncatalogos = 'datos/201128 Catalogos.xlsx'\nnombres_catalogos = ['Catálogo de ENTIDADES', # Acá están los nombres de las hojas del excel\n                      'Catálogo MUNICIPIOS',\n                      'Catálogo SI_NO',\n                      'Catálogo TIPO_PACIENTE',\n                      'Catálogo CLASIFICACION_FINAL',\n                      'Catálogo RESULTADO_LAB'\n                     ]\n# read_excel nos regresa un diccionario que relaciona el nombre de cada hoja con \n# el contenido de la hoja como DataFrame\ndict_catalogos = pd.read_excel(catalogos,\n                          nombres_catalogos,\n                          dtype=str,\n                          engine='openpyxl')\nclasificacion_final = dict_catalogos['Catálogo CLASIFICACION_FINAL']\n# Aquí le damos nombre a las columnas porque en el excel se saltan dos líneas\nclasificacion_final.columns = [\"CLAVE\", \"CLASIFICACIÓN\", \"DESCRIPCIÓN\"] \nclasificacion_final\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n    \n      2\n      1\n      CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍ...\n      Confirmado por asociación aplica cuando el cas...\n    \n    \n      3\n      2\n      CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n      Confirmado por dictaminación solo aplica para ...\n    \n    \n      4\n      3\n      CASO DE SARS-COV-2  CONFIRMADO\n      Confirmado aplica cuando:\\nEl caso tiene muest...\n    \n    \n      5\n      4\n      INVÁLIDO POR LABORATORIO\n      Inválido aplica cuando el caso no tienen asoci...\n    \n    \n      6\n      5\n      NO REALIZADO POR LABORATORIO\n      No realizado aplica cuando el caso no tienen a...\n    \n    \n      7\n      6\n      CASO SOSPECHOSO\n      Sospechoso aplica cuando: \\nEl caso no tienen ...\n    \n    \n      8\n      7\n      NEGATIVO A SARS-COV-2\n      Negativo aplica cuando el caso:\\n1. Se le tomo...\n    \n  \n\n\n\n\nLo que estamos viendo aquí es el catálogo de datos de la columna CLASIFICACION_FINAL. Este catálogo relaciona el valor de la CLAVE con su significado. En particular, la columna CLASIFICACION_FINAL es la que nos permite identificar los casos positivos como veremos más adelante.\nEl resto de los catálogos funciona de la misma forma, en este momento sólo vamos a utilizar la clasificación de los pacientes, pero más adelante podemos utilzar algunas de las columnas restantes."
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#aplanado-de-datos",
    "href": "parte_1/02_ejemplo_transformacion.html#aplanado-de-datos",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.2 Aplanado de datos",
    "text": "2.2 Aplanado de datos\nComo acabamos de ver, de alguna forma la información viene distribuida en tres archivos, uno con los datos, otro con las categorías que usa y un tercero con sus descripciones. Para utilizar los datos más fácilmente, sobre todo para poder hablarle a las cosas por su nombre en lugar de referirnos a sus valores codificados, vamos a realizar un conjunto de operaciones para aplanar los datos.\nEn el bajo mundo del análisis de datos, aplanar una base de datos es la operación de substituir los valores codificados a partir de un diccionario. En este caso, los datos que leímos traen valores codificados, entonces la primera misión es substituir esos valores por sus equivalentes en el diccionario.\nComo la base de datos es muy grande, vamos a trabajar sólo con un estado de la república, en este caso la Ciudad de México (pero ustedes podrían elegir otro cualquiera).\nPara seleccionar un estado, tenemos que elegir las filas del DataFrame que contengan el valor que queremos en la columna ENTIDAD, para eso vamos a aprender a usar nuestro primer operador de Pandas, el operador loc que nos permite seleccionar filas a partir de los valores de una o más columnas.\n\n# el copy() nos asegura tener una copia de los datos en lugar de una referencia, \n# con eso podemos liberar la memoria más fácil\ndf = df.loc[df['ENTIDAD_RES'] == '09'].copy()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      09\n      2\n      09\n      09\n      016\n      2\n      ...\n      2\n      1\n      4\n      2\n      97\n      2\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      09\n      1\n      09\n      09\n      012\n      1\n      ...\n      99\n      2\n      97\n      1\n      1\n      3\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      09\n      1\n      18\n      09\n      007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nFíjense que lo que hicimos fue reescribir en la variable df el resultado de nuestra selección, de forma que df ahora sólo contiene resultados para la CDMX.\nAhora ya con los datos filtrados y, por lo tanto, con un tamaño más manejable, vamos a empezar a trabajarlos. Lo primero que vamos a hacer es cambiar los valores de la columna MUNICIPIO_RES por la concatenación de las claves de estado y municipio, esto porque nos hará más adelante más fácil el trabajo de unir los datos con las geometrías de los municipios y porque además así tendremos un identificador único para estos (claro que esto sólo tiene sentido al trabajar con varios estados al mismo tiempo).\n\ndf['MUNICIPIO_RES'] = df['ENTIDAD_RES'] + df['MUNICIPIO_RES']\ndf.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      RESULTADO_LAB\n      TOMA_MUESTRA_ANTIGENO\n      RESULTADO_ANTIGENO\n      CLASIFICACION_FINAL\n      MIGRANTE\n      PAIS_NACIONALIDAD\n      PAIS_ORIGEN\n      UCI\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      09\n      2\n      09\n      09\n      09012\n      2\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      09\n      2\n      09\n      09\n      09007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      09\n      2\n      09\n      09\n      09016\n      2\n      ...\n      2\n      1\n      4\n      2\n      97\n      2\n      99\n      MÃ©xico\n      97\n      2\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      09\n      1\n      09\n      09\n      09012\n      1\n      ...\n      99\n      2\n      97\n      1\n      1\n      3\n      99\n      MÃ©xico\n      97\n      97\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      09\n      1\n      18\n      09\n      09007\n      1\n      ...\n      2\n      2\n      97\n      1\n      2\n      7\n      99\n      MÃ©xico\n      97\n      97\n    \n  \n\n5 rows × 40 columns\n\n\n\nAhora vamos a corregir el nombre de una columna en la base de datos para que coincida con el nombre en el diccionario y después podamos buscar automáticamente. Pra corregir el nombre de la columna vamos a utilizar la función rename de Pandas. Esta función nos sirve para renombrar filas (el índice del DataFrame, que vamos a ver más adelante) o columnas dependiendo de qué eje seleccionemos. El eje 0 son las filas y el 1 las columnas.\n\n# Como estamos usando explícitamente el parámetro columns, \n# no necesitamos especificar el eje\ndf = df.rename(columns={'OTRA_COM': 'OTRAS_COM'})\ndf.columns\n\nIndex(['FECHA_ACTUALIZACION', 'ID_REGISTRO', 'ORIGEN', 'SECTOR', 'ENTIDAD_UM',\n       'SEXO', 'ENTIDAD_NAC', 'ENTIDAD_RES', 'MUNICIPIO_RES', 'TIPO_PACIENTE',\n       'FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF', 'INTUBADO', 'NEUMONIA',\n       'EDAD', 'NACIONALIDAD', 'EMBARAZO', 'HABLA_LENGUA_INDIG', 'INDIGENA',\n       'DIABETES', 'EPOC', 'ASMA', 'INMUSUPR', 'HIPERTENSION', 'OTRAS_COM',\n       'CARDIOVASCULAR', 'OBESIDAD', 'RENAL_CRONICA', 'TABAQUISMO',\n       'OTRO_CASO', 'TOMA_MUESTRA_LAB', 'RESULTADO_LAB',\n       'TOMA_MUESTRA_ANTIGENO', 'RESULTADO_ANTIGENO', 'CLASIFICACION_FINAL',\n       'MIGRANTE', 'PAIS_NACIONALIDAD', 'PAIS_ORIGEN', 'UCI'],\n      dtype='object')\n\n\nFíjense cómo otra vez reescribimos la variable df. La mayor parte de las operaciones en Pandas regresan un DataFrame con el resultado de la operación y no modifican el DataFrame original, entonces para guardar los resultados, necesitamos reescribir la variable (o guardarla con otro nombre)\nAhora sí podemos empezar a aplanar los datos. Vamos a empezar por resolver las claves de resultado de las pruebas COVID. En los datos originales estos vienen codificados en la columna RESULTADO_LAB, pero en el diccionario ese velor se llama RESULTADO, entonces otra vez vamos a empezar por renombrar una columna.\n\ndf = df.rename(columns={'RESULTADO_LAB': 'RESULTADO'})\n\nPara sustituir los valores en nuestros datos originales vamos a usar la función map que toma una serie (una serie es una columna de un dataframe) y mapea sus valores de acuerdo a una correspondencia que podemos pasar como un diccionario. Veamos poco a poco cómo hacer lo que queremos.\nLo primero que necesitamos es un diccionario que relacione los valores en nuestros datos con los nombres en el diccionario. Recordemos cómo se ve el diccionario:\n\nclasificacion_final\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      CLAVE\n      CLASIFICACIÓN\n      DESCRIPCIÓN\n    \n    \n      2\n      1\n      CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍ...\n      Confirmado por asociación aplica cuando el cas...\n    \n    \n      3\n      2\n      CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n      Confirmado por dictaminación solo aplica para ...\n    \n    \n      4\n      3\n      CASO DE SARS-COV-2  CONFIRMADO\n      Confirmado aplica cuando:\\nEl caso tiene muest...\n    \n    \n      5\n      4\n      INVÁLIDO POR LABORATORIO\n      Inválido aplica cuando el caso no tienen asoci...\n    \n    \n      6\n      5\n      NO REALIZADO POR LABORATORIO\n      No realizado aplica cuando el caso no tienen a...\n    \n    \n      7\n      6\n      CASO SOSPECHOSO\n      Sospechoso aplica cuando: \\nEl caso no tienen ...\n    \n    \n      8\n      7\n      NEGATIVO A SARS-COV-2\n      Negativo aplica cuando el caso:\\n1. Se le tomo...\n    \n  \n\n\n\n\nNecesitamos un diccionario {CLASIFICACION:CLAVE} (ya sé que hay unos valores espurios, pero no nos importan porque simplemente esos no los va a encontrar en nuestra base de datos).\nPara construir este diccionario, vamos a empezar por construir la tupla que mantiene la relación que buscamos, para eso vamos a utilizar la función zip que toma dos iteradores como entrada y regresa un iterador que tiene por elementos las tuplas hechas elemento a elemento entre los dos iteradores de inicio. Veamoslo con calma:\n\nl1 = ['a', 'b', 'c']\nl2 = [1, 2, 3]\nl3 = list(zip(l1,l2))\nl3\n\n[('a', 1), ('b', 2), ('c', 3)]\n\n\nLo que nos regresa zip es un iterado con las tuplas formadas por los pares ordenados de los iteradores de entrada. En Python un iterador es cualquier cosa que se pueda recorrer en orden, a veces estos iteradores, como en el caso de zip no regresan todas las entradas sino, para ahorrar memoria, las generan conforme se recorren, por eso hay que hacer list(zip) para que se generen las entradas.\nAhora sí podemos entonces crear el diccionario con el que vamos a actualizar los datos:\n\nclasificacion_final = dict(zip(clasificacion_final['CLAVE'], clasificacion_final['CLASIFICACIÓN']))\nclasificacion_final\n\n{nan: nan,\n 'CLAVE': 'CLASIFICACIÓN',\n '1': 'CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n '2': 'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n '3': 'CASO DE SARS-COV-2  CONFIRMADO',\n '4': 'INVÁLIDO POR LABORATORIO',\n '5': 'NO REALIZADO POR LABORATORIO',\n '6': 'CASO SOSPECHOSO',\n '7': 'NEGATIVO A SARS-COV-2'}\n\n\nY entonces pasarlo como argumento a la función map. Hay un truco aquí, map toma como argumento una función que, para cada llave, regresa el valor correspondiente, entonces no es propiamente el diccionario lo que vamos a pasar, sino la función get del diccionario que hace justo lo que queremos. Esto nos revela una prpiedad curiosa de Python, los argumentos de una función pueden ser funciones.\n\ndf['CLASIFICACION_FINAL'] = df['CLASIFICACION_FINAL'].map(clasificacion_final.get)\ndf['CLASIFICACION_FINAL'].head()\n\n1                                 NEGATIVO A SARS-COV-2\n4                                 NEGATIVO A SARS-COV-2\n8     CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DIC...\n13                       CASO DE SARS-COV-2  CONFIRMADO\n15                                NEGATIVO A SARS-COV-2\nName: CLASIFICACION_FINAL, dtype: object\n\n\nAhora vamos a hacer una sustitución un poco más compleja, tenemos que encontrar todos los campos de tipo “SI - NO” y resolverlos (sustituir por valores que podamos manejar más fácil). Los campos que tienen este tipo de datos vienen en el excel de descriptores:\n\ndescriptores = pd.read_excel('datos/201128 Descriptores_.xlsx',\n                             index_col='Nº',\n                             engine='openpyxl')\ndescriptores\n\n\n\n\n\n  \n    \n      \n      NOMBRE DE VARIABLE\n      DESCRIPCIÓN DE VARIABLE\n      FORMATO O FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      1\n      FECHA_ACTUALIZACION\n      La base de datos se alimenta diariamente, esta...\n      AAAA-MM-DD\n    \n    \n      2\n      ID_REGISTRO\n      Número identificador del caso\n      TEXTO\n    \n    \n      3\n      ORIGEN\n      La vigilancia centinela se realiza a través de...\n      CATÁLOGO: ORIGEN                              ...\n    \n    \n      4\n      SECTOR\n      Identifica el tipo de institución del Sistema ...\n      CATÁLOGO: SECTOR                              ...\n    \n    \n      5\n      ENTIDAD_UM\n      Identifica la entidad donde se ubica la unidad...\n      CATALÓGO: ENTIDADES\n    \n    \n      6\n      SEXO\n      Identifica al sexo del paciente.\n      CATÁLOGO: SEXO\n    \n    \n      7\n      ENTIDAD_NAC\n      Identifica la entidad de nacimiento del paciente.\n      CATALÓGO: ENTIDADES\n    \n    \n      8\n      ENTIDAD_RES\n      Identifica la entidad de residencia del paciente.\n      CATALÓGO: ENTIDADES\n    \n    \n      9\n      MUNICIPIO_RES\n      Identifica el municipio de residencia del paci...\n      CATALÓGO: MUNICIPIOS\n    \n    \n      10\n      TIPO_PACIENTE\n      Identifica el tipo de atención que recibió el ...\n      CATÁLOGO: TIPO_PACIENTE\n    \n    \n      11\n      FECHA_INGRESO\n      Identifica la fecha de ingreso del paciente a ...\n      AAAA-MM-DD\n    \n    \n      12\n      FECHA_SINTOMAS\n      Idenitifica la fecha en que inició la sintomat...\n      AAAA-MM-DD\n    \n    \n      13\n      FECHA_DEF\n      Identifica la fecha en que el paciente falleció.\n      AAAA-MM-DD\n    \n    \n      14\n      INTUBADO\n      Identifica si el paciente requirió de intubación.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      15\n      NEUMONIA\n      Identifica si al paciente se le diagnosticó co...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      16\n      EDAD\n      Identifica la edad del paciente.\n      NÚMERICA EN AÑOS\n    \n    \n      17\n      NACIONALIDAD\n      Identifica si el paciente es mexicano o extran...\n      CATÁLOGO: NACIONALIDAD\n    \n    \n      18\n      EMBARAZO\n      Identifica si la paciente está embarazada.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      19\n      HABLA_LENGUA_INDIG\n      Identifica si el paciente habla lengua índigena.\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      20\n      INDIGENA\n      Identifica si el paciente se autoidentifica co...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      21\n      DIABETES\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      22\n      EPOC\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      23\n      ASMA\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      24\n      INMUSUPR\n      Identifica si el paciente presenta inmunosupre...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      25\n      HIPERTENSION\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      26\n      OTRAS_COM\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      27\n      CARDIOVASCULAR\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      28\n      OBESIDAD\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      29\n      RENAL_CRONICA\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      30\n      TABAQUISMO\n      Identifica si el paciente tiene hábito de taba...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      31\n      OTRO_CASO\n      Identifica si el paciente tuvo contacto con al...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      32\n      TOMA_MUESTRA_LAB\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      33\n      RESULTADO_LAB\n      Identifica el resultado del análisis de la mue...\n      CATÁLOGO: RESULTADO_LAB\n    \n    \n      34\n      TOMA_MUESTRA_ANTIGENO\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      35\n      RESULTADO_ANTIGENO\n      Identifica el resultado del análisis de la mue...\n      CATÁLOGO: RESULTADO_ANTIGENO\n    \n    \n      36\n      CLASIFICACION_FINAL\n      Identifica si el paciente es un caso de COVID-...\n      CATÁLOGO: CLASIFICACION_FINAL\n    \n    \n      37\n      MIGRANTE\n      Identifica si el paciente es una persona migra...\n      CATÁLOGO: SI_ NO                              ...\n    \n    \n      38\n      PAIS_NACIONALIDAD\n      Identifica la nacionalidad del paciente.\n      TEXTO, 99= SE IGNORA\n    \n    \n      39\n      PAIS_ORIGEN\n      Identifica el país del que partió el paciente ...\n      TEXTO, 97= NO APLICA\n    \n    \n      40\n      UCI\n      Identifica si el paciente requirió ingresar a ...\n      CATÁLOGO: SI_ NO                              ...\n    \n  \n\n\n\n\nFíjense en alguno de estos campos en los datos:\n\ndf['OBESIDAD'].unique()\n\narray(['2', '98', '1'], dtype=object)\n\n\nTenemos tres valores diferentes que corresponden (vean el diccionario) a SI, NO y NO ESPECIFICADO. Para todos los análisis que vamos a hacer en general sólo nos van a interesar los casos que sabemos que son SI, entonces lo que más nos conviene es codificar todos estos como binarios, es decir, sólo SI o NO. Además, podemos mejor decirles 1,0 respectivamente y así vamos a poder hacer cuentas mucho más fácil\nDe estos descriptores nos interesan los que tienen CATÁLOGO: SI_ NO en el campo FORMATO O FUENTE. Para poder encontrar y sustituir de forma más sencilla y automática vamos a hacer un par de modificaciones a los datos:\n\nReemplazar los espacios en los nombres de columnas por guiones bajos (para poder “hablarles” más fácil a las columnas)\nQuitar espacios al principio o al final de los valores de los campos (para asegurarnos de que siempre van a ser los mismos)\n\n\ndescriptores.columns = list(map(lambda col: col.replace(' ', '_'), descriptores.columns))\ndescriptores.head()\n\n\n\n\n\n  \n    \n      \n      NOMBRE_DE_VARIABLE\n      DESCRIPCIÓN_DE_VARIABLE\n      FORMATO_O_FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      1\n      FECHA_ACTUALIZACION\n      La base de datos se alimenta diariamente, esta...\n      AAAA-MM-DD\n    \n    \n      2\n      ID_REGISTRO\n      Número identificador del caso\n      TEXTO\n    \n    \n      3\n      ORIGEN\n      La vigilancia centinela se realiza a través de...\n      CATÁLOGO: ORIGEN                              ...\n    \n    \n      4\n      SECTOR\n      Identifica el tipo de institución del Sistema ...\n      CATÁLOGO: SECTOR                              ...\n    \n    \n      5\n      ENTIDAD_UM\n      Identifica la entidad donde se ubica la unidad...\n      CATALÓGO: ENTIDADES\n    \n  \n\n\n\n\nPoco a poco:\n\ndescriptores.columns nos regresa (o les da valor, cuando está del lado izquierdo de un =) los nombres de las columnas del DataFrame\nmap(lambda col: col.replace(' ', '_'), descriptores.columns) la función map regresa una asosiación, como ya vimos. En este caso esta asosiación se hace a través de una función anónima lambda que toma como argumento el nombre de una columna y regresa el mismo nombre pero con los espacios sustituidos por guines bajos\n\nAl final, lo que hacemos es sustituir los nombres de las columnas por una lista hecha por nosotros, para que esto funcione la lista que pasamos debe ser de igual tamaño que la lista original de columnas.\nAhora vamos a hacer lo mismo pero con los valores de los campos:\n\ndescriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\ndescriptores['FORMATO_O_FUENTE'].head()\n\nNº\n1             AAAA-MM-DD\n2                  TEXTO\n3       CATÁLOGO: ORIGEN\n4       CATÁLOGO: SECTOR\n5    CATALÓGO: ENTIDADES\nName: FORMATO_O_FUENTE, dtype: object\n\n\nEste fué más fácil. Fíjense cómo pedimos el campo del lado derecho: descriptores.FORMATO_O_FUENTE, esto es equivalente a descriptores['FORMATO_O_FUENTE'] y los pueden usar indistintamente (claro, el primero sólo funciona si el nombre del campo no tiene espacios).\nFiltremos ahora los descriptores para quedarnos sólo con los que nos interesan, para eso vamos a usar la función query de Pandas, que nos permite filtrar un DataFrame de forma conveniente usando una expresión booleana:\n\ndatos_si_no = descriptores.query('FORMATO_O_FUENTE == \"CATÁLOGO: SI_ NO\"')\ndatos_si_no\n\n\n\n\n\n  \n    \n      \n      NOMBRE_DE_VARIABLE\n      DESCRIPCIÓN_DE_VARIABLE\n      FORMATO_O_FUENTE\n    \n    \n      Nº\n      \n      \n      \n    \n  \n  \n    \n      14\n      INTUBADO\n      Identifica si el paciente requirió de intubación.\n      CATÁLOGO: SI_ NO\n    \n    \n      15\n      NEUMONIA\n      Identifica si al paciente se le diagnosticó co...\n      CATÁLOGO: SI_ NO\n    \n    \n      18\n      EMBARAZO\n      Identifica si la paciente está embarazada.\n      CATÁLOGO: SI_ NO\n    \n    \n      19\n      HABLA_LENGUA_INDIG\n      Identifica si el paciente habla lengua índigena.\n      CATÁLOGO: SI_ NO\n    \n    \n      20\n      INDIGENA\n      Identifica si el paciente se autoidentifica co...\n      CATÁLOGO: SI_ NO\n    \n    \n      21\n      DIABETES\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      22\n      EPOC\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      23\n      ASMA\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      24\n      INMUSUPR\n      Identifica si el paciente presenta inmunosupre...\n      CATÁLOGO: SI_ NO\n    \n    \n      25\n      HIPERTENSION\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      26\n      OTRAS_COM\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      27\n      CARDIOVASCULAR\n      Identifica si el paciente tiene un diagnóstico...\n      CATÁLOGO: SI_ NO\n    \n    \n      28\n      OBESIDAD\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      29\n      RENAL_CRONICA\n      Identifica si el paciente tiene diagnóstico de...\n      CATÁLOGO: SI_ NO\n    \n    \n      30\n      TABAQUISMO\n      Identifica si el paciente tiene hábito de taba...\n      CATÁLOGO: SI_ NO\n    \n    \n      31\n      OTRO_CASO\n      Identifica si el paciente tuvo contacto con al...\n      CATÁLOGO: SI_ NO\n    \n    \n      32\n      TOMA_MUESTRA_LAB\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      34\n      TOMA_MUESTRA_ANTIGENO\n      Identifica si al paciente se le tomó muestra d...\n      CATÁLOGO: SI_ NO\n    \n    \n      37\n      MIGRANTE\n      Identifica si el paciente es una persona migra...\n      CATÁLOGO: SI_ NO\n    \n    \n      40\n      UCI\n      Identifica si el paciente requirió ingresar a ...\n      CATÁLOGO: SI_ NO\n    \n  \n\n\n\n\nPor si acaso, quitémosle también los espacios al campo FORMATO_O_FUENTE\n\ndescriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\n\nAhora sí, vamos a sustituir los valores como queremos en los datos originales. Para eso, lo primero que tenemos que hacer es fijarnos en el catálogo de estos campos:\n\ncat_si_no = dict_catalogos['Catálogo SI_NO']\ncat_si_no\n\n\n\n\n\n  \n    \n      \n      CLAVE\n      DESCRIPCIÓN\n    \n  \n  \n    \n      0\n      1\n      SI\n    \n    \n      1\n      2\n      NO\n    \n    \n      2\n      97\n      NO APLICA\n    \n    \n      3\n      98\n      SE IGNORA\n    \n    \n      4\n      99\n      NO ESPECIFICADO\n    \n  \n\n\n\n\nJusto estos valores los queremos cambiar por claves binarias (acuérdense, para distinguirlos fácilmente). Entonces lo que necesitamos ahora es:\n\nUna lista de los nombres de los campos en donde vamos a hacer la sustitución\nUn mapeo de los valores con los que vamos a sustituir\nHacer la sustitución primero en el diccionario y a partir de eso en los datos originales\n\n\n# lista de los nombres de los campos\ncampos_si_no = datos_si_no.NOMBRE_DE_VARIABLE\n# sustituimos en el catálogo de acuerdo a lo que nos interesa\ncat_si_no['DESCRIPCIÓN'] = list(map(lambda val: 1 if val == 'SI' else 0, cat_si_no['DESCRIPCIÓN']))\n# sustituimos en los datos originales\ndf[campos_si_no] = df[datos_si_no.NOMBRE_DE_VARIABLE].replace(\n                                            to_replace=cat_si_no['CLAVE'].values,\n                                            value=cat_si_no['DESCRIPCIÓN'].values)\ndf[campos_si_no]\n\n\n\n\n\n  \n    \n      \n      INTUBADO\n      NEUMONIA\n      EMBARAZO\n      HABLA_LENGUA_INDIG\n      INDIGENA\n      DIABETES\n      EPOC\n      ASMA\n      INMUSUPR\n      HIPERTENSION\n      OTRAS_COM\n      CARDIOVASCULAR\n      OBESIDAD\n      RENAL_CRONICA\n      TABAQUISMO\n      OTRO_CASO\n      TOMA_MUESTRA_LAB\n      TOMA_MUESTRA_ANTIGENO\n      MIGRANTE\n      UCI\n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      13\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6393642\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394417\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394626\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394988\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6395781\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n1896084 rows × 20 columns\n\n\n\nAcá utilizamos para la última sustitución la función replace de Pandas que toma dos parámetros: la lista de valores a reemplazar y la lista de los valoresa de reemplazo. El reemplazo sucede elemento a elemento, es decir, se sustituye el primer elemento de la lista to_replace por el primer elemento de la lista value y así sucesivamente.\nHay más campos que podemos aplanar en la base de datos, como ejercicio pueden explorar algunos de ellos y sustituir como le hemos hecho aquí. Regresaremos a esto más adelante en el taller, pero por lo pronto nos vamos a mover a otra etapa del pre-procesamiento: el manejo de las fechas"
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#manejo-de-fechas",
    "href": "parte_1/02_ejemplo_transformacion.html#manejo-de-fechas",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.3 Manejo de fechas",
    "text": "2.3 Manejo de fechas\nEn Python las fechas son un tipo especial de datos, nosotros estamos acostumbrados a verlas como cadenas de caractéres: 20 de febrero de 2010, por ejemplo. Python puede hacer muchas cosas con las fechas, pero para eso tienen que estar codificados de la forma correcta.\nEn general el módulo datetime de Python provee las utilerías necesarias para manejar/transformar objetos del tipo fecha. Una de las cosas más útiles es transformar strings en objetos datetime:\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime_object\n\ndatetime.datetime(2005, 6, 1, 13, 33)\n\n\nAcá usamos un formato de fecha, '%b %d %Y %I:%M%p', para convertir el string 'Jun 1 2005  1:33PM'. De esa misma forma podemos especificar formatos diferentes:\n\ndatetime_object = datetime.strptime('06-01-2005  1:33PM', '%m-%d-%Y %I:%M%p')\ndatetime_object\n\ndatetime.datetime(2005, 6, 1, 13, 33)\n\n\nPandas tiene la interfase to_datetime para este tipo de operaciones que nos permite transformar campos de forma muy sencilla, por ejemplo, para transformar la columna FECHA_INGRESO de los datos originales en objetos de tipo datetime podemos hacer:\n\npd.to_datetime(df['FECHA_INGRESO'].head())\n\n1    2022-01-19\n4    2022-03-09\n8    2022-02-20\n13   2022-01-01\n15   2022-06-28\nName: FECHA_INGRESO, dtype: datetime64[ns]\n\n\nVean la diferencia con el tipo de datos original:\n\ndf['FECHA_INGRESO'].head()\n\n1     2022-01-19\n4     2022-03-09\n8     2022-02-20\n13    2022-01-01\n15    2022-06-28\nName: FECHA_INGRESO, dtype: object\n\n\nPandas intenta transformar los datos al tipo fecha usando formatos comunes. En general hace un buen trabajo, sin embargo, si nosotros conocemos el formato en el que están escritas las fechas, siempre es mejor ser explícito y usarlo para la transformación. En el caso de nuestros datos, el formato es: %Y-%m-%d, es decir, el año en cuatro caractéres, dos para el mes y dos para los días, separados por guiones medios. Para pasar el formato utilizamos la opción format de pd.to_datetime()\n\npd.to_datetime(df.FECHA_INGRESO, format=\"%Y-%m-%d\")\n\n1         2022-01-19\n4         2022-03-09\n8         2022-02-20\n13        2022-01-01\n15        2022-06-28\n             ...    \n6393642   2022-01-23\n6394417   2022-11-17\n6394626   2022-11-16\n6394988   2022-12-01\n6395781   2022-12-16\nName: FECHA_INGRESO, Length: 1896084, dtype: datetime64[ns]\n\n\nAunque el resultado debería ser el mismo, ser explícito nos ayuda a entender mejor el código y a asegurarnos de que nuestros datos se comportan como nosotros esperamos. Por ejemplo, ¿qué sucedería si algún registro no se contiene datos en el formato que especificamos? Veamos el campo FECHA_DEF que contiene registros intencionalmente inválidos.\n\npd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\")\n\nValueError: time data \"9999-99-99\" at position 0 doesn't match format specified\n\n\n!Tenemos un ERROR! Pandas no puede transformar algunos datos utilizando el formato que le especificamos. En estos casos hay que especificar el copmportamiento que queremos cuando Pandas encuentra una fecha que no se ajusta al formato. El comportamiento por defecto es arrojar una excepción, es decir, detenerse al encontrar un error y reportárnoslo. Eso puede resultar útil en algunos casos, sin embargo no en el nuestro en el que una fecha que no se ajusta al formato significa que el paciente no ha fallecido, es decit las fechas codificadas como 9999-99-99 corresponden a valores nulos en el campo. Para que pandas regrese un valor nulo cuiando encuentre un error en la conversión de fechas, usamos la opción coerce:\n\npd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\", errors='coerce')\n\n1                NaT\n4                NaT\n8         2022-02-21\n13               NaT\n15               NaT\n             ...    \n6393642          NaT\n6394417          NaT\n6394626          NaT\n6394988          NaT\n6395781          NaT\nName: FECHA_DEF, Length: 1896084, dtype: datetime64[ns]\n\n\nAhora los registros que no se pueden convertir en fechas con el formato que especificamos regresan NaT (Not a Time) en lugar de error.\nUna vez que entendimos las formas en las que queremos convertir las columnas con fechas, podemos transformar todas:\n\ndf['FECHA_INGRESO'] = pd.to_datetime(df['FECHA_INGRESO'], format=\"%Y-%m-%d\")\ndf['FECHA_SINTOMAS'] = pd.to_datetime(df['FECHA_SINTOMAS'], format=\"%Y-%m-%d\")\ndf['FECHA_DEF'] = pd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\", errors='coerce')\ndf[['FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF']].head()\n\n\n\n\n\n  \n    \n      \n      FECHA_INGRESO\n      FECHA_SINTOMAS\n      FECHA_DEF\n    \n  \n  \n    \n      1\n      2022-01-19\n      2022-01-17\n      NaT\n    \n    \n      4\n      2022-03-09\n      2022-03-09\n      NaT\n    \n    \n      8\n      2022-02-20\n      2022-02-13\n      2022-02-21\n    \n    \n      13\n      2022-01-01\n      2022-01-01\n      NaT\n    \n    \n      15\n      2022-06-28\n      2022-06-28\n      NaT"
  },
  {
    "objectID": "parte_1/02_ejemplo_transformacion.html#exportar-datos",
    "href": "parte_1/02_ejemplo_transformacion.html#exportar-datos",
    "title": "2  Limpieza y transformación de datos de COVID-19 en México",
    "section": "2.4 Exportar datos",
    "text": "2.4 Exportar datos\nYa que tenemos procesados los datos, es muy posible que los querramos guardar para usarlos más adelante. La forma más sencilla de exportar los datos es guardarlos como un csv. Para esto Pandas tiene el método to_csv\n\ndf.to_csv(\"datos/covid_enero_2023_procesados.csv\")\n\nHasta aquí hemos cubierto más o menos todo el pre-proceso de los datos. Claro no vimos todas las columnas, sólo nos fijamos en algunas, pero eso basta para darnos una buena idea de cómo se hacen las demás.\n\n2.4.1 Tarea\nSustituyan los valores de la columna TIPO_PACIENTE por sus valores en el catálogo correspondiente"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#bajar-y-guardar-datos",
    "href": "parte_1/03_automatizacion_transformacion.html#bajar-y-guardar-datos",
    "title": "3  Automatización",
    "section": "3.1 Bajar y guardar datos",
    "text": "3.1 Bajar y guardar datos\nEn el taller anterior bajamos los datos directamente del sitio de la Secretaría de Salud, ahora vamos a automatizar el proceso de descarga de datos de forma que, desde Python, podamos descargar los datos y asegurarnos de que tenemos la última versión disponible.\nDescargar y guardar archivos en Python es relativamente sencillo, vamos a usar tres módulos de la distribución base de Python:\n\nos. Este módulo provee herramientas para interactuar con el sistema operativo. La vamos a usar para construir los paths en donde vamos a guardar los datos y preguntar si el archivo ya existe.\nrequests. Esta librería provee diferentes formas de interactuar con el protocolo HTTP. La vamos a usar para hacer las peticiones a la página y procesar la respuesta.\nzipfile. Esta librería sirve para trabajar con archivos comprimidos en formato zip. En nuestro caso la usaremos para descomprimir los diccionarios.\n\nLa parte complicada de entender es el uso de requests pra comunicarse con la página en donde están los datos.\n\nr = requests.get(\"https://www.centrogeo.org.mx/\")\nr.content[0:500]\n\nb'\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"es-es\" dir=\"ltr\" class=\\'com_content view-featured itemid-101 home j31 mm-hover\\'>\\r\\n<head>\\r\\n<base href=\"https://www.centrogeo.org.mx/\" />\\n\\t<meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\\n\\t<meta name=\"keywords\" content=\"M\\xc3\\xa9xico, CONACYT, CentroGeo, Ciencias de Informaci\\xc3\\xb3n Geoespacial, Centro de Investigaci\\xc3\\xb3n, Ciencias de Informaci\\xc3\\xb3n, Investigaci\\xc3\\xb3n, Geoespacial\" />\\n\\t<meta name=\"rights\" content=\"Esta obra est\\xc3\\xa1 bajo una licencia d'\n\n\nComo ven, una petición de tipo get simplemente nos regresa, a través de la propiedad content, el contenido de la respuesta del servidor. En el caso de la página de CentroGeo, el contenido es el HTML de la página (que podríamos ver mejor con un browser), pero en el caso de que la dirección apunte a un archivo de descarga, el contenido es el stream de datos del archivo. Este stream de datos lo podemos usar como entrada para escribir un archivo utilizando la función open.\nLa función open va a tomar como entrada el path en donde queremos guardar el archivo, este path puede ser simplemente una cadena de caracteres, sin embargo esto haría que nuestro código no fuera interoperable entre sistemas operativos, entonces, en lugar de escribir el path como cadena de caracteres, vamos a escribirlo como un objeto de os:\n\nos.path.join(\"datos\", \"datos_covid.zip\")\n\n'datos/datos_covid.zip'\n\n\nEsta forma de construir el path nos asegura que va a funcionar en cualquier sistema operativo.\nYa con estas explicaciones, podemos escribir la función que descarga los datos:\n\ndef bajar_datos_salud(directorio_datos='data/'):\n    '''\n        Descarga el ultimo archivo disponible en datos abiertos y los diccionarios correspondientes.\n    '''\n    fecha_descarga = datetime.now().date()\n    url_datos = 'https://datosabiertos.salud.gob.mx/gobmx/salud/datos_abiertos/datos_abiertos_covid19.zip'    \n    archivo_nombre = f'{fecha_descarga.strftime(\"%y%m%d\")}COVID19MEXICO.csv.zip'\n    archivo_ruta = os.path.join(directorio_datos, archivo_nombre)\n    url_diccionario = 'https://datosabiertos.salud.gob.mx/gobmx/salud/datos_abiertos/diccionario_datos_covid19.zip'\n    diccionario_ruta = os.path.join(directorio_datos, 'diccionario.zip')\n    if os.path.exists(archivo_ruta):\n        logging.debug(f'Ya existe {archivo_nombre}')\n    else:\n        print(f'Bajando datos...')\n        r = requests.get(url_datos, allow_redirects=True)\n        open(archivo_ruta, 'wb').write(r.content)\n        r = requests.get(url_diccionario, allow_redirects=True)\n        open(diccionario_ruta, 'wb').write(r.content)\n        with zipfile.ZipFile(diccionario_ruta, 'r') as zip_ref:\n          zip_ref.extractall(directorio_datos)\n\nPara utilizar la función hacemos:\n\nbajar_datos_salud('datos/')\n\nBajando datos..."
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#preproceso",
    "href": "parte_1/03_automatizacion_transformacion.html#preproceso",
    "title": "3  Automatización",
    "section": "3.2 Preproceso",
    "text": "3.2 Preproceso\nAhora, ya que tenemos los datos descargados, vamos a empaquetar en una función el flujo de preproceso que trabajamos en el taller anterior. Esta función va a tomar como entrada la carpeta en donde se encuentran los datos y dicionariosy el nombre del archivo de datos que queremos procesar. Toma dos parámetros adicionales, uno para decidir si queremos resolver o no las claves binarias y otro para definir la entidad que queremos procesar.\n\ndef carga_datos_covid19_MX(data_dir='datos/', archivo='datos_abiertos_covid19.zip', resolver_claves='si_no_binarias', entidad='09'):\n    \"\"\"\n        Lee en un DataFrame el CSV con el reporte de casos de la Secretaría de Salud de México publicado en una fecha dada. Esta función\n        también lee el diccionario de datos que acompaña a estas publicaciones para preparar algunos campos, en particular permite la funcionalidad\n        de generar columnas binarias para datos con valores 'SI', 'No'.\n\n        **Nota 2**: Por las actualizaciones a los formatos de datos, esta función sólo va a servir para archivos posteriores a 20-11-28\n\n        resolver_claves: 'sustitucion', 'agregar', 'si_no_binarias', 'solo_localidades'. Resuelve los valores del conjunto de datos usando el\n        diccionario de datos y los catálogos. 'sustitucion' remplaza los valores en las columnas, 'agregar'\n        crea nuevas columnas. 'si_no_binarias' cambia valores SI, NO, No Aplica, SE IGNORA, NO ESPECIFICADO por 1, 0, 0, 0, 0 respectivamente.\n\n    \"\"\"\n    catalogo_nombre ='201128 Catalogos.xlsx'\n    catalogo_path = os.path.join(data_dir, catalogo_nombre)\n    descriptores_nombre = '201128 Descriptores.xlsx'\n    descriptores_path = os.path.join(data_dir, descriptores_nombre)\n    data_file = os.path.join(data_dir, archivo)\n    print(data_file)\n    df = pd.read_csv(data_file, dtype=object, encoding='latin-1')\n    if entidad is not None:\n      df = df[df['ENTIDAD_RES'] == entidad]\n    # Hay un error y el campo OTRA_COMP es OTRAS_COMP según los descriptores\n    df.rename(columns={'OTRA_COM': 'OTRAS_COM'}, inplace=True)\n    # Asignar clave única a municipios\n    df['MUNICIPIO_RES'] = df['ENTIDAD_RES'] + df['MUNICIPIO_RES']\n    df['CLAVE_MUNICIPIO_RES'] = df['MUNICIPIO_RES']\n    # Leer catalogos\n    nombres_catalogos = ['Catálogo de ENTIDADES',\n                         'Catálogo MUNICIPIOS',\n                         'Catálogo RESULTADO',\n                         'Catálogo SI_NO',\n                         'Catálogo TIPO_PACIENTE']\n    nombres_catalogos.append('Catálogo CLASIFICACION_FINAL')\n    nombres_catalogos[2] = 'Catálogo RESULTADO_LAB'\n\n    dict_catalogos = pd.read_excel(catalogo_path,\n                              nombres_catalogos,\n                              dtype=str,\n                              engine='openpyxl')\n\n    entidades = dict_catalogos[nombres_catalogos[0]]\n    municipios = dict_catalogos[nombres_catalogos[1]]\n    tipo_resultado = dict_catalogos[nombres_catalogos[2]]\n    cat_si_no = dict_catalogos[nombres_catalogos[3]]\n    cat_tipo_pac = dict_catalogos[nombres_catalogos[4]]\n    # Arreglar los catálogos que tienen mal las primeras líneas\n    dict_catalogos[nombres_catalogos[2]].columns = [\"CLAVE\", \"DESCRIPCIÓN\"]\n    dict_catalogos[nombres_catalogos[5]].columns = [\"CLAVE\", \"CLASIFICACIÓN\", \"DESCRIPCIÓN\"]\n\n\n    clasificacion_final = dict_catalogos[nombres_catalogos[5]]\n\n\n    # Resolver códigos de entidad federal\n    cols_entidad = ['ENTIDAD_RES', 'ENTIDAD_UM', 'ENTIDAD_NAC']\n    df['CLAVE_ENTIDAD_RES'] = df['ENTIDAD_RES']\n    df[cols_entidad] = df[cols_entidad].replace(to_replace=entidades['CLAVE_ENTIDAD'].values,\n                                               value=entidades['ENTIDAD_FEDERATIVA'].values)\n\n    # Construye clave unica de municipios de catálogo para resolver nombres de municipio\n    municipios['CLAVE_MUNICIPIO'] = municipios['CLAVE_ENTIDAD'] + municipios['CLAVE_MUNICIPIO']\n\n    # Resolver códigos de municipio\n    municipios_dict = dict(zip(municipios['CLAVE_MUNICIPIO'], municipios['MUNICIPIO']))\n    df['MUNICIPIO_RES'] = df['MUNICIPIO_RES'].map(municipios_dict.get)\n\n    # Resolver resultados\n\n    df.rename(columns={'RESULTADO_LAB': 'RESULTADO'}, inplace=True)\n    tipo_resultado['DESCRIPCIÓN'].replace({'POSITIVO A SARS-COV-2': 'Positivo SARS-CoV-2'}, inplace=True)\n\n    tipo_resultado = dict(zip(tipo_resultado['CLAVE'], tipo_resultado['DESCRIPCIÓN']))\n    df['RESULTADO'] = df['RESULTADO'].map(tipo_resultado.get)\n    clasificacion_final = dict(zip(clasificacion_final['CLAVE'], clasificacion_final['CLASIFICACIÓN']))\n    df['CLASIFICACION_FINAL'] = df['CLASIFICACION_FINAL'].map(clasificacion_final.get)\n    # Resolver datos SI - NO\n\n    # Necesitamos encontrar todos los campos que tienen este tipo de dato y eso\n    # viene en los descriptores, en el campo FORMATO_O_FUENTE\n    descriptores = pd.read_excel(f'{data_dir}201128 Descriptores_.xlsx',\n                                 index_col='Nº',\n                                 engine='openpyxl')\n    descriptores.columns = list(map(lambda col: col.replace(' ', '_'), descriptores.columns))\n    descriptores['FORMATO_O_FUENTE'] = descriptores.FORMATO_O_FUENTE.str.strip()\n\n    datos_si_no = descriptores.query('FORMATO_O_FUENTE == \"CATÁLOGO: SI_ NO\"')\n    cat_si_no['DESCRIPCIÓN'] = cat_si_no['DESCRIPCIÓN'].str.strip()\n\n    campos_si_no = datos_si_no.NOMBRE_DE_VARIABLE\n    nuevos_campos_si_no = campos_si_no\n\n    if resolver_claves == 'agregar':\n        nuevos_campos_si_no = [nombre_var + '_NOM' for nombre_var in campos_si_no]\n    elif resolver_claves == 'si_no_binarias':\n        nuevos_campos_si_no = [nombre_var + '_BIN' for nombre_var in campos_si_no]\n        cat_si_no['DESCRIPCIÓN'] = list(map(lambda val: 1 if val == 'SI' else 0, cat_si_no['DESCRIPCIÓN']))\n\n    df[nuevos_campos_si_no] = df[datos_si_no.NOMBRE_DE_VARIABLE].replace(\n                                                to_replace=cat_si_no['CLAVE'].values,\n                                                value=cat_si_no['DESCRIPCIÓN'].values)\n\n    # Resolver tipos de paciente\n    cat_tipo_pac = dict(zip(cat_tipo_pac['CLAVE'], cat_tipo_pac['DESCRIPCIÓN']))\n    df['TIPO_PACIENTE'] = df['TIPO_PACIENTE'].map(cat_tipo_pac.get)\n\n    df = procesa_fechas(df)\n\n    return df\n\ndef procesa_fechas(covid_df):\n    df = covid_df.copy()\n    df['FECHA_INGRESO'] = pd.to_datetime(df['FECHA_INGRESO'], format=\"%Y-%m-%d\")\n    df['FECHA_SINTOMAS'] = pd.to_datetime(df['FECHA_SINTOMAS'], format=\"%Y-%m-%d\")\n    df['FECHA_DEF'] = pd.to_datetime(df['FECHA_DEF'], format=\"%Y-%m-%d\", errors='coerce')\n    df['DEFUNCION'] = (df['FECHA_DEF'].notna()).astype(int)\n    df['EDAD'] = df['EDAD'].astype(int)\n    return df"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#preprocesar-usando-nuestras-funciones",
    "href": "parte_1/03_automatizacion_transformacion.html#preprocesar-usando-nuestras-funciones",
    "title": "3  Automatización",
    "section": "3.3 Preprocesar usando nuestras funciones",
    "text": "3.3 Preprocesar usando nuestras funciones\n\ndf = carga_datos_covid19_MX(entidad='09')\ndf\n\ndatos/datos_abiertos_covid19.zip\n\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      CARDIOVASCULAR_BIN\n      OBESIDAD_BIN\n      RENAL_CRONICA_BIN\n      TABAQUISMO_BIN\n      OTRO_CASO_BIN\n      TOMA_MUESTRA_LAB_BIN\n      TOMA_MUESTRA_ANTIGENO_BIN\n      MIGRANTE_BIN\n      UCI_BIN\n      DEFUNCION\n    \n  \n  \n    \n      1\n      2023-01-03\n      180725\n      2\n      9\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      HOSPITALIZADO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      4\n      2023-01-03\n      1933c0\n      1\n      12\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      IZTAPALAPA\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      MIGUEL HIDALGO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      CIUDAD DE MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      15\n      2023-01-03\n      0a6cd6\n      2\n      6\n      CIUDAD DE MÉXICO\n      1\n      NAYARIT\n      CIUDAD DE MÉXICO\n      IZTAPALAPA\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6393642\n      2023-01-03\n      m1cd235\n      2\n      12\n      MÉXICO\n      1\n      MÉXICO\n      CIUDAD DE MÉXICO\n      GUSTAVO A. MADERO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6394417\n      2023-01-03\n      m0dbc4c\n      2\n      12\n      MÉXICO\n      1\n      AGUASCALIENTES\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      6394626\n      2023-01-03\n      m13431e\n      2\n      12\n      MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      6394988\n      2023-01-03\n      m1493ea\n      2\n      12\n      MÉXICO\n      1\n      MÉXICO\n      CIUDAD DE MÉXICO\n      GUSTAVO A. MADERO\n      AMBULATORIO\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      6395781\n      2023-01-03\n      m0a22b8\n      2\n      12\n      MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n1896084 rows × 63 columns"
  },
  {
    "objectID": "parte_1/03_automatizacion_transformacion.html#guardando-el-resultado",
    "href": "parte_1/03_automatizacion_transformacion.html#guardando-el-resultado",
    "title": "3  Automatización",
    "section": "3.4 Guardando el resultado",
    "text": "3.4 Guardando el resultado\nListo, con nuestras funciones tenemos ya nuestros datos preprocesados, ahora vamos a guardarlos para poder utlizarlos rápidamente en otros notebooks. En general tenemos muchas opciones para guardar los datos, csv, por ejemplo. En esta ocasión vamos a usar un formato nativo de Python el pickle, que es una forma de serializar un objeto de Python. Pandas nos provee una función para guardar directamente un dataframe como pickle:\n\ndf.to_pickle(\"data/datos_covid_ene19.pkl\")\n\nEn la documentación de to_pickle pueden ver las opcioones completas.\n\ndf.to_csv(\"datos/covid_enero_2023_procesados.csv\")"
  },
  {
    "objectID": "parte_1/04_visualizacion_covid_1.html#curvas-epidémicas",
    "href": "parte_1/04_visualizacion_covid_1.html#curvas-epidémicas",
    "title": "4  Curvas epidémicas",
    "section": "4.1 Curvas epidémicas",
    "text": "4.1 Curvas epidémicas\nLo primero que haremos será el desarrollo de Curvas Epidémicas es decir, la evolución temporal de los casos confirmados y las defunciones. Si consultamos los diccionarios de datos, podemos ver que los casos confirmados para COVID-19 corresponden a 3 categorías de la columna clasificación final:\n\nCASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA\nCASO DE COVID-19 CONFIRMADO POR COMITÉ DE DICTAMINACIÓN\nCASO DE SARS-COV-2 CONFIRMADO\n\nmientras que las defunciones corresponden a todos aquellos registros que tengan una fecha de defunción válida, es decir, en nuestros datos preprocesados, todas las fechas válidas.\n\n4.1.1 Curva de casos confirmados\nEl primer paso es extraer las filas que corresponden a casos confirmados\n\ndf.CLASIFICACION_FINAL.unique()\n\narray(['NEGATIVO A SARS-COV-2',\n       'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n       'CASO DE SARS-COV-2  CONFIRMADO', 'CASO SOSPECHOSO',\n       'NO REALIZADO POR LABORATORIO',\n       'CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n       'INVÁLIDO POR LABORATORIO'], dtype=object)\n\n\nA partir de estos valores podemos seleccionar las filas que queremos\n\nvalores_confirmados = ['CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n                       'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE  DICTAMINACIÓN',\n                       'CASO DE SARS-COV-2  CONFIRMADO']\nconfirmados = df.loc[df['CLASIFICACION_FINAL'].isin(valores_confirmados)]\nconfirmados.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_ACTUALIZACION\n      ID_REGISTRO\n      ORIGEN\n      SECTOR\n      ENTIDAD_UM\n      SEXO\n      ENTIDAD_NAC\n      ENTIDAD_RES\n      MUNICIPIO_RES\n      TIPO_PACIENTE\n      ...\n      CARDIOVASCULAR_BIN\n      OBESIDAD_BIN\n      RENAL_CRONICA_BIN\n      TABAQUISMO_BIN\n      OTRO_CASO_BIN\n      TOMA_MUESTRA_LAB_BIN\n      TOMA_MUESTRA_ANTIGENO_BIN\n      MIGRANTE_BIN\n      UCI_BIN\n      DEFUNCION\n    \n  \n  \n    \n      8\n      2023-01-03\n      0741e4\n      2\n      6\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      MIGUEL HIDALGO\n      HOSPITALIZADO\n      ...\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      13\n      2023-01-03\n      1c4d2e\n      2\n      9\n      CIUDAD DE MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      TLALPAN\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      25\n      2023-01-03\n      0a98b4\n      2\n      12\n      CIUDAD DE MÉXICO\n      1\n      MICHOACÁN DE OCAMPO\n      CIUDAD DE MÉXICO\n      MILPA ALTA\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      42\n      2023-01-03\n      13cf10\n      2\n      9\n      CIUDAD DE MÉXICO\n      1\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      BENITO JUÁREZ\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      54\n      2023-01-03\n      0fef08\n      1\n      12\n      CIUDAD DE MÉXICO\n      2\n      CIUDAD DE MÉXICO\n      CIUDAD DE MÉXICO\n      AZCAPOTZALCO\n      AMBULATORIO\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 63 columns\n\n\n\nAhora tenemos una tabla con todos los casos confirmados, para hacer una curva epidémica, tenemos que agregar en una escala temporal. Lo más sencillo es primero agregar por día y a partir de ahí podemos construir agregados para cualquier intervalo que queramos.\nNecesitamos decidir cuál fecha de todas las disponibles vamos a utilizar para agregar los casos. En este caso, la DGE sugiere utilizar la fecha de inicio de síntomas (FECHA_SINTOMAS) para construir la curva de casos confirmados y la de defunción (FECHA_DEF) para la curva de defunciones.\nEntonces, para construir la curva de confirmados lo primero que tenemos que hacer es indexar el DataFrame por la fecha de inicio de síntomas\n\nconfirmados = confirmados.set_index('FECHA_SINTOMAS')\nconfirmados.index\n\nDatetimeIndex(['2022-02-13', '2022-01-01', '2022-04-22', '2022-08-07',\n               '2022-01-10', '2022-01-14', '2022-12-01', '2022-05-25',\n               '2022-12-23', '2022-08-02',\n               ...\n               '2022-06-23', '2022-08-16', '2022-08-19', '2022-08-01',\n               '2022-07-05', '2022-07-08', '2022-09-05', '2022-06-19',\n               '2022-06-20', '2022-09-23'],\n              dtype='datetime64[ns]', name='FECHA_SINTOMAS', length=769894, freq=None)\n\n\nYa con los datos indexados es fácil construir agregados diarios, sólo tenemos que seleccionar qué columnas queremos agregar. Por lo pronto hagamos un conteo sólo de casos confirmados. Para eso sólo tenemos que agrupár el índice usando una frecuencia diaría y tomar el tamaño de los grupos (de alguna columna, realmente no importa cual).\n\nconfirmados_diarios = (confirmados\n                       .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                       .size() # Calculamos el tamaño de cada grupo\n                       .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                       .rename({0:'Confirmados'}, axis=1) # Le damos nombre a la columna que obtenemos\n                       )\nconfirmados_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      Confirmados\n    \n  \n  \n    \n      0\n      2022-01-01\n      6748\n    \n    \n      1\n      2022-01-02\n      6585\n    \n    \n      2\n      2022-01-03\n      10398\n    \n    \n      3\n      2022-01-04\n      9729\n    \n    \n      4\n      2022-01-05\n      10924\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      362\n      2022-12-29\n      715\n    \n    \n      363\n      2022-12-30\n      399\n    \n    \n      364\n      2022-12-31\n      258\n    \n    \n      365\n      2023-01-01\n      187\n    \n    \n      366\n      2023-01-02\n      80\n    \n  \n\n367 rows × 2 columns\n\n\n\nHay muchas formas de visualizar estos datos, la primera y más sencilla es utilizar los métodos que provee Pandas, por ejemplo:\n\nconfirmados_diarios.set_index('FECHA_SINTOMAS').plot()\n\n<AxesSubplot: xlabel='FECHA_SINTOMAS'>\n\n\n\n\n\nUna alternativa que nos provee herramientas interactivas para visualizar los datos y que es muy fácil de usar es Plotly. A través del módulo Plotly express podemos crear de forma muy simple gráficas que nos permitan interactuar con ellas.\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y=\"Confirmados\")\nfig.show()\n\n\n                                                \n\n\nComo pueden ver, fue muy simple hacer una gráfica con herramientas para pan y zoom. Estas herramientas hacen más fácil ver que los datos de casos confirmados contienen la mezcla de dos señales: una de alta frecuancia que representa la variación diaria, con una especie de periodicidad semanal y una seññal de baja frecuencia que contiene las olas epidémicas.\nLa señal de alta frecuencia contiene mucho ruido que corresponde a los ciclos de actualización de la información y que realmente nos dice poco de la tendencia de los datos. Una forma sencilla de filtrar este ruido es utilizando la media móvil. Para calcular este promedio, Pandas provee la función rolling\n\nconfirmados_diarios['Media Móvil'] = (confirmados_diarios\n                                      .rolling(window=7)\n                                      .mean())\nconfirmados_diarios.head(10)\n\n/tmp/ipykernel_89594/2896601829.py:3: FutureWarning:\n\nDropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['FECHA_SINTOMAS'], dtype='object')\n\n\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      Confirmados\n      Media Móvil\n    \n  \n  \n    \n      0\n      2022-01-01\n      6748\n      NaN\n    \n    \n      1\n      2022-01-02\n      6585\n      NaN\n    \n    \n      2\n      2022-01-03\n      10398\n      NaN\n    \n    \n      3\n      2022-01-04\n      9729\n      NaN\n    \n    \n      4\n      2022-01-05\n      10924\n      NaN\n    \n    \n      5\n      2022-01-06\n      9816\n      NaN\n    \n    \n      6\n      2022-01-07\n      11910\n      9444.285714\n    \n    \n      7\n      2022-01-08\n      11229\n      10084.428571\n    \n    \n      8\n      2022-01-09\n      11794\n      10828.571429\n    \n    \n      9\n      2022-01-10\n      19673\n      12153.571429\n    \n  \n\n\n\n\nY ahora la podemos graficar\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y='Media Móvil')\nfig.show()\n\n\n                                                \n\n\nPara graficar las dos series en la misma gráfica lo más sencillo es pasar los datos del formato ancho (en columnas) al formato largo (en filas con una columna que los distinga). Para esto vamos a usar la función melt de Pandas\n\n confirmados_diarios = confirmados_diarios.melt(id_vars=['FECHA_SINTOMAS'], value_vars=['Confirmados', 'Media Móvil'])\n confirmados_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      variable\n      value\n    \n  \n  \n    \n      0\n      2022-01-01\n      Confirmados\n      6748.000000\n    \n    \n      1\n      2022-01-02\n      Confirmados\n      6585.000000\n    \n    \n      2\n      2022-01-03\n      Confirmados\n      10398.000000\n    \n    \n      3\n      2022-01-04\n      Confirmados\n      9729.000000\n    \n    \n      4\n      2022-01-05\n      Confirmados\n      10924.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      2022-12-29\n      Media Móvil\n      1137.142857\n    \n    \n      730\n      2022-12-30\n      Media Móvil\n      1045.571429\n    \n    \n      731\n      2022-12-31\n      Media Móvil\n      934.571429\n    \n    \n      732\n      2023-01-01\n      Media Móvil\n      796.714286\n    \n    \n      733\n      2023-01-02\n      Media Móvil\n      573.714286\n    \n  \n\n734 rows × 3 columns\n\n\n\nCon los datos de esta forma, ahora podemos usar Plotly para graficar ambas variables utilizando como color la columna variable. El parámetro color nos permite separar dos series de datos cuando estas vienen en formato largo.\n\nfig = px.line(confirmados_diarios, x='FECHA_SINTOMAS', y='value', color='variable')\nfig.show()\n\n\n                                                \n\n\n\n\n4.1.2 Curva de defunciones\nYa que construimos la curva de casos confirmados, la de defunciones es exáctamente igual, sólo necesitamos seleccionar al inicio del proceso los renglones que tengan una fecha de defunción válida e indexar por fecha de defunción\n\ndefunciones = confirmados.loc[confirmados['FECHA_DEF'].notnull()] # Seleccionamos los casos con fecha de defunción\ndefunciones = defunciones.set_index('FECHA_DEF') # indexamos por fecha de defunción\ndefunciones_diarios = (defunciones\n                       .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                       .size() # Calculamos el tamaño de cada grupo\n                       .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                       .rename({0:'Defunciones'}, axis=1) # Le damos nombre a la columna que obtenemos\n                       )\ndefunciones_diarios['Media Móvil'] = defunciones_diarios.rolling(window=7).mean() # Calculamos la media móvil\ndefunciones_diarios = defunciones_diarios.melt(id_vars=['FECHA_DEF'], value_vars=['Defunciones', 'Media Móvil']) # Pasamos al formato largo\nfig = px.line(defunciones_diarios, x='FECHA_DEF', y='value', color='variable')\nfig.show()\n\n/tmp/ipykernel_89594/3052884713.py:9: FutureWarning:\n\nDropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['FECHA_DEF'], dtype='object')\n\n\n\n\n                                                \n\n\n\n\n4.1.3 Combinando las dos gráficas\nPara entenmder la evolución de la epidemia conviene poder ver las dos gráficas al mismo tiempo y explorarlas de forma conjunta. En general este tipo de combinaciones en las que dos o más gráficas comparten por lo menos un eje (en nuestro caso el tiempo) se llaman Facetas. Plotly nos permite crear este tipo de visualizaciones de forma muy sencilla, lo que necesitamos es combinar ambos datos (casos y defunciones) en un sólo DataFrame en formato largo y asegurarnos de que cada fila pueda distinguir a qué se refiere. En nuestros datos vamos a tener cuatro series diferentes: datos crudos y media móvil para casos y definciones.\nComencemos con la serie de defunciones, lo primero que tenemos que hacer es agregar una columna con el tipo de serie, es decir, defunciones:\n\ndefunciones_diarios['Tipo'] = 'Defunciones'\ndefunciones_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_DEF\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-03\n      Defunciones\n      1.000000\n      Defunciones\n    \n    \n      1\n      2022-01-04\n      Defunciones\n      2.000000\n      Defunciones\n    \n    \n      2\n      2022-01-05\n      Defunciones\n      1.000000\n      Defunciones\n    \n    \n      3\n      2022-01-06\n      Defunciones\n      3.000000\n      Defunciones\n    \n    \n      4\n      2022-01-07\n      Defunciones\n      8.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n726 rows × 4 columns\n\n\n\nEn la columna variable tenemos los valores Defunciones y Media Móvil, necesitamos cambiar el valor de Defunciones por algo que sea compatible con tener los casois confirmados en el mismo DataFrame, pienses que al combinar ambas series queremos tener sólo dos valores diferentes en esta columna. Cambiemos entonces el valor de Defunciones por Conteo:\n\ndefunciones_diarios.loc[defunciones_diarios['variable'] == 'Defunciones', 'variable'] = 'Conteo'\ndefunciones_diarios\n\n\n\n\n\n  \n    \n      \n      FECHA_DEF\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-03\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      1\n      2022-01-04\n      Conteo\n      2.000000\n      Defunciones\n    \n    \n      2\n      2022-01-05\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      3\n      2022-01-06\n      Conteo\n      3.000000\n      Defunciones\n    \n    \n      4\n      2022-01-07\n      Conteo\n      8.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n726 rows × 4 columns\n\n\n\nAhora tenemos dos nombres diferentes para los campos con los que vamos a construir el eje de las X: FECHA_DEF y FECHA_SINTOMAS. Para poder combinar ambas series en una sola, necesitamos que esos campos tengan el mismo nombre en las dos series.\n\ndefunciones_diarios = defunciones_diarios.rename({'FECHA_DEF': 'Fecha'}, axis=1)\ndefunciones_diarios\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-03\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      1\n      2022-01-04\n      Conteo\n      2.000000\n      Defunciones\n    \n    \n      2\n      2022-01-05\n      Conteo\n      1.000000\n      Defunciones\n    \n    \n      3\n      2022-01-06\n      Conteo\n      3.000000\n      Defunciones\n    \n    \n      4\n      2022-01-07\n      Conteo\n      8.000000\n      Defunciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n726 rows × 4 columns\n\n\n\nRepetimos el proceso para los datos de casos confirmados:\n\nconfirmados_diarios['Tipo'] = 'Casos Confirmados'\nconfirmados_diarios.loc[confirmados_diarios['variable'] == 'Confirmados', 'variable'] = 'Conteo'\nconfirmados_diarios = confirmados_diarios.rename({'FECHA_SINTOMAS': 'Fecha'}, axis=1)\nconfirmados_diarios\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      6748.000000\n      Casos Confirmados\n    \n    \n      1\n      2022-01-02\n      Conteo\n      6585.000000\n      Casos Confirmados\n    \n    \n      2\n      2022-01-03\n      Conteo\n      10398.000000\n      Casos Confirmados\n    \n    \n      3\n      2022-01-04\n      Conteo\n      9729.000000\n      Casos Confirmados\n    \n    \n      4\n      2022-01-05\n      Conteo\n      10924.000000\n      Casos Confirmados\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      2022-12-29\n      Media Móvil\n      1137.142857\n      Casos Confirmados\n    \n    \n      730\n      2022-12-30\n      Media Móvil\n      1045.571429\n      Casos Confirmados\n    \n    \n      731\n      2022-12-31\n      Media Móvil\n      934.571429\n      Casos Confirmados\n    \n    \n      732\n      2023-01-01\n      Media Móvil\n      796.714286\n      Casos Confirmados\n    \n    \n      733\n      2023-01-02\n      Media Móvil\n      573.714286\n      Casos Confirmados\n    \n  \n\n734 rows × 4 columns\n\n\n\nAhora que ambas series tienen la misma forma y columnas que distinguen los cuatro casos que nos interesan, sólo resta combinar las series. En este caso, lo que queremos es pegar los datos de una abajo de la otra (el orden da igual). Para esto vamos a usar la función concat que toma una lista de DataFrames y regresa un DataFrame concatenado a lo largo del eje que queramos.\n\ncasos_defunciones = pd.concat([confirmados_diarios, defunciones_diarios], axis=0)\ncasos_defunciones\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      6748.000000\n      Casos Confirmados\n    \n    \n      1\n      2022-01-02\n      Conteo\n      6585.000000\n      Casos Confirmados\n    \n    \n      2\n      2022-01-03\n      Conteo\n      10398.000000\n      Casos Confirmados\n    \n    \n      3\n      2022-01-04\n      Conteo\n      9729.000000\n      Casos Confirmados\n    \n    \n      4\n      2022-01-05\n      Conteo\n      10924.000000\n      Casos Confirmados\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n1460 rows × 4 columns\n\n\n\nYa con la nueva serie como la queremos, podemos hacer nuestras Facetas utlizando el parámetro facet_col que le dice a Plotly qué columna usar para distinguir las dos series. Es importante decirle que no queremos que compartan el eje \\(y\\) porque las escalas son muy diferentes\n\nfig = px.line(casos_defunciones, x='Fecha', y='value', color='variable', facet_col='Tipo', facet_col_wrap=1)\nfig.update_yaxes(matches=None)\nfig.show()\n\n\n                                                \n\n\n\n\n4.1.4 Hospitalizaciones\nOtra gráfica muy interesante para comprender la evolucióón de la epidemia es la de hospitalizaciones. Para obtener esta gráfica primero tenemos que seleccionar los pacientes confirmados como positivos a COVID-19 y que además fueron hospitalizados.\nLos casos confirmados ya los tenemos calculados en la variable confirmados, entonces falta ver cómo obtener los pacientes hospitalizados\n\nconfirmados.TIPO_PACIENTE.unique()\n\narray(['HOSPITALIZADO', 'AMBULATORIO'], dtype=object)\n\n\nGracias a nuentra base aplanada es muy fácil distinguirlos, entonces sólo los tenemos que seleccionar, agregar por día y podemos hacer una gráfica como las anteriores (incluyendo la media móvil). Recordemos que confirmados está indexado por fecha de inicio de síntomas, entonces nuestra curva de hospitalización estará indexada por la misma fecha\n\nhospitalizados = confirmados[confirmados.TIPO_PACIENTE == 'HOSPITALIZADO']\nhospitalizados_diarios = (hospitalizados\n                          .groupby(pd.Grouper(freq='D'))[['ID_REGISTRO']] # grupos por dia y seleccionamos 'ID_REGISTRO'\n                          .size() # Calculamos el tamaño de cada grupo\n                          .reset_index() # Convertimos el resultado (que es una serie) en DataFrame\n                          .rename({0:'Hospitalizaciones'}, axis=1) # Le damos nombre a la columna que obtenemos\n                        )\nhospitalizados_diarios['Media Móvil'] = hospitalizados_diarios.rolling(window=7).mean()\nhospitalizados_diarios = hospitalizados_diarios.melt(id_vars=['FECHA_SINTOMAS'], value_vars=['Hospitalizaciones', 'Media Móvil'])\nfig = px.line(hospitalizados_diarios, x='FECHA_SINTOMAS', y='value', color='variable')\nfig.show()\n\n/tmp/ipykernel_89594/1651814779.py:8: FutureWarning:\n\nDropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['FECHA_SINTOMAS'], dtype='object')\n\n\n\n\n                                                \n\n\nY, una vez más, para comparar vamos a poner las tres gráficas (casos confirmados, defunciones y hospitalizacones) en un Facet\n\nhospitalizados_diarios['Tipo'] = 'Hospitalizaciones'\nhospitalizados_diarios.loc[hospitalizados_diarios['variable'] == 'Hospitalizaciones', 'variable'] = 'Conteo'\nhospitalizados_diarios = hospitalizados_diarios.rename({'FECHA_SINTOMAS': 'Fecha'}, axis=1)\nhospitalizados_diarios\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      117.000000\n      Hospitalizaciones\n    \n    \n      1\n      2022-01-02\n      Conteo\n      91.000000\n      Hospitalizaciones\n    \n    \n      2\n      2022-01-03\n      Conteo\n      124.000000\n      Hospitalizaciones\n    \n    \n      3\n      2022-01-04\n      Conteo\n      120.000000\n      Hospitalizaciones\n    \n    \n      4\n      2022-01-05\n      Conteo\n      144.000000\n      Hospitalizaciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      2022-12-29\n      Media Móvil\n      35.285714\n      Hospitalizaciones\n    \n    \n      730\n      2022-12-30\n      Media Móvil\n      33.857143\n      Hospitalizaciones\n    \n    \n      731\n      2022-12-31\n      Media Móvil\n      31.285714\n      Hospitalizaciones\n    \n    \n      732\n      2023-01-01\n      Media Móvil\n      27.857143\n      Hospitalizaciones\n    \n    \n      733\n      2023-01-02\n      Media Móvil\n      21.571429\n      Hospitalizaciones\n    \n  \n\n734 rows × 4 columns\n\n\n\nCombinamos con la serie de casos y defunciones\n\ncasos_defunciones_hospitalizaciones = pd.concat([hospitalizados_diarios, casos_defunciones], axis=0)\ncasos_defunciones_hospitalizaciones\n\n\n\n\n\n  \n    \n      \n      Fecha\n      variable\n      value\n      Tipo\n    \n  \n  \n    \n      0\n      2022-01-01\n      Conteo\n      117.000000\n      Hospitalizaciones\n    \n    \n      1\n      2022-01-02\n      Conteo\n      91.000000\n      Hospitalizaciones\n    \n    \n      2\n      2022-01-03\n      Conteo\n      124.000000\n      Hospitalizaciones\n    \n    \n      3\n      2022-01-04\n      Conteo\n      120.000000\n      Hospitalizaciones\n    \n    \n      4\n      2022-01-05\n      Conteo\n      144.000000\n      Hospitalizaciones\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      721\n      2022-12-27\n      Media Móvil\n      4.714286\n      Defunciones\n    \n    \n      722\n      2022-12-28\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      723\n      2022-12-29\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      724\n      2022-12-30\n      Media Móvil\n      5.285714\n      Defunciones\n    \n    \n      725\n      2022-12-31\n      Media Móvil\n      4.571429\n      Defunciones\n    \n  \n\n2194 rows × 4 columns\n\n\n\nGraficamos las tres series\n\nfig = px.line(casos_defunciones_hospitalizaciones, x='Fecha', y='value', color='variable', facet_col='Tipo', facet_col_wrap=1)\nfig.update_yaxes(matches=None)\nfig.show()"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#leyendo-datos-geográficos",
    "href": "parte_1/05_intro_geopandas.html#leyendo-datos-geográficos",
    "title": "5  Objetos geográficos",
    "section": "5.1 Leyendo datos geográficos",
    "text": "5.1 Leyendo datos geográficos\nA través del método read_file(), Geopandas nos permite leer archivos en una variedad de formatos. Vamos a comenzar por leer el shapefile de las AGEBS para la Ciudad de México.\n\nagebs = gpd.read_file(\"datos/agebs_cdmx.zip\")  # Importar los datos espaciales\nagebs\n\n\n\n\n\n  \n    \n      \n      CVEGEO\n      CVE_ENT\n      CVE_MUN\n      CVE_LOC\n      CVE_AGEB\n      geometry\n    \n  \n  \n    \n      0\n      0901000011716\n      09\n      010\n      0001\n      1716\n      POLYGON ((2787237.541 816989.461, 2787288.728 ...\n    \n    \n      1\n      0901000012150\n      09\n      010\n      0001\n      2150\n      POLYGON ((2794154.458 823013.444, 2794155.774 ...\n    \n    \n      2\n      0901000011133\n      09\n      010\n      0001\n      1133\n      POLYGON ((2795690.723 820050.788, 2795684.238 ...\n    \n    \n      3\n      0901000011307\n      09\n      010\n      0001\n      1307\n      POLYGON ((2792584.475 815678.668, 2792624.325 ...\n    \n    \n      4\n      0901000010281\n      09\n      010\n      0001\n      0281\n      POLYGON ((2788845.392 823526.074, 2788840.549 ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2426\n      0900700012564\n      09\n      007\n      0001\n      2564\n      POLYGON ((2814016.268 821043.511, 2814019.319 ...\n    \n    \n      2427\n      0900700012615\n      09\n      007\n      0001\n      2615\n      POLYGON ((2814358.791 820744.850, 2814405.087 ...\n    \n    \n      2428\n      0900700012969\n      09\n      007\n      0001\n      2969\n      POLYGON ((2815993.470 819777.763, 2816019.848 ...\n    \n    \n      2429\n      0900700013721\n      09\n      007\n      0001\n      3721\n      POLYGON ((2807966.150 821578.350, 2807941.550 ...\n    \n    \n      2430\n      0900700011034\n      09\n      007\n      0001\n      1034\n      POLYGON ((2808319.196 821552.683, 2808243.251 ...\n    \n  \n\n2431 rows × 6 columns\n\n\n\nComo pueden ver, el resultado es similar a un DataFrame, pero con la columna especial geometry\n\nagebs.geometry.head()\n\n0    POLYGON ((2787237.541 816989.461, 2787288.728 ...\n1    POLYGON ((2794154.458 823013.444, 2794155.774 ...\n2    POLYGON ((2795690.723 820050.788, 2795684.238 ...\n3    POLYGON ((2792584.475 815678.668, 2792624.325 ...\n4    POLYGON ((2788845.392 823526.074, 2788840.549 ...\nName: geometry, dtype: geometry\n\n\nEsta columna guarda las geometrías de nuestros datos. Una primera cosa que siempre queremos hacer cuando leemos una capa es visualizarla rápidamente, de la misma forma en la que lo hacemos en QGis o Arc. Para esto, los GeoDataFrames tienen el método plot que nos permite una exploración rápida.\n\nagebs.plot(figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nEsta es sólo una primera visualización rápida que nos permite asegurarnos explorar los datos, más addelante vamos a ver diferentes formas de mejorar estos mapas.\n\n5.1.0.1 Líneas\nDe la misma forma en que leimos un archivo con polígonos, podemos leer un archivo que contiene líneas, en este caso las calles de la delegación Cuahutemoc. Noten además que este archivo es un geojson y GeoPandas es capaz de inferir el tipo de archiovo a través de la extensión.\n\nvias = gpd.read_file(\"datos/vias_cuauhtemoc.geojson\") # Se importan los datos espaciales\nvias = vias.set_index('id')                         # Se establece una columna como índice\nvias.head()                                         # Visualizar los primeros registros de la fila\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n    \n      id\n      \n    \n  \n  \n    \n      1\n      LINESTRING (-99.17041 19.40092, -99.17047 19.4...\n    \n    \n      2\n      LINESTRING (-99.17850 19.40720, -99.17868 19.4...\n    \n    \n      3\n      LINESTRING (-99.14905 19.43796, -99.14871 19.4...\n    \n    \n      4\n      LINESTRING (-99.14735 19.44531, -99.14718 19.4...\n    \n    \n      5\n      LINESTRING (-99.17655 19.42105, -99.17645 19.4...\n    \n  \n\n\n\n\nAl igual que con los polígonos, es posible utilizar la función .plot() para graficar las líneas rápidamente:\n\nvias.plot(figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nUna vez más, estas no son las mejores visualizaciones pero nos ayudan a explorar rápidamente la capa.\n\n\n5.1.0.2 Puntos\nLas capas de puntos se comportan de la misma forma. Por ejemplo, leamos un shape con las estaciones de metro:\n\nestaciones = gpd.read_file(\"datos/estaciones_metro.zip\")\nestaciones.head()\n\n\n\n\n\n  \n    \n      \n      stop_lat\n      stop_lon\n      geopoint\n      agency_id\n      stop_id\n      stop_desc\n      stop_name\n      trip_heads\n      geometry\n    \n  \n  \n    \n      0\n      19.443082\n      -99.139034\n      (2:19.443082,-99.139034)\n      METRO\n      14169.0\n      Metro Línea 8 correspondencia con línea B.\n      Garibaldi_1\n      Garibaldi - Constitución de 1917\n      POINT (485405.843 2149860.572)\n    \n    \n      1\n      19.468965\n      -99.136176\n      (2:19.468965,-99.13617600000001)\n      METRO\n      14103.0\n      Metro Línea 5 correspondencia con línea 3.\n      La Raza_1_3\n      Pantitlán - Politécnico\n      POINT (485708.110 2152724.378)\n    \n    \n      2\n      19.376256\n      -99.187746\n      (2:19.37625563,-99.18774605)\n      METRO\n      14079.0\n      Metro Línea 7 correspondencia con línea 12.\n      Mixcoac_1\n      Tláhuac - Mixcoac\n      POINT (480284.558 2142470.874)\n    \n    \n      3\n      19.408944\n      -99.122279\n      (2:19.40894369,-99.12227869)\n      METRO\n      14144.0\n      Metro Línea 9 correspondencia con línea 4.\n      Jamaica\n      Tacubaya - Pantitlán\n      POINT (487161.939 2146081.726)\n    \n    \n      4\n      19.375679\n      -99.186866\n      (2:19.37567873,-99.18686628)\n      METRO\n      132131.0\n      Metro Línea 12 correspondencia con línea 7.\n      Mixcoac\n      Mixcoac - Tláhuac\n      POINT (480376.875 2142406.938)\n    \n  \n\n\n\n\nY la visualización se produce de forma idéntica a los casos anteriores:\n\nestaciones.plot(figsize = (10,10))\n\n<AxesSubplot: >"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#produciendo-mejores-mapas",
    "href": "parte_1/05_intro_geopandas.html#produciendo-mejores-mapas",
    "title": "5  Objetos geográficos",
    "section": "5.2 Produciendo mejores mapas",
    "text": "5.2 Produciendo mejores mapas\nLos mapas que hicimos hasta aquí sirven como forma de explorar los datos rápidamente, sin embargo, es posible producir mucho mejores visualizaciones de forma relativamente sencilla cambiando los parámetros que usamos para graficar.\n\n5.2.1 Transpariencia\nLa intensidad del color de un polígono puede ser cambiado a través del parámetro alpha del método plot(). Este parámetro es un valor entre cero y uno, donde el 0 representa transparencia completa y el 1 completa opacidad (máxima intensidad):\n\nagebs.plot(alpha = 0.5 , figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\n\n5.2.1.1 Eliminar Ejes\nLos mapas que hemos visto son, por debajo, gráficas de matplotlib, matplotlib está hecha para producir gráficas generales sobre una infinidad de temáticas. En general, entender una gráfica necesita información sobre la escala de variación de los datos, esta información suele proveerse a través de ejes. Por otro lado, en los mapas, a veces los ejes resultan redundantes y es mejor omitirlos.\nElimiar los ejes (y otras operaciones) involucra hablar con la figura que contiene nuestro mapa. Las gráficas de matplotlib están organizadas en una estructura jerárquica que, en su forma más sencilla contiene un objeto de tipo figure, adentro del cual hay uno o más objetos de tipo axes. Son estos últimos los que se encargan propiamente de dibujar la gráfica.\nPara poder manipular las propiedades de nuestros mapas (o gráficas en general), necesitamos tener acceso a los objetos figure y axes. La forma más simple es instanciándolos diréctamente a la hora de crear nuestra gráfica.\n\nfig , ax = plt.subplots(1, figsize=(10,10))  # Preparación de la Figura y sus Ejes, así como el tamaño\nagebs.plot(ax = ax)                          # Grafica la capa de polígonos sobre la fila\nax.set_axis_off()                            # Eliminar las ventanas de los ejes\nplt.show()                                     # Mostrar el resultado\n\n\n\n\nAnalicemos a detalle cada una de las líneas anteriores: 1. Creamos una figura con el nombre fig que contiene un sólo eje llamado ax. Para eso usamos la función subplots(). Este método genera dos elementos que pueden ser asignados a dos variables diferentes (fig y ax), lo cual se logra colocando sus nombres al inicio de la línea, separándolos por comas. 2. Graficamos las geometrías de la misma forma que en los ejemplos anteriores, esta vez indicándole a la función que dibuje los polígonos en el eje que generamos anteriormente, a través del argumento ax. 3. Finalmente, eliminamos los ejes llamando al método set_axis_off() del eje.\n\n\n\n5.2.2 Añadir un Título\nAsí como los ejes son parte del objeto axes, el título de la gráfica es propiedad del objeto figure. Para cambiarlo simplemente llamamos al método suptitle de la figura.\n\nfig , ax = plt.subplots(1, figsize=(10,10))\nagebs.plot(ax = ax)\nfig.suptitle(\"AGEB's de la CDMX\") # A través de la función '.suptitle()' aplicada a la figura se coloca el título.\nplt.show()\n\n\n\n\n\n5.2.2.1 Cambiar el Tamaño del Mapa\nEn los primeros ejemplos fijamos el tamaño de la gráfica a través del argumento figsize del m´todo plot() de nuestros GeoDataFrames. También podemos cambiarlo en el momento de crear las figuras.\n\nfig , ax = plt.subplots(1, figsize=(4,4))\nagebs.plot(ax = ax)\nfig.suptitle(\"AGEB's de la CDMX\")\nplt.show()\n\n\n\n\n\n\n\n5.2.3 Propiedades del dibujo\nHasta aquí hemos modificado algunas propiedades generales de la figura y los ejes. También queremos tener una forma de modoficar propiedades del dibujo como el estilo de las líneas y de los polígonos.\nCambiar estas propiedades se hace desde el método plot()y, aunque tienen unos nombres medio extraños, es muy fácil cambiarlos.\n\nfig , ax = plt.subplots(1, figsize=(10,10))\n# En la siguiente línea se modifica el color de los polígonos ('facecolor'), del borde ('edgecolor') y su ancho ('linewidth')\nagebs.plot(linewidth = 0.1, facecolor = '#50C879', edgecolor='#000702',ax = ax)\nplt.show()\n\n\n\n\nSimplemente modoficamos los valores de los parámetros linewidth, facecolor y edgecolor que modifican el ancho de la línea, el color del polígono y el color del borde respectivamente.\nDe la misma forma podemos cambiar los colores y ancho de una capa de líneas.\n\nfig , ax = plt.subplots(1, figsize=(10,10))\nvias.plot(linewidth = 0.5, color = 'red',ax = ax)\nplt.show()\n\n\n\n\nAhora para cambiar el color usamos la propiedad color, las líneas no tienen facecolor."
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#sistemas-de-coordenadas",
    "href": "parte_1/05_intro_geopandas.html#sistemas-de-coordenadas",
    "title": "5  Objetos geográficos",
    "section": "5.3 Sistemas de coordenadas",
    "text": "5.3 Sistemas de coordenadas\nUna de las características distintivas de los datos geográficos es el Sistema de Referencia espacial (CRS, en inglés). El CRS nos dice cómo están referidos los datos a la superficie de la tierra.\nGeopandas provee métodos para trabajar con sistemas de referencia. Primero, si la fuente de donde leímos los datos, tiene una referencia, esta se va a conservar a través de la propiedad crs:\n\nagebs.crs\n\n<Derived Projected CRS: PROJCS[\"MEXICO_ITRF_2008_LCC\",GEOGCS[\"ITRF2008\",DA ...>\nName: MEXICO_ITRF_2008_LCC\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: unnamed\n- method: Lambert Conic Conformal (2SP)\nDatum: International Terrestrial Reference Frame 2008\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nPara reproyectar nuestros datos a un sistema diferente se requiere conocer el código SRID del nuevo sistema. En el sitio Spatial Reference podemos buscar los códigos de diferentes proyecciones y sistemas de referencia.\nPor ejemplo, para cambiar la proyección de nuestros datos al sistema UTM, hacemos:\n\nagebs.to_crs(32614).plot(figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nDebido a que el área de trabajo es relativamente pequeña, la forma de los polígonos observados es casi idéntica a la observada en los ejemplos anteriores. Sin embargo, la escala utilizada para la gráfica es ahora diferente. ___ ### Para practicar\nGenera un mapa de las AGEB’s de la Ciudad de México que posea las siguientes características: * Posea un título * No muestre los ejes * Posea un tamaño de 10in x 11in * Todos los polígonos tengan un relleno del color “#525252” y sean completamente opacos * Los bordes del polígono tengan un ancho de 0.3 y sean del color “#B9EBE3” ___"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#múltiples-capas",
    "href": "parte_1/05_intro_geopandas.html#múltiples-capas",
    "title": "5  Objetos geográficos",
    "section": "5.4 Múltiples Capas",
    "text": "5.4 Múltiples Capas\nPor lo pronto nuestros mapas se han limitado a una sola capa, sin embargo es bastante común querer sobreponer visualmente diferentes capas de información. Hacer esto con Geopandas es muy sencillo, simplemente necesitamos agragar los nuevos dibujos a los ejes ya existentes. Por ejemplo, podemos combinar en un sólo mapa las vialidades de la alcaldía Cuahutemoc con sus límites.\n\n# Primero se necesita importar el polígono de la alcaldía, pues no se ha utilizado en ejemplos anteriores\ncuauhtemoc = gpd.read_file(\"datos/cuauhtemoc.geojson\")\n# Se establece la figura y su eje con '.subplots()'\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade primero el polígono base en la fila\ncuauhtemoc.plot(ax = ax)\n# Y después se colocan las vialidades en la misma fila\nvias.plot(ax = ax)\n# Por último, y como se ha hecho en ejemplos anteriores, el comando para mostrar el resultado\nplt.show()\n\n\n\n\nSe alcanza a ver que ahí debajo del polígono están las líneas, pero claramente necesitamos trabajar más el mapa. Cambiemos los colores y la transparencia.\n\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Para el polígono, se utilizan los argumentos aplicables al polígono\ncuauhtemoc.plot(ax = ax, facecolor = 'grey', alpha = 0.8)\n# Y pra las líneas, se utilizan los argumentos usados para las líneas\nvias.plot(ax = ax, color = 'white', linewidth = 0.5)\nplt.show()"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#paletas-de-colores",
    "href": "parte_1/05_intro_geopandas.html#paletas-de-colores",
    "title": "5  Objetos geográficos",
    "section": "5.5 Paletas de Colores",
    "text": "5.5 Paletas de Colores\nLa elección de colores en un mapa está muy relacionada con la efectividad para comunicar. Aunque en algunos casos elegir estos colores a mano sea suficiente, a veces es conveniente recurrir a paletas ya previamente diseñadas (por ejemplo, para dseñar mapas accesibles a personas daltónicas).\nEn este caso vamos a usar la librería palettable que nos provee con diferentes paletas de colores.\nLlamemos entonces a una paleta de colores y guardemosla en una variable.\n\npaleta = pltt.wesanderson.Darjeeling2_5.hex_colors\n\nPara visualizar la apariencia de esta paleta, se recurre a la función palplot() de seaborn:\n\npalplot(paleta)\n\n\n\n\nSi se ve la variable por sí misma, podrá notarse que no se trata más que de una lista de colores bajo su Código Hexadecimal:\n\npaleta\n\n['#D5E3D8', '#618A98', '#F9DA95', '#AE4B16', '#787064']\n\n\nEsta paleta será la que le dará color a nuestro mapa. Como elemento adicional, se colocarán las colonias en las que se subdivide la alcaldía y en donde se han registrado 10 o más homicidios entre los años 2016 a 2018, por lo que primero es necesario importarlas:\n\nagebs_cuau = gpd.read_file(\"datos/agebs_cuauhtemoc.geojson\")\n\nPara esto, no se utilizará nada más que lo aprendido en secciones anteriores de la práctica, siendo la única diferencia que dentro de los argumentos de color correspondientes se colocará el Código Hexadecimal del color que se pretenda utilizar para esa capa:\n\n# Se define la figura con sus respectivas filas\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade el polígono base de la alcaldía\ncuauhtemoc.plot(ax = ax, facecolor = '#F9DA95', edgecolor= '#787064', linewidth = 3)\n# Se añaden las vías\nvias.plot(ax = ax, color = '#AE4B16', linewidth = 0.5)\n# Se aladen las AGEB's con el mayor número de homicidios\nagebs_cuau.plot(ax = ax, facecolor = '#D5E3D8', edgecolor = '#618A98', linewidth = 2, alpha = 0.8)\n# Se remueve el marco con los ejes de la fila\nax.set_axis_off()\nplt.show()"
  },
  {
    "objectID": "parte_1/05_intro_geopandas.html#exportar-mapas",
    "href": "parte_1/05_intro_geopandas.html#exportar-mapas",
    "title": "5  Objetos geográficos",
    "section": "5.6 Exportar Mapas",
    "text": "5.6 Exportar Mapas\nUna vez producido el mapa final, puede que se busque el exportar la imagen de modo que pueda ser colocada en un reporte, artículo, sitio web, etc. Para exportar mapas en Python, basta con sustituir la función plt.show() por plt.savefig() al final de las líneas de código para especificar el dónde y cómo guardarla. Por ejemplo, para guardar el mapa anterior en un archivo de tipo .png en la carpeta data, donde se encuentra toda la información con la que se ha trabajado hasta ahora, simplemente basta con:\n\n# Se define la figura con sus respectivas filas\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade el polígono base de la alcaldía\ncuauhtemoc.plot(ax = ax, facecolor = '#F9DA95', edgecolor= '#787064', linewidth = 3)\n# Se añaden las vías\nvias.plot(ax = ax, color = '#AE4B16', linewidth = 0.5)\n# Se aladen las AGEB's con el mayor número de homicidios\nagebs_cuau.plot(ax = ax, facecolor = '#D5E3D8', edgecolor = '#618A98', linewidth = 2, alpha = 0.8)\n# Se remueve el marco con los ejes de la fila\nax.set_axis_off()\n\n# Se guarda el mapa como un archivo PNG en la carpeta 'data/'\nplt.savefig('datos/mapa_final.png')\n\n\n\n\nSi se revisa la carpeta, se encontrará la imagen .png con el mapa.\nLa función plt.savefig(), de la librería matplotlib.pyplot contiene un gran número de parámetros y opciones para trabajar. Dado que el tamaño del mapa generado no es muy grande, es posible incrementar la calidad de éste a través del argumento dpi, o Puntos Por Pulgada (dpi), el cual es una medida estándar de la resolución de las imágenes. Por ejemplo, para obtener una imagen de Alta Definición (HD), se puede cambiar el argumento a 1,080:\nImportante - Si el proceso tarda demasiado, cambiar el argumento por 500 también funciona, pues también arrojará una imagen de buena calidad y más fácil de generar.\n\n# Se define la figura con sus respectivas filas\nfig , ax = plt.subplots(1, figsize = (10,10))\n# Se añade el polígono base de la alcaldía\ncuauhtemoc.plot(ax = ax, facecolor = '#F9DA95', edgecolor= '#787064', linewidth = 3)\n# Se añaden las vías\nvias.plot(ax = ax, color = '#AE4B16', linewidth = 0.5)\n# Se aladen las AGEB's con el mayor número de homicidios\nagebs_cuau.plot(ax = ax, facecolor = '#D5E3D8', edgecolor = '#618A98', linewidth = 2, alpha = 0.8)\n# Se remueve el marco con los ejes de la fila\nax.set_axis_off()\n\n# Se guarda el mapa como un archivo PNG en la carpeta 'data/'\nplt.savefig('datos/mapa_final.png', dpi = 1080)"
  },
  {
    "objectID": "parte_1/06_manipulacion_espacial.html#manipulación-de-datos-espaciales",
    "href": "parte_1/06_manipulacion_espacial.html#manipulación-de-datos-espaciales",
    "title": "6  Trabajando con objetos espaciales",
    "section": "6.1 Manipulación de Datos Espaciales",
    "text": "6.1 Manipulación de Datos Espaciales\nEn el taller anterior vimos una introducción a algunos de los conceptos básicos del trabajo con datos geográficos usando GeoPandas. Ahora vamos a usar la librería para realizar algunas manipulaciones más avanzadas.\nComo siempre, primero importamos las librerías.\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n6.1.1 Datos del Censo\nEn este taller vamos a trabajar una vez más con los datos del Censo de Población y Vivienda del INEGI. Lo primero que vamos a hacer es leer los datos y volverlos a procesar para obtener los totales por AGEB.\n\ndb = pd.read_csv('datos/conjunto_de_datos_ageb_urbana_09_cpv2020.zip',\n                 dtype={'ENTIDAD': object,\n                        'MUN':object,\n                        'LOC':object,\n                        'AGEB':object})\ndb = db.loc[db['NOM_LOC'] == 'Total AGEB urbana']\ndb = (db  \n      .replace('999999999', np.nan)\n      .replace('99999999', np.nan)\n      .replace('*', np.nan)\n      .replace('N/D', np.nan))\ndiccionario = pd.read_csv('datos/diccionario_datos_ageb_urbana_09_cpv2020.csv', skiprows=3)\ncampos_datos = diccionario.loc[8:,]['Mnemónico']\ndb[campos_datos] = db[campos_datos].astype('float')\ndb['AGEB_cvgeo'] = db['ENTIDAD'] + db['MUN'] + db['LOC'] + db['AGEB']\ndb.head()\n\n/tmp/ipykernel_4795/1463692114.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  db['AGEB_cvgeo'] = db['ENTIDAD'] + db['MUN'] + db['LOC'] + db['AGEB']\n\n\n\n\n\n\n  \n    \n      \n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      NOM_LOC\n      AGEB\n      MZA\n      POBTOT\n      POBFEM\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      AGEB_cvgeo\n    \n  \n  \n    \n      3\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0010\n      0\n      3183.0\n      1695.0\n      ...\n      772.0\n      692.0\n      313.0\n      221.0\n      145.0\n      8.0\n      14.0\n      148.0\n      5.0\n      0900200010010\n    \n    \n      30\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0025\n      0\n      5593.0\n      2915.0\n      ...\n      1510.0\n      1203.0\n      478.0\n      349.0\n      238.0\n      28.0\n      68.0\n      393.0\n      14.0\n      0900200010025\n    \n    \n      82\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      003A\n      0\n      4235.0\n      2232.0\n      ...\n      1049.0\n      878.0\n      361.0\n      339.0\n      247.0\n      5.0\n      12.0\n      250.0\n      NaN\n      090020001003A\n    \n    \n      116\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0044\n      0\n      4768.0\n      2551.0\n      ...\n      1237.0\n      1076.0\n      481.0\n      452.0\n      294.0\n      10.0\n      17.0\n      254.0\n      NaN\n      0900200010044\n    \n    \n      163\n      09\n      Ciudad de México\n      002\n      Azcapotzalco\n      0001\n      Total AGEB urbana\n      0097\n      0\n      2176.0\n      1115.0\n      ...\n      562.0\n      507.0\n      276.0\n      260.0\n      153.0\n      4.0\n      3.0\n      70.0\n      0.0\n      0900200010097\n    \n  \n\n5 rows × 231 columns\n\n\n\n\n\n6.1.2 Unión de Tablas\nEn la práctica anterior usamos las geopmetrías de la AGEBS. Ahora podemos unir ambas tablas para tener una base de datos completa a partir de la cual hacer análisis espacial.\nLeamos entonces los datos de las geometrías.\n\nagebs = gpd.read_file(\"datos/agebs_cdmx.zip\")  # Importar los datos espaciales\nagebs\n\n\n\n\n\n  \n    \n      \n      CVEGEO\n      CVE_ENT\n      CVE_MUN\n      CVE_LOC\n      CVE_AGEB\n      geometry\n    \n  \n  \n    \n      0\n      0901000011716\n      09\n      010\n      0001\n      1716\n      POLYGON ((2787237.541 816989.461, 2787288.728 ...\n    \n    \n      1\n      0901000012150\n      09\n      010\n      0001\n      2150\n      POLYGON ((2794154.458 823013.444, 2794155.774 ...\n    \n    \n      2\n      0901000011133\n      09\n      010\n      0001\n      1133\n      POLYGON ((2795690.723 820050.788, 2795684.238 ...\n    \n    \n      3\n      0901000011307\n      09\n      010\n      0001\n      1307\n      POLYGON ((2792584.475 815678.668, 2792624.325 ...\n    \n    \n      4\n      0901000010281\n      09\n      010\n      0001\n      0281\n      POLYGON ((2788845.392 823526.074, 2788840.549 ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2426\n      0900700012564\n      09\n      007\n      0001\n      2564\n      POLYGON ((2814016.268 821043.511, 2814019.319 ...\n    \n    \n      2427\n      0900700012615\n      09\n      007\n      0001\n      2615\n      POLYGON ((2814358.791 820744.850, 2814405.087 ...\n    \n    \n      2428\n      0900700012969\n      09\n      007\n      0001\n      2969\n      POLYGON ((2815993.470 819777.763, 2816019.848 ...\n    \n    \n      2429\n      0900700013721\n      09\n      007\n      0001\n      3721\n      POLYGON ((2807966.150 821578.350, 2807941.550 ...\n    \n    \n      2430\n      0900700011034\n      09\n      007\n      0001\n      1034\n      POLYGON ((2808319.196 821552.683, 2808243.251 ...\n    \n  \n\n2431 rows × 6 columns\n\n\n\nEl campo a través del cual se deben unir es CVEGEO en las geometrías y AGEB_cvgeo en los tabulados. Como ya hemos visto hay diferentes formas de lograr esto, en este caso vamos a usar join() que nos permite unir a través de una columna o índice en la bese izquierda (la que llama a la función) y el índice en la base derecha. Observen entonces lo que sicede si unimos a través de los índices:\n\nagebs.set_index('CVEGEO').join(db.set_index('AGEB_cvgeo'), how='inner')\n\n\n\n\n\n  \n    \n      \n      CVE_ENT\n      CVE_MUN\n      CVE_LOC\n      CVE_AGEB\n      geometry\n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      0901000011716\n      09\n      010\n      0001\n      1716\n      POLYGON ((2787237.541 816989.461, 2787288.728 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      1119.0\n      1660.0\n      1181.0\n      644.0\n      279.0\n      210.0\n      45.0\n      82.0\n      601.0\n      9.0\n    \n    \n      0901000012150\n      09\n      010\n      0001\n      2150\n      POLYGON ((2794154.458 823013.444, 2794155.774 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      1162.0\n      1497.0\n      1290.0\n      993.0\n      695.0\n      450.0\n      21.0\n      24.0\n      261.0\n      NaN\n    \n    \n      0901000011133\n      09\n      010\n      0001\n      1133\n      POLYGON ((2795690.723 820050.788, 2795684.238 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      689.0\n      746.0\n      733.0\n      568.0\n      545.0\n      212.0\n      19.0\n      NaN\n      23.0\n      0.0\n    \n    \n      0901000011307\n      09\n      010\n      0001\n      1307\n      POLYGON ((2792584.475 815678.668, 2792624.325 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      783.0\n      839.0\n      831.0\n      768.0\n      640.0\n      348.0\n      NaN\n      0.0\n      18.0\n      0.0\n    \n    \n      0901000010281\n      09\n      010\n      0001\n      0281\n      POLYGON ((2788845.392 823526.074, 2788840.549 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      64.0\n      68.0\n      64.0\n      60.0\n      48.0\n      30.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      0900700012564\n      09\n      007\n      0001\n      2564\n      POLYGON ((2814016.268 821043.511, 2814019.319 ...\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      ...\n      1064.0\n      1512.0\n      1103.0\n      331.0\n      248.0\n      154.0\n      48.0\n      120.0\n      645.0\n      18.0\n    \n    \n      0900700012615\n      09\n      007\n      0001\n      2615\n      POLYGON ((2814358.791 820744.850, 2814405.087 ...\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      ...\n      1181.0\n      1785.0\n      1206.0\n      406.0\n      288.0\n      204.0\n      64.0\n      101.0\n      712.0\n      11.0\n    \n    \n      0900700012969\n      09\n      007\n      0001\n      2969\n      POLYGON ((2815993.470 819777.763, 2816019.848 ...\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      ...\n      400.0\n      543.0\n      394.0\n      93.0\n      66.0\n      54.0\n      18.0\n      36.0\n      246.0\n      4.0\n    \n    \n      0900700013721\n      09\n      007\n      0001\n      3721\n      POLYGON ((2807966.150 821578.350, 2807941.550 ...\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      ...\n      975.0\n      1157.0\n      1049.0\n      606.0\n      342.0\n      192.0\n      10.0\n      32.0\n      197.0\n      4.0\n    \n    \n      0900700011034\n      09\n      007\n      0001\n      1034\n      POLYGON ((2808319.196 821552.683, 2808243.251 ...\n      09\n      Ciudad de México\n      007\n      Iztapalapa\n      0001\n      ...\n      1450.0\n      1669.0\n      1443.0\n      920.0\n      562.0\n      366.0\n      17.0\n      45.0\n      350.0\n      5.0\n    \n  \n\n2431 rows × 235 columns\n\n\n\nPara poder usar join() fijamos los índices en las dos bases usando set_index(). De esta forma la unión se hace sobre las filas en las que los valores de ambas columnas coincidan. La condicion how='inner' especifica el tipo de unión que queremos, en este caso la unión interior.\nYa que entendimos la lógica, guardemos el resultado de la unión en la variable en donde tenemos las geometrías:\n\nagebs = agebs.set_index('CVEGEO').join(db.set_index('AGEB_cvgeo'), how='inner')\nagebs.head()\n\n\n\n\n\n  \n    \n      \n      CVE_ENT\n      CVE_MUN\n      CVE_LOC\n      CVE_AGEB\n      geometry\n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      ...\n      VPH_TELEF\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n    \n  \n  \n    \n      0901000011716\n      09\n      010\n      0001\n      1716\n      POLYGON ((2787237.541 816989.461, 2787288.728 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      1119.0\n      1660.0\n      1181.0\n      644.0\n      279.0\n      210.0\n      45.0\n      82.0\n      601.0\n      9.0\n    \n    \n      0901000012150\n      09\n      010\n      0001\n      2150\n      POLYGON ((2794154.458 823013.444, 2794155.774 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      1162.0\n      1497.0\n      1290.0\n      993.0\n      695.0\n      450.0\n      21.0\n      24.0\n      261.0\n      NaN\n    \n    \n      0901000011133\n      09\n      010\n      0001\n      1133\n      POLYGON ((2795690.723 820050.788, 2795684.238 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      689.0\n      746.0\n      733.0\n      568.0\n      545.0\n      212.0\n      19.0\n      NaN\n      23.0\n      0.0\n    \n    \n      0901000011307\n      09\n      010\n      0001\n      1307\n      POLYGON ((2792584.475 815678.668, 2792624.325 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      783.0\n      839.0\n      831.0\n      768.0\n      640.0\n      348.0\n      NaN\n      0.0\n      18.0\n      0.0\n    \n    \n      0901000010281\n      09\n      010\n      0001\n      0281\n      POLYGON ((2788845.392 823526.074, 2788840.549 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      64.0\n      68.0\n      64.0\n      60.0\n      48.0\n      30.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows × 235 columns\n\n\n\nAhora que tenemos ya los tabulados y las geometrías en un mismo GeoDataFrame, podemos fácilmente hacer cálculos y mapas a partir de los datos. Por ejemplo, veamos dónde quedan las 10 Agebs con mayor población.\n\nagebs_mas_pobladas = agebs.sort_values('POBTOT', ascending = False).head(10)\nfig, ax = plt.subplots(1, figsize=(10, 10))\nagebs.plot(facecolor='black', linewidth=0.025, ax=ax)              # Capa Base de AGEB's\nagebs_mas_pobladas.plot(alpha=1, facecolor='red', linewidth=0, ax=ax) # Capa AGEB's más pobladas\nax.set_axis_off()\nfig.suptitle(\"AGEB's de la CDMX con Mayor Población\")\nplt.show()\n\n\n\n\n\n\n6.1.3 Manipulaciones Espaciales\nAdemás de las operaciones basadas únicamente en valores, como las realizadas anteriormente, es posible realizar sobre un GeoDataFrame una gran variedad de operaciones encontradas en los SIG. A continuación se detallarán algunas de las más comunes\n\n6.1.3.1 Cálculo de Centroides\nEn algunos casos, resulta útil simplificar un polígono en un sólo punto y, para ello, se calculan los Centroides (siendo algo como el análogo espacial de la media estadística). El siguiente comando dará como resultado un objeto del tipo GeoSeries (una sola columna con datos espaciales) con los Centroides de los polígonos contenidos en un GeoDataFrame:\n\ncentroides = agebs.centroid\ncentroides.head()\n\n0901000011716    POINT (2787091.708 816590.463)\n0901000012150    POINT (2793986.972 823047.548)\n0901000011133    POINT (2794967.016 819439.549)\n0901000011307    POINT (2792230.506 815397.361)\n0901000010281    POINT (2788669.707 823554.634)\ndtype: geometry\n\n\nLos centroides son un objeto del tipo GeoSeries, el equivalente en Geopandas a una Serie de Pandas.\n\ncentroides.plot(figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\n6.1.4 Punto en Polígono (Point-In-Polygon)\nUna operación común es preguntar si un punto determinado cae dentro o no de un polígono. Para este tipo de operaciones GeoPandas provee un conjunto de predicados espaciales. El que nos permite responder a la pregunta que planteamos es contains() que regresa cierto siempre que la geometría que llama contenga a la geometría con que se compara:\n\npoligono = agebs['geometry'][0]  # Se aisla el primer polígono dela tabla\npunto1 = centroides[0]              # Se aisla el primer punto de la serie\npunto2 = centroides[1]              # Se aisla el segundo punto de la serie\n\n\npoligono.contains(punto1)\n\nTrue\n\n\n\npoligono.contains(punto2)\n\nFalse\n\n\nEl método anterior permite realizar una verificación rápida y cualitativa de si un punto se encuentra dentro de un polígono; sin embargo, en muchos otros casos esto no resulta ser muy eficiente, por lo que se recurre a una operación conocida como Spatial Join; éstos serán estudiados más a fondo en futuras prácticas.\n\n\n6.1.5 Buffers\nLos Buffers son parte de las operaciones clásicas de un SIG, y consisten en trazar un área alrededor de una geometría en particular, dado un radio específico. Éstos resultan bastate útiles al momento de combinarlos, por ejemplo, con operaciones de Point-In-Polygon para calcular valores de accesibilidad, áreas de influencia, entre otros.\nPara crear un Buffer a través de GeoPandas, puede utilizarse el método buffer(), al cual se le coloca como argumento el radio deseado. Es importante tomar en cuenta que el radio especificado necesita encontrarse en las mismas unidades que el Sistema de Coordenadas de Referencia (CRS) de la geometría con la que se esté trabajando. Por ejemplo, revisando la capa importada anteriormente de Estaciones del Metro:\n\nestaciones = gpd.read_file(\"datos/estaciones_metro.zip\")\nestaciones.crs\n\n<Derived Projected CRS: EPSG:32614>\nName: WGS 84 / UTM zone 14N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 102°W and 96°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Manitoba; Nunavut; Saskatchewan. Mexico. United States (USA).\n- bounds: (-102.0, 0.0, -96.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 14N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLa propiedad crs indica que se trata de la proyección con Código EPSG 32614, de la cual, al investigar sobre ella, se tiene que se trata de una proyección que trabaja en metros. Como tal, si se buscara generar un Buffer de 500m alrededor de cada estación, simplemente se tendría que:\n\nbuff = estaciones.buffer(500)\nbuff.head()\n\n0    POLYGON ((485905.843 2149860.572, 485903.435 2...\n1    POLYGON ((486208.110 2152724.378, 486205.702 2...\n2    POLYGON ((480784.558 2142470.874, 480782.151 2...\n3    POLYGON ((487661.939 2146081.726, 487659.532 2...\n4    POLYGON ((480876.875 2142406.938, 480874.468 2...\ndtype: geometry\n\n\nPara representar éstos en un mapa, se recurre a los métodos estudiados anteriormente:\n\nfig, fila = plt.subplots(1, figsize=(10, 10))\n\n# Graficar los Buffers\nbuff.plot(ax = fila , alpha = 0.5 , facecolor = 'red', linewidth = 0)\n\n# Graficar las Estaciones de Metro sobre las referencias\nestaciones.plot(ax = fila , color = 'green')\nplt.show()\n\n\n\n\n\n6.1.5.1 Ejercicio Opcional\nGenera un mapa de la Ciudad de México donde los polígonos de las AGEB’s sean de color negro, y sobre ellos y de color amarillo los Buffers a 250m de cada uno de sus centroides.\n\n\n\n6.1.6 Coropletas\nPara terminar la práctica de geovisualización vamos a hacer mapas de coropletas usando dos métodos diferentes. Primero vamos a usar directamente GeoPandas para generar los mapas y después vamos a usar ipyLeaflet para hacer una visualización interactiva.\nPrimero, recordemos que, dado que ya tenemos los datos de población unidos a las geometrías de las AGEBS, hacer un primer mapa usando GeoPandas, es tan sencillo como pasarle el nombre de la columna que queremos usar para colorear el mapa\n\nagebs.plot('POBTOT', figsize=(10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nIncluir una leyenda también es muy sencillo\n\nagebs.plot('POBTOT', figsize=(10,10), legend=True)\n\n<AxesSubplot: >\n\n\n\n\n\nY cambiar la paleta de colores\n\nagebs.plot('POBTOT', figsize=(10,10), legend=True, cmap='OrRd')\n\n<AxesSubplot: >\n\n\n\n\n\nEn la documentación de GeoDtaFrame.plot() pueden ver la lista completa de opciones.\nEn estos primeros mapas que hemos hecho usamos una escala continua para representar la variable de interés. Otra forma de representar la variación espacial es utilizando un esquema de clasificación discreto sobre nuestra variable de interés, por ejemplo cuantiles, intervalos iguales, etcétera. Para esto, GeoDtaFrame.plot() admite pasarle el parámetro scheme que toma cualquier esquema de clasificación admitido por mapclassify.\n\nagebs.plot('POBTOT', figsize=(10,10), legend=True, cmap='OrRd', scheme='quantiles')\n\n<AxesSubplot: >\n\n\n\n\n\n\n6.1.6.1 Ejercicio\nPrueben diferentes esquemas de clasificación (aquí pueden encontrar la lista de esquemas disponibles) y discutan sobre qué esquema representa mejor la variación espacial de los datos\n\n\n\n6.1.7 Mejorando el estilo\nLos mapas que hemos hecho son relativamente sencillos, para darles una mejor presentación podemos tomar algunas cosas que ya hemos aprendido, por ejemplo eliminar los ejes y ponerles un título:\n\nfig , fila = plt.subplots(1, figsize=(10,10))\nagebs.plot('POBTOT', figsize=(10,10), legend=True, cmap='OrRd', scheme='quantiles', ax=fila)\nfig.suptitle(\"Población por AGEB\")\nfila.set_axis_off()  \nplt.show()\n\n\n\n\nTambién es posible agregar un mapa base utilizando la librería contextily\n\nimport contextily as ctx\n\n\nfig , fila = plt.subplots(1, figsize=(10,10))\nagebs.plot('POBTOT', figsize=(10,10), legend=True, cmap='OrRd', scheme='quantiles', ax=fila, alpha=0.8)\nfig.suptitle(\"Población por AGEB\")\nfila.set_axis_off()\nctx.add_basemap(fila, source=ctx.providers.OpenStreetMap.Mapnik, crs=agebs.crs.to_string())\nplt.show()\n\n\n\n\n\n6.1.7.1 Ejercicio\nPrueben usando diferentes mapas base, y cambiando el fondo de la imágen.\n\n\n\n6.1.8 Varios mapas en la misma figura\nA veces queremos hacer una imagen que nos permita comparar rápidamente una variable, para eso es conveniente poner juntos dos mapas. Pensemos en este momento en comparar la población por AGEB con la densidad de población por AGEB. Obviamente el pri er paso es calcular la densidad de población, para esto vamos a usar la propiedad área de los GeoDataFrames. Fíjense como reproyectamos antes de calcular el area para obtener un valor en metros cuadrados\n\nagebs.to_crs(32614).area\n\n0901000011716    2.667296e+05\n0901000012150    2.205941e+05\n0901000011133    5.884493e+05\n0901000011307    1.064274e+06\n0901000010281    1.915020e+05\n                     ...     \n0900700012564    3.412877e+05\n0900700012615    2.797808e+05\n0900700012969    1.567320e+05\n0900700013721    2.219631e+05\n0900700011034    3.472613e+05\nLength: 2431, dtype: float64\n\n\n\nagebs['densidad_pob'] = agebs['POBTOT'] / agebs.to_crs(32614).area\nagebs.head()\n\n\n\n\n\n  \n    \n      \n      CVE_ENT\n      CVE_MUN\n      CVE_LOC\n      CVE_AGEB\n      geometry\n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      ...\n      VPH_CEL\n      VPH_INTER\n      VPH_STVP\n      VPH_SPMVPI\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      densidad_pob\n    \n  \n  \n    \n      0901000011716\n      09\n      010\n      0001\n      1716\n      POLYGON ((2787237.541 816989.461, 2787288.728 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      1660.0\n      1181.0\n      644.0\n      279.0\n      210.0\n      45.0\n      82.0\n      601.0\n      9.0\n      0.026401\n    \n    \n      0901000012150\n      09\n      010\n      0001\n      2150\n      POLYGON ((2794154.458 823013.444, 2794155.774 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      1497.0\n      1290.0\n      993.0\n      695.0\n      450.0\n      21.0\n      24.0\n      261.0\n      NaN\n      0.020798\n    \n    \n      0901000011133\n      09\n      010\n      0001\n      1133\n      POLYGON ((2795690.723 820050.788, 2795684.238 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      746.0\n      733.0\n      568.0\n      545.0\n      212.0\n      19.0\n      NaN\n      23.0\n      0.0\n      0.003744\n    \n    \n      0901000011307\n      09\n      010\n      0001\n      1307\n      POLYGON ((2792584.475 815678.668, 2792624.325 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      839.0\n      831.0\n      768.0\n      640.0\n      348.0\n      NaN\n      0.0\n      18.0\n      0.0\n      0.002738\n    \n    \n      0901000010281\n      09\n      010\n      0001\n      0281\n      POLYGON ((2788845.392 823526.074, 2788840.549 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      68.0\n      64.0\n      60.0\n      48.0\n      30.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.001238\n    \n  \n\n5 rows × 236 columns\n\n\n\nAhora sí podemos hacer una figura que incluya dos mapas. Hasta ahora siempre hemos usado fig , ax = plt.subplots(1, figsize=(10,10)) para hacer un subplot con una sóla gráfica, lo que vamos a hacer es usarlo ahora para obtener dos ejes en los que graficar.\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12)) # Pedimos subfiguras con un renglón y dos columnas\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))# Pedimos subfiguras con un renglón y dos columnas\nagebs.plot('POBTOT', legend=True, cmap='OrRd', scheme='quantiles', ax=ax1, alpha=0.8)# Graficamos en el primer ax\nax1.set_title(\"Población por AGEB\") # Este título está en el nivel ax, no en el figure\nax1.set_axis_off()\nctx.add_basemap(ax1, source=ctx.providers.OpenStreetMap.Mapnik, crs=agebs.crs.to_string())\nagebs.plot('densidad_pob', legend=True, cmap='OrRd', scheme='quantiles', ax=ax2, alpha=0.8)# Graficamos en el segndo ax\nax2.set_title(\"Densidad de Población por AGEB\") # Este título está en el nivel ax, no en el figure\nax2.set_axis_off()\nctx.add_basemap(ax2, source=ctx.providers.OpenStreetMap.Mapnik, crs=agebs.crs.to_string())\nplt.tight_layout() # Para disminuir el espacio entre gráficas\n\n\n\n\nComo pueden ver es relatívamente fácil poner dos gráficas, sólo hay que especificar cómo las queremos organizar y entender que subplots nos va a regresar un array de ejes. Para que quede aún más claro, vamos a hacer ahora cuatro mapas, comparando los porcentajes de poblaión para distintos grupos de edad. Primero vamos a calcular las variables que vamos a mapear\n\nagebs['prop_0a2'] = agebs['P_0A2'] / agebs['POBTOT']\nagebs['prop_12a14'] = agebs['P_12A14'] / agebs['POBTOT']\nagebs['prop_18a24'] = agebs['P_18A24'] / agebs['POBTOT']\nagebs['prop_60ymas'] = agebs['P_60YMAS'] / agebs['POBTOT']\nagebs.head()\n\n\n\n\n\n  \n    \n      \n      CVE_ENT\n      CVE_MUN\n      CVE_LOC\n      CVE_AGEB\n      geometry\n      ENTIDAD\n      NOM_ENT\n      MUN\n      NOM_MUN\n      LOC\n      ...\n      VPH_CVJ\n      VPH_SINRTV\n      VPH_SINLTC\n      VPH_SINCINT\n      VPH_SINTIC\n      densidad_pob\n      prop_0a2\n      prop_12a14\n      prop_18a24\n      prop_60ymas\n    \n  \n  \n    \n      0901000011716\n      09\n      010\n      0001\n      1716\n      POLYGON ((2787237.541 816989.461, 2787288.728 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      210.0\n      45.0\n      82.0\n      601.0\n      9.0\n      0.026401\n      0.032803\n      0.054530\n      0.119710\n      0.131497\n    \n    \n      0901000012150\n      09\n      010\n      0001\n      2150\n      POLYGON ((2794154.458 823013.444, 2794155.774 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      450.0\n      21.0\n      24.0\n      261.0\n      NaN\n      0.020798\n      0.022014\n      0.034656\n      0.094377\n      0.192241\n    \n    \n      0901000011133\n      09\n      010\n      0001\n      1133\n      POLYGON ((2795690.723 820050.788, 2795684.238 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      212.0\n      19.0\n      NaN\n      23.0\n      0.0\n      0.003744\n      0.010894\n      0.033137\n      0.088970\n      0.261008\n    \n    \n      0901000011307\n      09\n      010\n      0001\n      1307\n      POLYGON ((2792584.475 815678.668, 2792624.325 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      348.0\n      NaN\n      0.0\n      18.0\n      0.0\n      0.002738\n      0.018874\n      0.027111\n      0.120453\n      0.237131\n    \n    \n      0901000010281\n      09\n      010\n      0001\n      0281\n      POLYGON ((2788845.392 823526.074, 2788840.549 ...\n      09\n      Ciudad de México\n      010\n      Álvaro Obregón\n      0001\n      ...\n      30.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.001238\n      0.012658\n      0.042194\n      0.067511\n      0.189873\n    \n  \n\n5 rows × 240 columns\n\n\n\nAhora vamos a hacer un layout con cuatro subplots en dos renglones y dos columnas\n\nfig, axes = plt.subplots(2,2, figsize=(24,24))# Pedimos subfiguras con un renglón y dos columnas\n\n\n\n\nFíjense lo que tenemos ahora en el objeto axes\n\naxes\n\narray([[<AxesSubplot: >, <AxesSubplot: >],\n       [<AxesSubplot: >, <AxesSubplot: >]], dtype=object)\n\n\nEs un array de 2x2 que tiene la misma forma de la figura. Para manejarlo más fácil, vamos a aplanar el array\n\naxes.ravel()\n\narray([<AxesSubplot: >, <AxesSubplot: >, <AxesSubplot: >, <AxesSubplot: >],\n      dtype=object)\n\n\nEso ya tiene una sola dimensión y lo podemos usar de la misma forma que antes\n\nfig, axes = plt.subplots(2,2, figsize=(24,12))# Pedimos subfiguras con un renglón y dos columnas\naxes = axes.ravel() # aplanamos el array\nagebs.plot('prop_0a2', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[0], alpha=0.8)# Graficamos en el primer ax\naxes[0].set_title(\"Proporción de 0 a 2\") # Este título está en el nivel ax, no en el figure\naxes[0].set_axis_off()\nctx.add_basemap(axes[0], source=ctx.providers.OpenStreetMap.Mapnik, crs=agebs.crs.to_string())\nagebs.plot('prop_12a14', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[1], alpha=0.8)# Graficamos en el segndo ax\naxes[1].set_title(\"Proporción de 12 a 14\") # Este título está en el nivel ax, no en el figure\naxes[1].set_axis_off()\nctx.add_basemap(axes[1], source=ctx.providers.OpenStreetMap.Mapnik, crs=agebs.crs.to_string())\nagebs.plot('prop_18a24', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[2], alpha=0.8)# Graficamos en el segndo ax\naxes[2].set_title(\"Proporción de 18 a 24\") # Este título está en el nivel ax, no en el figure\naxes[2].set_axis_off()\nctx.add_basemap(axes[2], source=ctx.providers.OpenStreetMap.Mapnik, crs=agebs.crs.to_string())\nagebs.plot('prop_60ymas', legend=True, cmap='OrRd', scheme='quantiles', ax=axes[3], alpha=0.8)# Graficamos en el segndo ax\naxes[3].set_title(\"Proporción de 60 y más\") # Este título está en el nivel ax, no en el figure\naxes[3].set_axis_off()\nctx.add_basemap(axes[3], source=ctx.providers.OpenStreetMap.Mapnik, crs=agebs.crs.to_string())\nplt.tight_layout() # Para disminuir el espacio entre gráficas\nplt.show()"
  },
  {
    "objectID": "parte_1/07_mapas_COVID.html#un-mapa-sencillo",
    "href": "parte_1/07_mapas_COVID.html#un-mapa-sencillo",
    "title": "7  Mapas COVID",
    "section": "7.1 Un mapa sencillo",
    "text": "7.1 Un mapa sencillo\nEmpecemos por leer las geometrías de municipios:\n\nmunicipios = gpd.read_file('datos/municipios_pob_2020_simple.json')\nmunicipios\n\n\n\n\n\n  \n    \n      \n      id\n      oid\n      municipio_cvegeo\n      municipio\n      pob2020\n      pob_0a4\n      pob_0a9\n      pob60ym\n      entidad_cvegeo\n      geometry\n    \n  \n  \n    \n      0\n      827\n      525\n      16046\n      Juárez\n      15290\n      1557\n      3122\n      1911\n      16\n      POLYGON ((-100.45693 19.33414, -100.45818 19.3...\n    \n    \n      1\n      828\n      209\n      16047\n      Jungapeo\n      22358\n      2470\n      4920\n      2608\n      16\n      POLYGON ((-100.44063 19.51413, -100.44814 19.5...\n    \n    \n      2\n      829\n      564\n      16048\n      Lagunillas\n      5862\n      550\n      1111\n      844\n      16\n      POLYGON ((-101.38329 19.59813, -101.38279 19.6...\n    \n    \n      3\n      830\n      524\n      16049\n      Madero\n      18769\n      2049\n      4136\n      2055\n      16\n      POLYGON ((-101.11644 19.53327, -101.11713 19.5...\n    \n    \n      4\n      67\n      44\n      05035\n      Torreón\n      744247\n      65682\n      129805\n      85778\n      05\n      MULTIPOLYGON (((-102.98871 24.79622, -102.9930...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2452\n      2452\n      2388\n      32053\n      Villa González Ortega\n      13945\n      1481\n      2926\n      1599\n      32\n      POLYGON ((-101.94821 22.65201, -101.95269 22.6...\n    \n    \n      2453\n      2453\n      2417\n      32054\n      Villa Hidalgo\n      20177\n      2078\n      4191\n      2036\n      32\n      POLYGON ((-101.65599 22.51381, -101.65651 22.5...\n    \n    \n      2454\n      2454\n      1407\n      32055\n      Villanueva\n      31804\n      2738\n      5540\n      5324\n      32\n      POLYGON ((-102.69428 22.62230, -102.69370 22.6...\n    \n    \n      2455\n      2455\n      2411\n      32056\n      Zacatecas\n      155533\n      12609\n      25488\n      15549\n      32\n      POLYGON ((-102.58542 22.81149, -102.58522 22.8...\n    \n    \n      2456\n      2457\n      2457\n      32058\n      Santa María de la Paz\n      2855\n      257\n      505\n      536\n      32\n      POLYGON ((-103.19774 21.58534, -103.21258 21.5...\n    \n  \n\n2457 rows × 10 columns\n\n\n\nLo primero que vamos a hacer es un mapa rápido de población a nivel municipal. Para eso vamos a usar el métdo plot de geopandas que nos da chance de hacer mapas bien fácil\n\nmunicipios.plot(column='pob2020', cmap='OrRd',figsize=(15, 10), scheme=\"quantiles\", legend=True)\n\n<AxesSubplot: >\n\n\n\n\n\nNoten cómo usamos el argumento scheme=\"quantiles\" para hacer un mapa de cualtiles. Podemos usar cualquier clasificación de mapclassify.\nIgual que antes, podemos hacer modificaciones al estilo para ajustarlo a nuestras necesidades.\n\nfig, ax = plt.subplots(1, figsize=(20,10))\nmunicipios.plot(column='pob2020', cmap='OrRd',figsize=(15, 10), scheme=\"quantiles\", legend=True, ax=ax)\nax.set_axis_off() \nfig.suptitle(\"Población por municipio\") # A través de la función '.suptitle()' aplicada a la figura se coloca el título.\nplt.show()"
  },
  {
    "objectID": "parte_1/07_mapas_COVID.html#mapas-de-covid",
    "href": "parte_1/07_mapas_COVID.html#mapas-de-covid",
    "title": "7  Mapas COVID",
    "section": "7.2 Mapas de COVID",
    "text": "7.2 Mapas de COVID\nEl primer paso para mapear los casos de covid es leerlos y unirlos a las geometrías de los municipios, empecemos por leer la misma base que hemos usado ya antes:\n\ndf = pd.read_pickle(\"datos/agregados_semana_municipio.pkl\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      Nuevos Casos\n      Defunciones\n    \n    \n      FECHA_SINTOMAS\n      CLAVE_MUNICIPIO_RES\n      MUNICIPIO_RES\n      \n      \n    \n  \n  \n    \n      2020-01-05\n      01001\n      AGUASCALIENTES\n      11\n      0\n    \n    \n      01006\n      PABELLÓN DE ARTEAGA\n      1\n      0\n    \n    \n      01009\n      TEPEZALÁ\n      1\n      0\n    \n    \n      02001\n      ENSENADA\n      1\n      0\n    \n    \n      02002\n      MEXICALI\n      11\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBajamos los datos en el formato pickle de Python. Es una forma estandarizada de serializar objetos de Python que es útil cuando queremos guardar un objeto arbitrario para después reutilizarlo en Python.\n\n\nEstos datos corresponden a toda la serie de tiempo, entonces para cada municipio hay toda una serie de valores. Para empezar a hacer mapas vamos a seleccionar una fecha en específico, lo más fácil es seleccionar la última disponible (vamos a seleccionar por FECHA_INGRESO, pero podríamos usar cualquier otra)\n\nultima_fecha = df.reset_index().loc[df.reset_index()['FECHA_SINTOMAS'] == df.reset_index()['FECHA_SINTOMAS'].max()]\nultima_fecha.head()\n\n\n\n\n\n  \n    \n      \n      FECHA_SINTOMAS\n      CLAVE_MUNICIPIO_RES\n      MUNICIPIO_RES\n      Nuevos Casos\n      Defunciones\n    \n  \n  \n    \n      159076\n      2022-01-23\n      01001\n      AGUASCALIENTES\n      10\n      0\n    \n    \n      159077\n      2022-01-23\n      01007\n      RINCÓN DE ROMOS\n      1\n      0\n    \n    \n      159078\n      2022-01-23\n      02001\n      ENSENADA\n      9\n      0\n    \n    \n      159079\n      2022-01-23\n      02002\n      MEXICALI\n      12\n      0\n    \n    \n      159080\n      2022-01-23\n      02004\n      TIJUANA\n      12\n      0\n    \n  \n\n\n\n\nTenemos la lista de los municipios que tuvieron casos en la fecha que estamos analizando, para hacer un mapa necesitamos unir estos datos a la geometría de los municipios.\nPrimero vamos a seleccionar, a partir del GeoDataFrame que ya tenemos sólo los municipios de la entidad que estamos analizando. A partir de eso podemos realizar una unión via la clave del municipio, sólo tenemos que tener cuidado de utilizar el tipo de unión adecuada para no dejar fuera los municipios sin casos.\n\ncasos_municipio = (municipios\n                   .merge(ultima_fecha, left_on='municipio_cvegeo', right_on='CLAVE_MUNICIPIO_RES', how='left') # Unimos con los municipios\n                   .drop(columns=['CLAVE_MUNICIPIO_RES', 'MUNICIPIO_RES']) # eliminamos dos columnas que ya no vamosd a usar\n                   .fillna(0) # Los municipios sin casos deben tener 0 en lugar de NaN\n                   )\ncasos_municipio\n\n\n\n\n\n  \n    \n      \n      id\n      oid\n      municipio_cvegeo\n      municipio\n      pob2020\n      pob_0a4\n      pob_0a9\n      pob60ym\n      entidad_cvegeo\n      geometry\n      FECHA_SINTOMAS\n      Nuevos Casos\n      Defunciones\n    \n  \n  \n    \n      0\n      827\n      525\n      16046\n      Juárez\n      15290\n      1557\n      3122\n      1911\n      16\n      POLYGON ((-100.45693 19.33414, -100.45818 19.3...\n      0\n      0.0\n      0.0\n    \n    \n      1\n      828\n      209\n      16047\n      Jungapeo\n      22358\n      2470\n      4920\n      2608\n      16\n      POLYGON ((-100.44063 19.51413, -100.44814 19.5...\n      0\n      0.0\n      0.0\n    \n    \n      2\n      829\n      564\n      16048\n      Lagunillas\n      5862\n      550\n      1111\n      844\n      16\n      POLYGON ((-101.38329 19.59813, -101.38279 19.6...\n      0\n      0.0\n      0.0\n    \n    \n      3\n      830\n      524\n      16049\n      Madero\n      18769\n      2049\n      4136\n      2055\n      16\n      POLYGON ((-101.11644 19.53327, -101.11713 19.5...\n      0\n      0.0\n      0.0\n    \n    \n      4\n      67\n      44\n      05035\n      Torreón\n      744247\n      65682\n      129805\n      85778\n      05\n      MULTIPOLYGON (((-102.98871 24.79622, -102.9930...\n      2022-01-23 00:00:00\n      13.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2452\n      2452\n      2388\n      32053\n      Villa González Ortega\n      13945\n      1481\n      2926\n      1599\n      32\n      POLYGON ((-101.94821 22.65201, -101.95269 22.6...\n      0\n      0.0\n      0.0\n    \n    \n      2453\n      2453\n      2417\n      32054\n      Villa Hidalgo\n      20177\n      2078\n      4191\n      2036\n      32\n      POLYGON ((-101.65599 22.51381, -101.65651 22.5...\n      0\n      0.0\n      0.0\n    \n    \n      2454\n      2454\n      1407\n      32055\n      Villanueva\n      31804\n      2738\n      5540\n      5324\n      32\n      POLYGON ((-102.69428 22.62230, -102.69370 22.6...\n      0\n      0.0\n      0.0\n    \n    \n      2455\n      2455\n      2411\n      32056\n      Zacatecas\n      155533\n      12609\n      25488\n      15549\n      32\n      POLYGON ((-102.58542 22.81149, -102.58522 22.8...\n      2022-01-23 00:00:00\n      67.0\n      0.0\n    \n    \n      2456\n      2457\n      2457\n      32058\n      Santa María de la Paz\n      2855\n      257\n      505\n      536\n      32\n      POLYGON ((-103.19774 21.58534, -103.21258 21.5...\n      0\n      0.0\n      0.0\n    \n  \n\n2457 rows × 13 columns\n\n\n\nY hacemos un primer mapa\n\nfig, ax = plt.subplots(1, figsize=(20,10))\ncasos_municipio.plot(column='Nuevos Casos', cmap='OrRd',figsize=(15, 10), legend=True, ax=ax)\nax.set_axis_off() \nfig.suptitle(\"Nuevos Casos por municipio\") # A través de la función '.suptitle()' aplicada a la figura se coloca el título.\nplt.show()"
  },
  {
    "objectID": "parte_1/07_mapas_COVID.html#mapa-interactivo",
    "href": "parte_1/07_mapas_COVID.html#mapa-interactivo",
    "title": "7  Mapas COVID",
    "section": "7.3 Mapa interactivo",
    "text": "7.3 Mapa interactivo\nCon los mismos datos podemos usar folium para hacer un mapa interactivo muy fácilmente\n\nm = folium.Map(location=[19.4326018, -99.1332049], zoom_start=5) # Creamos la instancia de folium\nfolium.Choropleth( # Instanciamos un mapa de coropletas\n    geo_data=casos_municipio[[\"municipio_cvegeo\", \"Nuevos Casos\", \"geometry\"]], # Pasamos la geometría de los municipios\n    data=casos_municipio[[\"municipio_cvegeo\", \"Nuevos Casos\"]], # Las variables que vamos a usar en el mapa\n    columns=[\"municipio_cvegeo\", \"Nuevos Casos\"], # La primera columna une geometrías y datos, la segunda es la variable que vamos a mapear\n    key_on=\"feature.properties.municipio_cvegeo\", # Cómo se unen los datos\n    bins=4, # Cuántos intervalos iguales queremos en la clasificación \n    fill_color=\"OrRd\", # La escala de colores\n    fill_opacity=0.7, # Opacidad del relleno\n    line_opacity=0.2, # opacidad de la línea\n    legend_name=\"Nuevos Casos\",\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "parte_1/08_pesos_espaciales.html#datos",
    "href": "parte_1/08_pesos_espaciales.html#datos",
    "title": "8  Pesos Espaciales",
    "section": "8.1 Datos",
    "text": "8.1 Datos\nVamos a seguir utilizando las AGEB’s de la Ciudad de México, esta vez con dos columnas importantes: alcaldia, que contiene la Clave Geográfica de la Alcaldía dentro de la cual se encuentra dicho AGEB, y p_sderech, que indica el número de personas sin derechohabienca a servicios de salud, de acuerdo con información colectada por el Consejo Nacional de Evaluación de Política del Desarrollo Social (CONEVAL) en 2010.\nComo siempre, los datos necesarios se encuentran en la carpeta de datos del libro, por practicidad, aquí está la liga para descargar los datos.\nUna vez descargados los datos, usamos GeoPandas para leerlos:\n\nagebs = gpd.read_file('datos/pob_sinderechohab.zip')\nagebs.head()\n\n\n\n\n\n  \n    \n      \n      cvegeo\n      alcaldia\n      p_sderech\n      p_nescu\n      p_hacin\n      p_analf\n      geometry\n    \n  \n  \n    \n      0\n      0900700013628\n      09007\n      389.0\n      475.0\n      0.0\n      9.0\n      POLYGON ((2810132.372 824698.172, 2810169.540 ...\n    \n    \n      1\n      0901500010235\n      09015\n      323.0\n      481.0\n      6.0\n      7.0\n      POLYGON ((2798881.634 831643.241, 2798843.076 ...\n    \n    \n      2\n      0900200010097\n      09002\n      448.0\n      780.0\n      5.0\n      37.0\n      POLYGON ((2792415.239 836846.390, 2792356.808 ...\n    \n    \n      3\n      0900200011184\n      09002\n      240.0\n      389.0\n      0.0\n      11.0\n      POLYGON ((2792260.139 836768.777, 2792333.695 ...\n    \n    \n      4\n      0900300011285\n      09003\n      1017.0\n      1291.0\n      0.0\n      23.0\n      POLYGON ((2802121.600 817466.682, 2802124.157 ..."
  },
  {
    "objectID": "parte_1/08_pesos_espaciales.html#pesos-espaciales-en-pysal",
    "href": "parte_1/08_pesos_espaciales.html#pesos-espaciales-en-pysal",
    "title": "8  Pesos Espaciales",
    "section": "8.2 Pesos Espaciales en PySAL",
    "text": "8.2 Pesos Espaciales en PySAL\n\n8.2.1 Contigüidad\nLas Matrices de Contigüidad definen las relaciones de vecindad a través de la existencia de fronteras comúnes. Esto facilita enormemente el utilizar este tipo de matrices con polígonos: si dos polígonos comparten frontera de alguna forma, serán entoces etiquetadas como vecinos; el nivel en el cual tienen que compartir esta frontera será dado por el criterio que se elija para ello, teniéndose los criterios de Reina o de Torre.\n\n8.2.1.1 Criterio de Reina\nBajo éste, basta con que dos polígonos compartan únicamente un sólo vértice de sus fronteras para ser considerados vecinos, en PySAL podemos crear una matriz de contigüidad con el criterio de reina haciendo:\n\nm_reina = Queen.from_dataframe(agebs, idVariable='cvegeo')\nm_reina\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 8 disconnected components.\n There is 1 island with id: 090090015012A.\n  warnings.warn(message)\n\n\n<libpysal.weights.contiguity.Queen at 0x7f9f027e5ed0>\n\n\nEstamos guardando en la variable m_reina un objeto del tipo libpysal.weights.contiguity.Queen que representa la matriz de vecindad que calculamos. Noten el warning que nos sale: “There is 1 island with id: 1356”, eso quiere decir que la observación con índice 1356 no está conectada a ninguna otra bajo el criterio que usamos.\nLas Matrices Espaciales en PySAL ofrecen la facilidad de encontrar rápidamente las relaciones de contigüidad que posee. Por ejemplo, si se quisiera saber cuáles son los vecinos de la AGEB en la que se encuentra el CentroGeo (0901200010337), se tendría que:\n\nm_reina['0901200010337']\n\n{'0901200012066': 1.0,\n '0901200010642': 1.0,\n '0901200011369': 1.0,\n '0901200010341': 1.0,\n '0901200010322': 1.0,\n '0901200010638': 1.0}\n\n\nEl comando anterior arroja un diccionario que contiene las Claves Geográficas de cada vecino como identificadores, y los pesos a los que se asocia como sus respectivos valores; debido a que se trata de una Matriz Espacial de contigüidad, todos los vecinos tienen asignados un peso de uno. Si se quisiera tener el peso de un vecino en particular, se puede realizar una búsqueda recursiva:\n\nm_reina['0901200010337']['0901200010341']\n\n1.0\n\n\nTambién pueden obtenerse de forma aislada una lista de los vecinos de cada observación, así como de sus respectivos pesos, gracias a los atributos neighbors y weights del objeto creado:\n\nm_reina.neighbors['0901200010337']\n\n['0901200012066',\n '0901200010642',\n '0901200011369',\n '0901200010341',\n '0901200010322',\n '0901200010638']\n\n\n\nm_reina.weights['0901200010337']\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nA través del objeto contenido en la variable m_reina es posible obtener información de la matriz que va mucho más allá de los atributos básicos que uno esperaría. Por ejemplo, es posible acceder de forma directa al número de vecinos que posee una observacioń a través del atributo cardinalities:\n\nm_reina.cardinalities['0901200010337']\n\n6\n\n\nY, debido a que el resultado de cardinalities es un diccionario, éste puede ser transformado directamente en un objeto del tipo Series:\n\ncard_reina = pd.Series(m_reina.cardinalities)\ncard_reina.head()\n\n0900200010010    3\n0900200010025    4\n090020001003A    4\n0900200010044    4\n0900200010097    4\ndtype: int64\n\n\nEsto, a su vez, permite realizar cosas como graficas que, en este caso, nos permite tener una visión general del tamaño de las vecindades en general:\n\nsns.displot(card_reina, rug = True, kde = False)\n\n<seaborn.axisgrid.FacetGrid at 0x7f9e91ae4d90>\n\n\n\n\n\nLa gráfica anterior permite deducir que la mayoría de las observaciones poseen alrededor de cinco y sesis vecinos, existiendo algo de variación entre los datos; así mismo, aunque son las menos, también existen observaciones con un gran número de vecinos, llegándose hasta los 20 vecinos.\nA continuación se presenta algo de la información adicional que puede obtenerse de la Matriz de Pesos Espaciales, a través de sus atributos particulares:\n\n# Número de Observaciones\nm_reina.n\n\n2397\n\n\n\n# Número Promedio de Vecinos\nm_reina.mean_neighbors\n\n5.677096370463079\n\n\n\n# Número Mínimo de Vecinos\nm_reina.min_neighbors\n\n0\n\n\n\n# Número Máximo de Vecinos\nm_reina.max_neighbors\n\n20\n\n\n\n# Islas (Observaciones Desconectadas del Resto / Sin Vecinos)\nm_reina.islands\n\n['090090015012A']\n\n\n\n# Orden en que se encuentran las Observaciones (en este caso, las primeras cinco)\nm_reina.id_order[:5]\n\n['0900200010010',\n '0900200010025',\n '090020001003A',\n '0900200010044',\n '0900200010097']\n\n\nTambién podemos hacer visualizaciones para observar algunas características de las matrices de contigüidad. Por ejemplo, podemos ver los vecinos de una unidad en particular. En el siguiente código podemos ver las agebs vecinas de la ageb en la que se encuentra el CentroGeo\n\n# Preparar la figura \nfig , filas = plt.subplots(1, figsize = (10, 10))\n\n# Graficar la Capa Base\nagebs.plot(ax = filas , facecolor = 'grey', linewidth = 0.1)\n\n# Seleccionar el Polígono Focal (Tanto la Clave Geográfica como la Geometría tienen sus propios corchetes)\ncentrogeo = agebs.set_index('cvegeo').loc[['0901200010337'] , ['geometry']]\n\n# Graficar el Polígono Focal\ncentrogeo.plot(ax = filas , facecolor = 'red', alpha = 1, linewidth = 0)\n\n# Graficar los Vecinos\nvecinos = agebs.set_index('cvegeo').loc[m_reina['0901200010337'], :]\nvecinos.plot(ax = filas , facecolor = 'lime')\n\n# Título\nfig.suptitle('Vecinos del AGEB 0901200010337 (CentroGeo)')\n\n# Acercamiento a los Polígonos\nfilas.set_ylim(811882.7509 , 815282.7509)\nfilas.set_xlim(2789417.0236 , 2792817.0236)\n\n/tmp/ipykernel_7969/3035649815.py:14: FutureWarning: Passing a dict as an indexer is deprecated and will raise in a future version. Use a list instead.\n  vecinos = agebs.set_index('cvegeo').loc[m_reina['0901200010337'], :]\n\n\n(2789417.0236, 2792817.0236)\n\n\n\n\n\nEs importante remarcar el proceso a través del cual fue construída la gráfica; primero, se agregó el mapa base, después el polígono de interés, para seguir con la vecindad y, finalmente, realizar el acercamiento deseado.\n\n\n8.2.1.2 Criterio de Torre\nLa Contigüidad de Torre es similar, en este caso dos observaciones se consideran contiguas si comparten una arista (es decir, una línea). Desde el punto de vista técnico, una Matriz de Pesos Espaciales con Contigüidad de Torre se construye de forma muy similar al de Reina:\n\nm_torre = Rook.from_dataframe(agebs, idVariable = 'cvegeo')\nm_torre\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 11 disconnected components.\n There is 1 island with id: 090090015012A.\n  warnings.warn(message)\n\n\n<libpysal.weights.contiguity.Rook at 0x7f9e90b50100>\n\n\nEl objeto contenido en la variable m_torre pude ser utilizado y explorado exactamente de la misma manera que m_reina.\n\n\n8.2.1.3 Ejercicio, haz un mapa de la vecindad de reina para el ageb de CentroGeo\n\n\n\n\n8.2.2 Distancia\nLas matrices basadas en distancia asignan el peso de cada par de observaciones como una función de qué tan lejos se encuentran entre sí; cómo se traduce esta regla en un valor fijo varía en función de múltiples criterios, pero todos giran alrededor del hecho de que la distancia entre las observaciones determinará el Peso Espacial.\n\n8.2.2.1 K-Vecinos Más Cercanos (KNN)\nUna forma de definir los Pesos Espaciales es tomar las distancias entre una observación determinada y el resto de los datos, ordenarlos en función de su distancia y considerar como vecinos a los \\(k\\) más cercanos; esto es exactamente lo que hace el criterio de \\(k\\)-Vecinos Más Cercanos (KNN, por sus siglas en inglés).\nPara calcular una Matriz de Pesos Espaciales a través del criterio KNN, se utiliza una función muy similar a la usada con los Criterios de Contigüidad:\n\nknn5 = KNN.from_dataframe(agebs, ids='cvegeo', k = 5)\nknn5\n\n<libpysal.weights.distance.KNN at 0x7f9e90b7e920>\n\n\nPuede observarse que es necesario indicar el número de vecinos que se deberán de considerar, a través del argumento k. Asímismo, ésta función cuenta con una forma diferente de asignar los ID’s a cada observación, haciéndolo a través del argumento ids; en lugar de indicar el nombre de la variable que funcionará como identificador.\nLa función automáticamente calcula los centroides de cada observación para calcular la distancia entre las mismas.\n\n\n8.2.2.2 Ejercicio\nVisualiza los vecinos más cercanos a la ageb de CentroGeo para valores de k de 4,5,6 y 7 y pon todos los mapas en la misma figura\n\n\n8.2.2.3 Banda de Distancia\nOtra forma de obtener una Matriz de Pesos Espaciales basada en Distancias es trazar un círculo de radio determinado y considerar como vecino a toda observación que se encuentre dentro de éste. Esta técnica posee dos variaciones: Binaria y Contínua; en la primera, todos los vecinos son asignados un peso de uno, mientras que en la segunda las observaciones son ajustadas en función a la distancia a la observación de interés.\nPara generar matrices con Bandas de Distancias Binarias en PySAL, se utiliza el siguiente comando:\n\nm_dist1kmB = DistanceBand.from_dataframe(agebs , threshold=1000 , binary=True , ids='cvegeo')\nm_dist1kmB\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 34 disconnected components.\n There are 14 islands with ids: 0901200011759, 0900200010877, 0900300010770, 0900400010369, 0900400010373, 0900400200316, 090090015012A, 0900900330488, 0900900360242, 0901200011195, 0901200012155, 0901200262297, 0901300011474, 0901300011578.\n  warnings.warn(message)\n\n\n<libpysal.weights.distance.DistanceBand at 0x7f9e90b50340>\n\n\nNoten que el parámetro threshold, que definoe el ancho de la banda de distancia, está en las unidades de la proyección de la capa.\nEsto crea una matriz binaria que considera como vecinos de una observación a todo polígono cuyo centroide se encuentre dentro de un radio de 1,000 metros (1km) del centroide de la observación, límite expresado a través del argumento threshold; además, se especifica que la matriz sea binaria a través del argumento binary. Como tal, si se observan los vecinos del AGEB de CentroGeo:\n\nm_dist1kmB['0901200010337']\n\n{'0901200010657': 1.0,\n '0901200010638': 1.0,\n '0901200010318': 1.0,\n '0901200010322': 1.0,\n '0901200010341': 1.0,\n '0901200010642': 1.0,\n '0901200011369': 1.0,\n '0901200011388': 1.0}\n\n\nEs importante destacar que las unidades en las que se especifica la distancia del círculo dependen del Sistema de Coordenadas de Referencia (CRS) en el que se encuentren proyectados los datos, por lo que es importante tomarlo en cuenta al momento de ingresar el valor. El CRS de un GeoDataFrame puede ser revisado a través de su atributo crs:\nEn este caso, se tiene el sistema con el EPSG 6362, equivalente a la Proyección Cónica Conforme de Lambert (LCC) para México, la cual utiliza como unidades al metro, teniendo entonces sentido el haber colocado dentro del argumento treshold el valor de 1,000 para representar 1km.\nUna extensión de lo anterior implica el añadir mayor detalle al asignar diferentes pesos a los diferentes vecinos dentro del círculo de radio determinado en función de su distancia a la observación de interés; una forma de hacer esto es utilizando el inverso de la distancia entre dos observaciones como su peso. Esto se realiza a través de la variante Contínua del método, la cual se consigue modificando el argumento binary del comando utilizado anteriormente:\n\nm_dist1kmC = DistanceBand.from_dataframe(agebs , threshold=1000 , binary=False, ids='cvegeo')\nm_dist1kmC\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/scipy/sparse/_data.py:117: RuntimeWarning: divide by zero encountered in reciprocal\n  return self._with_data(data ** n)\n\n\n<libpysal.weights.distance.DistanceBand at 0x7f9e9133e380>\n\n\nCon m_dist1kmC, a toda observación dentro del círculo de 1km se le asigna un peso igual al inverso de la distancia entre cada par de observaciones:\n\\[\nw_{ij} = \\dfrac{1}{d_{ij}}\n\\]\nDe esta forma, mientras más alejadas se encuentren \\(i\\) y \\(j\\) entre sí, menor será el peso \\(w_{ij}\\). Esto puede verificarse observando nuevamente los valores para los vecinos del AGEB de CentroGeo:\n\nm_dist1kmC['0901200010337']\n\n{'0901200010657': 0.0011186366205468501,\n '0901200010638': 0.001430708698453151,\n '0901200010318': 0.0015967606198248982,\n '0901200010322': 0.002801456949838892,\n '0901200010341': 0.0016416791117487674,\n '0901200010642': 0.0014321050161851287,\n '0901200011369': 0.0011074818342581747,\n '0901200011388': 0.0010068495493531873}\n\n\n\nSiguiendo esta lógica de ajustar los pesos a través de las distancias, existe la posibilidad de hacer que todos los elementos de un conjunto de datos se conviertan en vecinos entre sí, ya que siempre existirá alguna distancia entre todos los pares posibles; sin embargo, aunque conceptualmente correcto, esta decisión no siempre es la más práctica o computacionalmente correcta pues, debido a la naturaleza de las Matrices de Pesos Espaciales, particularmente el hecho de que tienen dimensiones de \\(N\\) por \\(N\\), éstas pueden crecer substancialmente en tamaño.\nUna forma de resolver el problema anterior es asegurándose de que la matriz posea múltiples ceros en su interior. En el caso de las matrices de contigüidad, así como las del criterio KNN, la presencia de estos ceros se encuentra asegurada; sin embargo, en el caso del Inverso de la Distancia, necesita imponerse la presencia de éstos pues, de lo contrario, puede convertise en una matriz muy densa (Esto es, con pocos o ningún cero más que los de la diagonal principal).\nEn términos prácticos, lo que usualmente se hace es imponer una distancia límite de la cual más allá no se asignará ningún peso, y se asume que no hay interacción alguna. Además de hacer el proceso más sencillo computacionalmente, los resultados obtenidos con esta distancia no difieren mucho de los que se tendría con una matriz completamente densa, debido a que la información eliminada con la distancia normalmente corresponde a pesos espaciales muy pequeños.\nEn este contexto, un umbral comúnmente utilizado, aunque no siempre el mejor, es aquel en el que se asegura que todas las observaciones tengan por lo menos un vecino. Esta distancia puede calcularse fácilmente usando la función min_threshold_distance. La entrada de esta función no es el GeoDataFrame sino la lista de coordenadas de los centroides, afortunadamente es muy fácil pasar del GeoDataFrame a la lista de coordenadas\n\ncoordenadas = np.array([(pt.x , pt.y) for pt in agebs.centroid])\ncoordenadas\n\narray([[2810184.3953237 ,  824630.90277965],\n       [2798795.09167579,  831549.7671602 ],\n       [2792280.44399879,  837098.86138355],\n       ...,\n       [2807793.98111279,  828749.42701419],\n       [2807892.48374243,  828610.97973225],\n       [2807674.47595458,  828330.05069586]])\n\n\nY ahora sí podemos calcular nuestro umbral de distancia\n\ndist_min = min_threshold_distance(coordenadas)\ndist_min\n\n1958.9642337770003\n\n\nTeniendo esto, puede volverse a calcular la Matriz de Pesos con Banda de Distancia Contínua utilizando ésta nueva distancia calculada como umbral:\n\nm_distmin = DistanceBand.from_dataframe(agebs, threshold = dist_min , binary = False, ids = 'cvegeo')\nm_distmin\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/scipy/sparse/_data.py:117: RuntimeWarning: divide by zero encountered in reciprocal\n  return self._with_data(data ** n)\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 2 disconnected components.\n  warnings.warn(message)\n\n\n<libpysal.weights.distance.DistanceBand at 0x7f9e913e6aa0>\n\n\nSi se revisan los vecinos de la AGEB del CentroGeo en esta matriz, se notará que se señalan como tal todas las AGEB’s contenidas dentro de la Alcaldía Tlalpan, donde se encuentra el Centro:\n\n\n8.2.2.4 Ejercicio Opcional\nUtilizando la Matriz de Pesos por Bloques, intenta crear un mapa similar al generado anteriormente con la Matriz de Contigüidad de Reina. ___ ## Estandarización de Matrices\nEn el contexto de muchas técnicas de análisis espacial, una Matriz de contigüidad, que nos indica sólo si dos unidades son o no vecinos,no es siempre la más adecuada para un análisis, por lo que algún tipo de transformación es requerida, lo cual implica modificar cada peso para que se conforme a ciertas reglas. PySAL posee múltiples transformaciones preparadas para las matrices que genera, así que resulta sencillo modificarla y examinarla.\nTomando en cuenta la Matriz de Contigüidad de Reina generada anteriormente, para la AGEB del CentroGeo:\n\nm_reina['0901200010337']\n\n{'0901200012066': 1.0,\n '0901200010642': 1.0,\n '0901200011369': 1.0,\n '0901200010341': 1.0,\n '0901200010322': 1.0,\n '0901200010638': 1.0}\n\n\nDebido a que se trata de un criterio de contigüidad, todos los vecinos del AGEB poseen un peso de 1, mientras que el resto poseen un cero. Es posible verificar si el objeto m_reina ha sido transformado o no al llamar al atributo transform:\n\nm_reina.transform\n\n'O'\n\n\nDonde 'O' significa ‘Original’, esto es, ninguna transformación ha sido aplicada. Si se buscara aplicar una transformación sobre las filas de la matriz, de modo que todas las filas de ésta sumen en total 1, el atributo transform es modificado de la siguiente forma:\n\nm_reina.transform = 'R'\n\n('WARNING: ', '090090015012A', ' is an island (no neighbors)')\n\n\nDe modo que, si se revisa nuevamente la vecindad del AGEB de CentroGeo, se encontrarán los pesos modificados:\n\nm_reina['0901200010337']\n\n{'0901200012066': 0.16666666666666666,\n '0901200010642': 0.16666666666666666,\n '0901200011369': 0.16666666666666666,\n '0901200010341': 0.16666666666666666,\n '0901200010322': 0.16666666666666666,\n '0901200010638': 0.16666666666666666}\n\n\nSin considerar algunas limitaciones de precisión del lenguaje, puede constatarse que la suma de los pesos de todos los vecinos es igual a uno:\n\npd.Series(m_reina['0901200010337']).sum()\n\n0.9999999999999999\n\n\nLa matriz puede ser regresada fácilmente a su estado original, simplemente modificando el atributo transform de la forma correcta:\n\nm_reina.transform = 'O'\nm_reina['0901200010337']\n\n{'0901200012066': 1.0,\n '0901200010642': 1.0,\n '0901200011369': 1.0,\n '0901200010341': 1.0,\n '0901200010322': 1.0,\n '0901200010638': 1.0}\n\n\nPySAL permite realizar sobre las matrices las siguientes transformaciones: * 'O' - Original, permite regresar a la matriz a su estado inicial. * 'B' - Binario, asignándole a todos los vecinos un peso de uno. * 'R' - Fila (Row), haciendo que el peso de todos los vecinos de una observación dada sumen uno. * 'V' - Estabilizados de Varianza, restringiéndose la suma de todos los pesos de la matriz al número de observaciones. ___ ## Exportar e Importar Pesos Espaciales con PySAL Existen casos en los que un Conjunto de Datos es muy detallado o de gran tamaño, lo cual puede complicar la construcción de las Matrices de Pesos Espaciales y, aún con las optimizaciones del código de PySAL, el tiempo de cómputo puede crecer enormemente; en estas situaciones, es útil el no tener que reconstruir la matriz desde cero cada vez que se necesite volver a correr un análisis. Una sólución para este problema es el construir la matriz una única ocasión, para después exportarla y tenerla almacenada en un sitio donde pueda ser consultada cuando se necesite.\nLa forma en la que PySAL realiza la exportación de cualqueir tipo de matriz es a través del comando open, siendo necesario únicamente determinar el formato en que se exportará ésta. Aunque existen múltiples formatos en los cuales las matrices pueden ser almacenadas, existen dos comúnmente utilizadas:\n\n\n8.2.2.5 Formato .gal - Matrices de Contigüidad\nLas Matrices de Contigüidad pueden ser almacenadas en un archivo tipo .gala través de los siguientes comandos:\n\nm_reina.to_file(\"datos/m_reina.gal\", 'gal')\n\nEste objeto lo podemos leer fácilmente\n\nm_reina2 = W.from_file(\"datos/m_reina.gal\", 'gal')\nm_reina2\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 8 disconnected components.\n There is 1 island with id: 090090015012A.\n  warnings.warn(message)\n\n\n<libpysal.weights.weights.W at 0x7f9e9138a1d0>"
  },
  {
    "objectID": "parte_1/08_pesos_espaciales.html#rezago-espacial",
    "href": "parte_1/08_pesos_espaciales.html#rezago-espacial",
    "title": "8  Pesos Espaciales",
    "section": "8.3 Rezago Espacial",
    "text": "8.3 Rezago Espacial\nUna de las aplicaciones más directas de las Matrices de Pesos Espaciales es el cálculo del Rezago Espacial; éste se define como el producto de una Matriz de Pesos Espaciales con una variable en particular:\n\\[\nY_{sl} = W Y\n\\]\nDonde \\(Y\\) es un vector de dimensiones \\(Nx1\\) con los valores de la variable. Cabe recordar que el producto entre una matriz y un vector es igual a la suma de todas las multiplicaciones entre la columna de la variable y las filas de la matriz, para los valores resultantes de una columna dada; lo anterior, dicho en términos de Rezago Espacial, se representa como:\n\\[\ny_{sl-i} = \\displaystyle \\sum_j w_{ij} y_j\n\\]\nSi se utilizan pesos estandarizados por fila en la Matriz de Pesos Espaciales, entonces \\(w_{ij}\\) se convierte en una proporción entre cero y uno, y \\(y_{sl-i}\\) puede ser visto como el valor promedio de \\(Y\\) para la vecindad de \\(i\\).\nEl Rezago Espacial es un elemento fundamental para muchas técnicas de análisis espacial y, como tal, PySAL provee la funciónlag_spatial para calcularlo de forma sencilla. En los datos originales existe una variable llamada p_sderech, referente al número de personas sin derechoahabienca a servicios públicos de salud dentro de la Ciudad de México, de acuerdo con información colectada por el Consejo Nacional de Evaluación de Política del Desarrollo Social (CONEVAL) en 2010; si se quisiera calcular el rezago espacial para esa variable, entonces se tendría:\n\n# Estandarización de la Matriz de Contigüidad de Reina por Filas\nm_reina.transform = 'R'\n\n# Cálculo del rezago espacial para el Número de Personas sin Derechohabiencia a Servicios Públicos de Salud\nrez_espacial = lag_spatial(m_reina , agebs['p_sderech'])\n\n# Mostrar los primeros cinco elementos del vector generado\nrez_espacial[:5]\n\narray([337. , 419.5, 484.5, 409. , 500. ])\n\n\nDe los comandos anteriores, la línea 5 contiene el comando donde se genera el cálculo deseado, el cual se encuentra altamente optimizado en PySAL. Aunque dentro de la función se utilizó un objeto del tipo Series, a través de agebs['p_sderech'], el resultado es una lista y, como tal, puede ser añadida directamente al GeoDataFrame con el que se ha trabajado hasta ahora:\n\nagebs['rez_espacial'] = rez_espacial\nagebs.head()\n\n\n\n\n\n  \n    \n      \n      cvegeo\n      alcaldia\n      p_sderech\n      p_nescu\n      p_hacin\n      p_analf\n      geometry\n      rez_espacial\n    \n  \n  \n    \n      0\n      0900700013628\n      09007\n      389.0\n      475.0\n      0.0\n      9.0\n      POLYGON ((2810132.372 824698.172, 2810169.540 ...\n      337.0\n    \n    \n      1\n      0901500010235\n      09015\n      323.0\n      481.0\n      6.0\n      7.0\n      POLYGON ((2798881.634 831643.241, 2798843.076 ...\n      419.5\n    \n    \n      2\n      0900200010097\n      09002\n      448.0\n      780.0\n      5.0\n      37.0\n      POLYGON ((2792415.239 836846.390, 2792356.808 ...\n      484.5\n    \n    \n      3\n      0900200011184\n      09002\n      240.0\n      389.0\n      0.0\n      11.0\n      POLYGON ((2792260.139 836768.777, 2792333.695 ...\n      409.0\n    \n    \n      4\n      0900300011285\n      09003\n      1017.0\n      1291.0\n      0.0\n      23.0\n      POLYGON ((2802121.600 817466.682, 2802124.157 ...\n      500.0\n    \n  \n\n\n\n\n\n8.3.0.1 Ejercicio Opcional\nAnaliza el Rezago Espacial calculado y almacenado en la variable rez_espacial generando una Gráfica de Densidad de Kernel o un Histograma de forma similar a como se hizo en prácticas anteriores; compara esta gráfica con una similar para la variable p_sderech y responde: ¿Qué diferencias se pueden observar? ___ ## Gráfica de Moran La Gráfica de Moran es una forma de visualización que permite empezar a explorar el concepto de Autocorrelación Espacial, y demuestra una aplicación directa de las Matrices de Pesos Espaciales y el Rezago Espacial. En escencia, se trata de un simple Diagrama de Dispersión en el que una determinada variable (p_sderech, por ejemplo) es graficada junto con su propio rezago espacial; usualmente se adiciona una regresión lineal para poder inferir una mayor cantidad de información:\n\n# Preparación de la Figura y sus Filas\nfig , filas = plt.subplots(1 , figsize = (10,10))\n\n# Variables de la Gráfica\nsns.regplot(x = 'p_sderech', y = 'rez_espacial', data = agebs)\n\n# Mostar Gráfica\nplt.show()\n\n\n\n\nPara tener la capacidad de comparar entre múltiples diagramas de dispersión y poder encontrar observaciones atípicas, es común estandarizar los valores de una variable antes de calcular su rezago espacial y graficarlo. Esto se consigue de forma sencilla al restar el promedio de la variable y dividir el resultado entre su desviación estándar:\n\\[\nz_i = \\dfrac{y - \\bar{y}}{\\sigma_y}\n\\]\nDonde \\(z_i\\) es la versión estandarizada de \\(y_i\\), \\(\\bar{y}\\) es el promedio de la variable y \\(\\sigma\\) su desviación estándar.\nCrear una Gráfica de Moran Estandarizada implica que los valores promedio se encuentren en el centro de la gráfica (debido a que se convierten en cero al estandarizar), y su dispersión se encuentra expresada en desviaciones estándar, teniéndose a todos los valores que sean más grandes o más pequeños que dos desviaciones estándar como valores atípicos. Una Gráfica de Moran Estandarizada también permite dividir al espacio en cuatro cuadrantes que representan situaciones diferentes:\n\nCuadrante I - Altos-Altos (HH), valores mayores a la media rodeados por valores mayores a la media.\nCuadrante II - Bajos-Altos (LH), valores menores a la media rodeados de valores mayores a la media.\nCuadrante III - Bajos-Bajos (LL), valores menores a la media rodeados de valores menores a la media.\nCuadrante IV - Altos-Bajos (HL), valores mayores a la media rodeados por valores menores a la media.\n\nÉstos se estudiarán más a detalle en futuras prácticas.\n\n# Estandarizar el Número de Robos por AGEB\nrobos_std = (agebs['p_sderech'] - agebs['p_sderech'].mean()) / agebs['p_sderech'].std()\n\n# Calcular el Rezago Espacial de la variable estandarizada y asignarle los índices originales\nrez_espacial_std = pd.Series(lag_spatial(m_reina , robos_std) , index = robos_std.index)\n\n# Preparación de la Figura y sus Filas\nfig , filas = plt.subplots(1 , figsize = (10,10))\n\n# Graficar los Valores\nsns.regplot(x = robos_std , y = rez_espacial_std)\n\n# Añadir líneas horizontal y vertical\nplt.axvline(0, c = 'grey', alpha = 0.5)\nplt.axhline(0, c = 'grey', alpha = 0.5)\n\n# Mostrar la Gráfica\nplt.show()"
  },
  {
    "objectID": "parte_1/08_pesos_espaciales.html#para-practicar",
    "href": "parte_1/08_pesos_espaciales.html#para-practicar",
    "title": "8  Pesos Espaciales",
    "section": "8.4 Para Practicar…",
    "text": "8.4 Para Practicar…\nCrea una Gráfica de Moran Estandarizada para cada una de las siguientes variables, también presentes en el ShapeFile original: * p_nescu - Población de 15 a 24 años que no asiste a la escuela * p_hacin - Población que vive en Hacinamiento * p_analf - Población de 15 años o más analfabeta\nIntenta generar todas las gráficas utilizando un bucle (Loop) de tipo for; también intenta explorar y utilizar la función .joinplot() de la librería seaborn para generar un resultado más completo (Información de la función aquí y aquí)"
  },
  {
    "objectID": "parte_1/09_autocorrelacion.html#autocorrelación-espacial-y-análisis-exploratorio-de-datos-espaciales",
    "href": "parte_1/09_autocorrelacion.html#autocorrelación-espacial-y-análisis-exploratorio-de-datos-espaciales",
    "title": "9  Autocorrelación espacial",
    "section": "9.1 Autocorrelación Espacial y Análisis Exploratorio de Datos Espaciales",
    "text": "9.1 Autocorrelación Espacial y Análisis Exploratorio de Datos Espaciales\nLa Autocorrelación Espacial se refiere a la relación local de una variable observada en el espacio ¿Qué tanto los valores cercanos tienden a parecerse? De forma muy semejante a como ocurre con la Correlación Tradicional (que determina cómo los valores de una variable cambian en función de los valores de otra) y de forma análoga a su homólogo en una serie temporal (que relaciona el valor de una variable en algún punto del tiempo con los valores en periodos anteriores), la Autocorrelación Espacial relaciona los valores de la variable de interés en un lugar dado con los valores de ella misma en sus alrededores.\nUna idea clave dentro de este contexto es el de Aleatoriedad Espacial, esto es, una situación en la cual la ubicación de una observación no guarda relación alguna con el valor de una variable; en otras palabras, una variable es aleatoria espacialmente si la distribución que sigue en el espacio parece no tener algún patrón discernible. Como tal, la Autocorrelación Espacial puede ser definida formalmente como la “absencia de Aleatoriedad Espacial”, y, al igual que con la correlación tradicional, puede presentarse en dos variaciones:\n\nCorrelación Espacial Positiva - Donde los valores similares tienden a agruparse entre sí en ubicaciones similares.\nCorrelación Espacial Negativa - Donde los valores similares tienden a estar dispersos y alejados entre sí.\n\nEn esta práctica, se aprenderá a explorar la Autocorrelación Espacial de un conjunto de datos determinado, analizando la presencia, naturaleza y robustez de los datos utilizados. Para esto, se utilizará un conjunto de herramientas colectivamente conocidos como Análisis Exploratorio de Datos Espaciales (ESDA, por sus siglas en inglés), diseñado específicamente para este propósito. El alcance de los Métodos ESDA es bastante amplio y abarca desde aproximaciones sencilllas, como Coropletas (Choropleths) y Consultas a Bases de Datos, hasta metodologías más avanzadas y robustas, como la Inferencia Estadística y la Identificación de la Dimensión Geográfica de los Datos. El propósito de esta sesión es explorar más a detalle estas últimas técnicas.\nLos Métodos ESDA comúnmente se dividen en dos grupos principales: herramientas para analizar a la autocorrelación espacial a nivel global, y otras a nivel local. Las primeras consideran el patrón general que sigue la localización de los valores, y hacen posible realizar conclusiones sobre el grado de Agrupamiento (Clustering) que siguen los datos; pueden responderse preguntas como ¿Los valores en general siguen un patrón en particular en su distribución geográfica? o ¿Los valores similares se encuentran más cerca de lo que se encontrarían en una distribución aleatoria?. Se analizará la Autocorrelación Espacial Global utilizando el estadístico de I de Moran.\nLos métodos utilizados para la Autocorrelación Espacial Local se enfocan en la inestabilidad espacial, esto es, que tanto se alejan ciertas ubicaciones en particular del patrón general; la idea detrás de esto es que, aún cuando existe un patrón general en términos de la naturaleza y robustez de la asociación espacial, algunas áreas en particular pueden divergir sustancialmente de éste mismo. Sin importar el grado general de concentración de los valores, existen focos donde valores inusualmente altos (o bajos) se encuentran cercanos a otros valores altos (o bajos), zonas que son conocidas como Hotspots (o Coldspots, para valores bajos); además, también es posible observar valores muy altos rodeados de valores muy bajos (o viceversa), los cuales reciben el nombre de Outliers Espaciales (Valores Atípicos). La ténica principal que se utilizará en esta sesión para explorar la Autocorrelación Espacial Local serán los Indicadores Locales de Asociación Espacial (LISA, por sus siglas en inglés).\nPara este talles vamos a instalas dos librerías nuevas:\n\nlibpysal\nesda\n\nEstas librerías son parte del conjunto de módulos pysal que es el estándar para realizar análisis espacial en Python.\n\n\nimport seaborn as sns\nimport pandas as pd\nfrom libpysal.weights import W,Queen, Rook, KNN, DistanceBand, min_threshold_distance, block_weights, lag_spatial\nimport esda\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mapclassify"
  },
  {
    "objectID": "parte_1/09_autocorrelacion.html#datos",
    "href": "parte_1/09_autocorrelacion.html#datos",
    "title": "9  Autocorrelación espacial",
    "section": "9.2 Datos",
    "text": "9.2 Datos\nEn esta práctica, se continuarán utilizando las AGEB’s como unidad espacial. Además vamos a trabajar con datos sobre Personas sin Derechohabiencia a Servicios Públic de Salud compilados por el Consejo Nacional de Evaluación de Política del Desarrollo Social\nLos datos los puedes encontrar en la carpeta de datos\n\n# Importar el ShapeFile\nagebs = gpd.read_file('datos/pob_sinderechohab.zip')\n\n# Graficar el GeoDataFrame\nagebs.plot(cmap = 'Set1', figsize = (10,10))\n\n<AxesSubplot: >\n\n\n\n\n\nAntes de continuar con la parte analítica, resulta pertinente crear un Mapa de Coropletas para, de entrada, tener una idea visual de cómo es que se distribuye la variable en el espacio. A continuación, se muestra el código necesario para construir este mapa, en el cual cabe resaltar el uso del argumento scheme dentro de la función .plot() para indicar cómo deben de ser clasificados los datos, para después asignarles el color correspondiente:\n\n# Preparación de la figura y sus filas\nfig , filas = plt.subplots(1, figsize = (10,10))\n\n# Gráfica del GeoDataFrame, generando un Mapa de Coropletas a través de 'scheme'\nagebs.plot(ax = filas, column = 'p_sderech', scheme = 'fisherjenks' , cmap = 'YlGn', legend = True)\n\n# Adición de título a la gráfica\nfig.suptitle('Población sin Derechohabiencia a Servicios de Salud Públicos en la CDMX', size = 20)\n\n# Mostrar la gráfica\nplt.show()\n\n\n\n\n\n9.2.0.1 Ejercicio Opcional\nCrea un mapa similar al anterior, pero utilizando como Métodos de Clasificación los Cuantiles (quantiles) e Intervalor Iguales (equalinterval), y responde las preguntas: ¿En qué difieren los mapas generados? ¿Cómo varía la interpretación de lo observado en cada uno de los casos? ¿Cuál es la distribución de los datos? Confirma esta última respuesta generando un Diagrama de Densidad de Kernel o un Histograma. ___ ## Matriz de Pesos Espaciales Una Matriz de Pesos Espaciales es la forma en la que el espacio geográfico es codificado de forma numérica para poder utilizar la estructura como base para el análisis. Pysal provee diferentes formas para calcular estas matrices a partir de conjuntos de datos geográficos. Podemos distinguir tres grandes grupos:\n\nContigüidad. Cuando la matriz obtiene las relaciones de vecindad a partir de la contigüidad en el espacio, es dicir, si los objetos comparten fronteras.\nBasados en distancia. En este caso los objetos espaciales son vecinos siempre que se encuentren a una distancia menos a un umbral especificado.\nBloques. Este tipo de matrices está dada por la pertenencia a una unidad de nivel superior, por ejemplo, podemos decir que todos los municipios pertenecientes a un mismo estado sonvecinos.\n\nComencemos por utilizar una vecindad de Reina para las AGEBs:\n\nm_reina = Queen.from_dataframe(agebs, idVariable='cvegeo')\nm_reina.transform = 'R'\n\n('WARNING: ', '090090015012A', ' is an island (no neighbors)')\n\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 8 disconnected components.\n There is 1 island with id: 090090015012A.\n  warnings.warn(message)\n\n\nEl parámetro transform obliga a que los pesos asignados a cada vecindad sumen \\(1\\), esto es importante para algunos cálculos estadísticos.\n\nm_reina['090150001056A']\n\n{'0901500010610': 0.25,\n '0901500010574': 0.25,\n '0901500010517': 0.25,\n '0901500010502': 0.25}\n\n\nAl calcular las vecindades nos salió una advertencia sobre la existencia de una isla, esto es, una observación que, bajo nuestro modelo de vecindad, no está conectyada a ninguna otra. Eliminemos esa observación para que no cause ruido.\n\nagebs = agebs.set_index('cvegeo').drop('090090015012A').reset_index()\n\nVolvemos a calcular nuestra matriz de vecindad\n\nm_reina = Queen.from_dataframe(agebs, idVariable='cvegeo')\nm_reina.transform = 'R'\n\n/home/plablo/.miniconda3/envs/geoinformatica/lib/python3.10/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 7 disconnected components.\n  warnings.warn(message)"
  },
  {
    "objectID": "parte_1/09_autocorrelacion.html#rezago-espacial",
    "href": "parte_1/09_autocorrelacion.html#rezago-espacial",
    "title": "9  Autocorrelación espacial",
    "section": "9.3 Rezago Espacial",
    "text": "9.3 Rezago Espacial\nUna vez que se encuentran listos los datos y la Matriz de Pesos Espaciales, es posible calcular el Rezago Espacial de la variable p_sderech. Importante recordar que el rezago espacial es el resultado del producto entre la Matriz de Pesos Espaciales y una variable dada y que, si \\(W\\) se encuentra estandarizada en filas, los resultados equivalen al valor promedio de una variable en la vecidad de una observación dada.\nEn la siguiente línea, se calcula el Rezago Espacial para la variable p_sderech y es almacenada directamente en la tabla original:\n\nagebs['rez_espacial'] = lag_spatial(m_reina , agebs['p_sderech'])\n\nPuede compararse entre sí la apariencia del rezago espacial calculado con respecto a su variable original:\n\nagebs[['p_sderech' , 'rez_espacial']].head()\n\n\n\n\n\n  \n    \n      \n      p_sderech\n      rez_espacial\n    \n  \n  \n    \n      0\n      389.0\n      337.0\n    \n    \n      1\n      323.0\n      419.5\n    \n    \n      2\n      448.0\n      484.5\n    \n    \n      3\n      240.0\n      409.0\n    \n    \n      4\n      1017.0\n      500.0\n    \n  \n\n\n\n\nLa forma en la que se interpreta rez_espacial para una observación dada es la siguiente: el AGEB con Clave ‘0901500010235’ (la segunda en la lista anterior) tiene una Población sin Derechohabiencia de 323 personas, y se encuentra rodeada por otras AGEB’s que, en promedio, tienen 419.5 Pesonas sin Derechohabiencia.\nPara ejemplificar lo anterior correctamente, pueden obtenerse primero los vecinos de la AGEB mencionada:\n\nm_reina.neighbors['0901500010235']\n\n['0901500010377',\n '0901500010381',\n '090150001024A',\n '0901500010362',\n '0901500010220',\n '0901500010076']\n\n\nY después, verificar los valores de las variables para estos vecinos:\n\nvecinos = agebs.set_index('cvegeo').loc[m_reina.neighbors['0901500010235'] , 'p_sderech']\nvecinos\n\ncvegeo\n0901500010377     226.0\n0901500010381     267.0\n090150001024A     226.0\n0901500010362     324.0\n0901500010220     146.0\n0901500010076    1428.0\nName: p_sderech, dtype: float64\n\n\nDe forma sencilla, puede calcularse el valor promedio de estos valores, en cuyo caso podemos comprobar que es muy similar al Rezago Espacial calculado para dicha observación:\n\nvecinos.mean()\n\n436.1666666666667\n\n\nPara algunas técnicas que se estudiarán más adelante, tiene más sentido el operar con la versión estandarizada de la variable, en lugar de sus valores brutos; estandarizar significa sencillamente el restar el valor de la media de la variable y dividirlo por la desviación estándar de cada observacion de la columna. Esto se consigue rápidamente a través de los siguientes comandos:\n\nagebs['p_sderech_std'] = (agebs['p_sderech'] - agebs['p_sderech'].mean()) / agebs['p_sderech'].std()\nagebs.head()\n\n\n\n\n\n  \n    \n      \n      cvegeo\n      alcaldia\n      p_sderech\n      p_nescu\n      p_hacin\n      p_analf\n      geometry\n      rez_espacial\n      p_sderech_std\n    \n  \n  \n    \n      0\n      0900700013628\n      09007\n      389.0\n      475.0\n      0.0\n      9.0\n      POLYGON ((2810132.372 824698.172, 2810169.540 ...\n      337.0\n      -0.959096\n    \n    \n      1\n      0901500010235\n      09015\n      323.0\n      481.0\n      6.0\n      7.0\n      POLYGON ((2798881.634 831643.241, 2798843.076 ...\n      419.5\n      -1.034215\n    \n    \n      2\n      0900200010097\n      09002\n      448.0\n      780.0\n      5.0\n      37.0\n      POLYGON ((2792415.239 836846.390, 2792356.808 ...\n      484.5\n      -0.891944\n    \n    \n      3\n      0900200011184\n      09002\n      240.0\n      389.0\n      0.0\n      11.0\n      POLYGON ((2792260.139 836768.777, 2792333.695 ...\n      409.0\n      -1.128684\n    \n    \n      4\n      0900300011285\n      09003\n      1017.0\n      1291.0\n      0.0\n      23.0\n      POLYGON ((2802121.600 817466.682, 2802124.157 ...\n      500.0\n      -0.244323\n    \n  \n\n\n\n\nY, de forma análoga, debe de calcularse el Rezago Espacial para estos valores estandarizados:\n\nagebs['rez_espacial_std'] = lag_spatial(m_reina , agebs['p_sderech_std'])\nagebs.head()\n\n\n\n\n\n  \n    \n      \n      cvegeo\n      alcaldia\n      p_sderech\n      p_nescu\n      p_hacin\n      p_analf\n      geometry\n      rez_espacial\n      p_sderech_std\n      rez_espacial_std\n    \n  \n  \n    \n      0\n      0900700013628\n      09007\n      389.0\n      475.0\n      0.0\n      9.0\n      POLYGON ((2810132.372 824698.172, 2810169.540 ...\n      337.0\n      -0.959096\n      -1.018281\n    \n    \n      1\n      0901500010235\n      09015\n      323.0\n      481.0\n      6.0\n      7.0\n      POLYGON ((2798881.634 831643.241, 2798843.076 ...\n      419.5\n      -1.034215\n      -0.924382\n    \n    \n      2\n      0900200010097\n      09002\n      448.0\n      780.0\n      5.0\n      37.0\n      POLYGON ((2792415.239 836846.390, 2792356.808 ...\n      484.5\n      -0.891944\n      -0.850400\n    \n    \n      3\n      0900200011184\n      09002\n      240.0\n      389.0\n      0.0\n      11.0\n      POLYGON ((2792260.139 836768.777, 2792333.695 ...\n      409.0\n      -1.128684\n      -0.936332\n    \n    \n      4\n      0900300011285\n      09003\n      1017.0\n      1291.0\n      0.0\n      23.0\n      POLYGON ((2802121.600 817466.682, 2802124.157 ...\n      500.0\n      -0.244323\n      -0.832759"
  },
  {
    "objectID": "parte_1/09_autocorrelacion.html#autocorrelación-espacial-global",
    "href": "parte_1/09_autocorrelacion.html#autocorrelación-espacial-global",
    "title": "9  Autocorrelación espacial",
    "section": "9.4 Autocorrelación Espacial Global",
    "text": "9.4 Autocorrelación Espacial Global\nLa Autocorrelación Espacial Global se relaciona con el patrón geográfico general presente en un conjunto de datos. Los estadísticos diseñados para medir ésta caracterizan a los datos espaciales en términos de su grado de agrupamiento (clustering) y lo resumen. Este resumen puede ser visual o numérico; en esta sección, se estudiará un ejemplo de cada uno de éstos: el Gráfico de Moran, y el Estadístico de I de Moran de Autocorrelación Espacial\n\n9.4.1 Gráfico de Moran\nEl Gráfico de Moran es una forma de visualizar un conjunto de datos espaciales para explorar la naturaleza y robustez de la Autocorrelación Espacial. En escencia se trata de un Diagrama de Dispersión tradicional en el que la variable de interés se contrasta contra su Rezago Espacial. Para poder interpretar los valores en términos de la media, y sus cantidades en términos de desviaciones estándar, la variable de interés usualmente se estandariza.\nDesde el punto de vista técnico, crear un Gráfico de Moran en Python sigue un proceso muy similar al de calcular cualquier otro Diagrama de Dispersión, dado que se tenga la variable estandarizada y su Rezago Espacial de antemano:\n\n# Preparación de la Gráfica\nfig, filas = plt.subplots(1, figsize = (10,10))\n\n# Generación del Diagrama de Dispersión\nsns.regplot(x = 'p_sderech_std' , y = 'rez_espacial_std', data = agebs)\n\n# Adición de lineas horizontal y vertical\nplt.axvline(0, c='k', alpha=0.5)\nplt.axhline(0, c='k', alpha=0.5)\n\n# Muestra del resultado\nplt.show()\n\n\n\n\nLa gráfica anterior muestra la relación entre la variable estandarizada (p_sderech_std) y su rezago espacial (rez_espacial_std) el cual, debido a la estandarización del Matriz de Pesos Espaciales, se puede interpretar como el promedio de la variable en su vecindad. Para facilitar la interpretación de la gráfica, también se incluye una Regresión Lineal, junto con sus intervalos de confianza; esta línea representa el mejor ajuste para el Diagrama de Despersión o, en otras palabras, cuál es la mejor manera de representar la relación entre las dos variables en una línea recta. Debido a que la línea proviene de una regresión, también puede incluirse una medida de la incertidumbre sobre la regresión en la forma de Intervalos de Confianza (el área sombreada alrededor de la línea).\nLa gráfica muestra una relacíon positiva entre ambas variables. Esto se asocia directamente con la presencia de una Autocorrelación Espacial Positiva: los valores similares tienden a encontrarse cercanos entre sí. Lo anterior significa que el Patrón General es que los valores altos se encuentran cercanos a los valores altos, y viceversa. De todas formas, esto no significa que ésta sea la única situacíon en el conjunto de datos; deben de exitir casos particulares en los que valores altos se encuentren rodeados de valores bajos, y lo contrario también es cierto. Lo que sí significa es que, si se tuviera que resumir el patrón principal de los datos en términos de qué tan agregados se encuentran valores similares, la mejor forma sería decir que se encuentran correlacionados positivamente y, por ende, agrupados en el espacio.\nEn el contexto de este ejemplo, esto puede ser interpretado como que las AGEB’s donde existe un gran número de personas sin derechohabiencia a servicios públicos de salud se encuentran rodeadas por otras AGEB’s donde ocurre el mismo fenómeno.\n\n\n9.4.2 I de Moran\nEl Gráfico de Moran es una herramienta excelente para explorar los datos y tener una buena idea de qué tanto se encuentran agregados los valores en el espacio. Sin embargo, debido a que se trata de una herramienta gráfica, resulta en ocasiones difícil el condensar cualquier conclusión de forma más concisa. Para estos casos, resulta útil encontrar un método estadístico que resuma las conclusiones a las que se pretende llegar; esto es exactamente lo que la I de Moran pretende hacer.\nDe la misma forma en la que la Media resume un elemeto crucial de la distribución de los valores en un contexto no espacial, la I de Moran también lo hace para un conjunto de datos con la componente espacial. Continuando con la comparación, puede pensarse en la Media como un único valor numérico que describe un Histograma o un Gráfico de Densidad de Kernel; de forma similar, la I de Moran captura mucha de la escencia del Gráfico de Moran. De hecho, inclusive existe una conección entre ambos: el valor de la I de Moran corresponde con la pendiente de la Regresíon Lineal presente en la gráfica.\nPara calcular la I de Moran del conjunto de datos, se puede llamar directamente a una función de PySAL:\n\nmoran = esda.Moran(agebs['p_sderech'], m_reina)\nmoran\n\n<esda.moran.Moran at 0x7f25687ca860>\n\n\nCabe destacar como no es necesario utilizar la versión estandarizada de la variable en el comando anterior, pues no se producirá una representación visual.\nLa función anterior crea un objeto que contiene mucha más información que el propio estadístico aislado. Para recupear el valor de la I de Moran, puede recurrise a la propiedad I del objeto generado:\n\nmoran.I\n\n0.12761494747644506\n\n\nLa otra información pertinente para la interpretación de la I de Moran se realaciona con la Inferencia Estadística: ¿Qué tan probable es que el patrón que se observa en el mapa y su I de Moran sean generado por un proceso completamente aleatorio? Si se considerara exactamente la misma variable, pero se distribuyera su ubicación al azar, ¿se obtendría un mapa con características similares?\nLos detalles específicos del mecanismo para calcular esto rebasa el alcance de esta práctica, pero resulta importante saber que un valor lo suficientemente pequeño de p-value asociado a la I de Moran de un mapa permite rechazar la hipótesis de que el conjunto de datos se encuentra distribuido de forma aleatoria. En otras palabras, puede concluirse que los datos muestran un Patrón Espacial mayor al que se esperaría si los valores hubiesen sido localizados de forma aleatoria en el espacio.\nEl valor más confiable de p-value para la I de Moran puede ser encontrado a través del atributo p_sim:\n\nmoran.p_sim\n\n0.001\n\n\nEsta significancia es mucho menor que el 1% y, en términos generales, se consideraría que se trata de un estadístico significativo. Nuevamente, una explicación detallada del verdadero significado de esto y sus implicaciones rebasan el objetivo de la práctica, pero se pueden realizar algunas interpretaciones rápidas: lo que el 0.001 (0.1%) significa es que, si se generaran un enorme número de mapas con exactamente los mismos valores, pero distribuídos de formas diferentes, y se calculará la I de Moran para cada uno de estos mapas, sólo el 0.1% mostrarían un valor del estadístico mayor al que se está obteniendo de los datos reales, mientras que el resto de los mapas generados al azar tendrían un valor menor. Si se recuerda que el valor de la I de Moran puede ser interpretado como la pendiente del Gráfico de Moran, lo que se tiene es que, para este caso, el acomodo particular de los valores en el espacio para la variable de personas sin derechohabiencia se encuentra mucho más concentrado que si se intercambiaran la cantidad de personas entre todas las AGEB’s de la CDMX, razón por la cual el estadístico se vuelve significativo.\nComo un primer paso, el Análisis de Autocorrelación Global muestra que las observaciones guardan una correlación positiva entre sí en el espacio. En términos de la variable que se está analizando, puede empezarse a suponer que el espacio juega un papel importante en el valor que ésta adquirirá dentro de cada AGEB. ___ ## Autocorrelación Espacial Local La I de Moran es útil para resumir un conjunto de datos en un sólo valor que informa sobre su grado de agrupamiento (Clustering). Sin embargo, no es una medida apropiada para identificar las áreas dentro del mapa en las que ciertos valores se ubican; en otras palabras, la I de Moran permite determinar si los valores se encuentra agrupados en general, pero no informará sobre dónde se encuentran estos agrupamientos.\nPara este propósito, es necesario utilizar una medida local de la Autocorrelación Espacial. Las medidas locales consideran a cada una de las observaciones de un conjunto de datos y trabajan sobre ellas, al contrario de la generalidad de los datos, como hacen las medidas globales. Debido a esto, no son buenas para resumir la información de un mapa, pero si para obtener un mayor transfondo del mismo.\nEn esta práctica, se considerarán Indicadores Locales de Asociación Espacial (LISA’s, en inglés), una contraparte local de las medidas globales. En el corazón de este método se encuentra la clasificación de las observaciones de un conjunto de datos entre cuatro grupos derivados del Gráfico de Moran: valores altos rodeados por valores altos (HH), valores bajos rodeados por valores bajos (LL), valores altos rodeados de bajos (HL) y viceversa (LH). Cada uno de estos grupos son comúnmente llamados Cuadrantes; ilustrativamente, se pueden observar de la siguiente manera:\n\n# Preparación de la Gráfica\nfig , filas = plt.subplots(1, figsize = (10,10))\n\n# Generación de la Gráfica\nsns.regplot(x = 'p_sderech_std' , y = 'rez_espacial_std', data = agebs)\n\n# Adición de Línas\nplt.axvline(0, c='k', alpha=0.5)\nplt.axhline(0, c='k', alpha=0.5)\n\n# Definición de límites según número de Desviaciones Estándar\nfilas.set_xlim(-2, 5)\nfilas.set_ylim(-2.5, 2.5)\n\n# Adición de nombre de los cuadrantes\nplt.text(3, 1.5, \"HH\", fontsize=25)\nplt.text(3, -1.5, \"HL\", fontsize=25)\nplt.text(-1, 1.5, \"LH\", fontsize=25)\nplt.text(-1, -1.5, \"LL\", fontsize=25)\n\n# Gráfica Resultante\nplt.show()\n\n\n\n\nHasta ahora se ha clasificado cada observación del conjunto de datos en función de su valor o el de sus vecinos; esto es sólo la mitad del trabajo de identificar las áreas con concentraciones inusuales de valores. Para saber la ubicación de cada uno de las aglomeraciones (clusters) estadísticamente significativos de cualquier tipo, primero es necesario comparar los datos reales con lo que se esperaría obtener con datos localizados aleatoriamente. Después de todo, por definición, toda observación será de algún tipo según lo observado en el Gráfico de Moran; sin embargo, lo que resulta de interés es saber la significancia de la concentración de estos valores.\nEsto es exactamente para lo que los métodos LISA’s fueron diseñados. Como ha ocurrido anteriormente, una descripción mucho más detallada del procedimiento estadístico involucrado rebasa el propósito de la práctica, pero se analizará un poco la lógica detrás del proceso. La idea central es el identificar casos en los que la comparación entre el valor de una observación y el promedio de sus vecinos es muy similar (HH y LL) o diferente (HL y LH) de lo que se esperaría obtener por mera suerte; el mecanismo para hacer esto es similar al existente en la I de Moran Global, pero aplicado para cada observación, resultando en tener tantas estadísticos como observaciones.\nLos métodos LISA’s son utilizados en muchos campos para encontrar agrupaciones de valores en el espacio. Son una herramienta bastante útil que permite obtener rápidamente áreas en las que los valores se encuentren concentrados, y proveen de evidencia sugestiva sobre el proceso que puede originar lo observado; debido a esto, resultan una herramienta clave en el análisis espacial. Ejemplos en los que los métodos LISA pueden ser de utilidad incluyen la identificación de aglomeraciones (clusters) espaciales de pobreza en una región, detección de enclaves étnicos, delineación de áreas con mucha o poca presencia de un fenónemo, entre muchos otros.\nEn Python, el cálculo de LISA’s es bastante sencillo gracias a las facilidades de PySAL:\n\nlisa = esda.Moran_Local(agebs['p_sderech'] , m_reina)\nlisa\n\n<esda.moran.Moran_Local at 0x7f25db79b100>\n\n\nLo único que se necesita es establecer la variable de interés (Personas sin Derechohabiencia, en este caso), y la Matriz de Pesos Espaciales que describe las vecindades de cada una de las observaciones del conjunto de datos.\nDebido a su naturaleza intrínseca, observar los resultados numéricos de LISA no siempre resulta la manera más útil de aprovechar toda la información que pueden proporcionar. LISA implica calcular un estadístico para todas y cada una de las observaciones del conjunto de datos por lo que, si se tienen muchos de ellos, será difícil descifrar cualquier patrón. Como tal, lo que normalmente se hace es crear un mapa, llamado Mapa de Clusters, que extrae las observaciones significativas (aquellas que muy probablemente no derivaron de la aleatoriedad) y asignarles un color en particular según la categoría del cuadrante.\nTodas las piezas necesarias para armar este mapa se encuentran contenidas en el objeto lisa creado en el comando anterior. Pero, para que el mapa sea más fácil de comprender, es conveniente retirarlos del objeto y colocarlos en la tabla principal:\n\n# Determinar si una observación es significativa o no\nagebs['significativo'] = lisa.p_sim < 0.05\n\n# Almacenar el cuadrante al cual pertenecen\nagebs['cuadrante'] = lisa.q\n\nDe forma similar a como ocurre con la I de Moran Global, PySAL automáticamente calcula un p-value para cada LISA; debido a que no todas las observaciones son estadísticamente significativas, lo que se busca es identificar a éstas con un p-value lo suficientemente pequeño para asegurar que no fueron producto del azar. Tomando como significativos a todos los valores con un p-value menor al 5%, se crea la columna significativo que contiene True si el p-value de la observación satisface esa condición, y False si no es así. Esto puede ser verificado sencillamente:\n\nagebs[['cvegeo','significativo']].head()\n\n\n\n\n\n  \n    \n      \n      cvegeo\n      significativo\n    \n  \n  \n    \n      0\n      0900700013628\n      True\n    \n    \n      1\n      0901500010235\n      True\n    \n    \n      2\n      0900200010097\n      True\n    \n    \n      3\n      0900200011184\n      True\n    \n    \n      4\n      0900300011285\n      True\n    \n  \n\n\n\n\nY los primeros cinco p-values pueden ser obtenidos de la siguiente forma:\n\nlisa.p_sim[:5]\n\narray([0.011, 0.009, 0.014, 0.008, 0.017])\n\n\nDebido a que todos los valores son menores a 0.05, todas son asignadas un True dentro de la tabla.\nEn términos del cuadrante al que corresponde cada observación, este puede ser verificado fácilmente pues forma parte del objeto lisa directamente:\n\nagebs[['cvegeo','cuadrante']].head()\n\n\n\n\n\n  \n    \n      \n      cvegeo\n      cuadrante\n    \n  \n  \n    \n      0\n      0900700013628\n      3\n    \n    \n      1\n      0901500010235\n      3\n    \n    \n      2\n      0900200010097\n      3\n    \n    \n      3\n      0900200011184\n      3\n    \n    \n      4\n      0900300011285\n      3\n    \n  \n\n\n\n\nLa correspondencia entre los números de la variable y el nombre de los cuadrantes es la siguiente:\n\n1 - HH\n2 - LH\n3 - LL\n4 - HL\n\nCon estos dos elementos, significativo y cuadrante, es posible construir un Mapa de Clusters tradicional, combinando todo lo ya aprendido sobre la construcción de mapas en Python:\n\nfig , filas = plt.subplots(1 , figsize = (10,10))\nagebs.plot(color = 'grey' , linewidth = 0.5 , ax = filas)\n\n# AGEB's del tipo Alto-Alto (Rojo Oscuro)\nhh = agebs.loc[(agebs['cuadrante'] == 1) & (agebs['significativo'] == True) , 'geometry']\nhh.plot(ax = filas , color = 'red')\n\n# AGEB's del tipo Bajo-Alto (Azul Claro)\nll = agebs.loc[(agebs['cuadrante'] == 3) & (agebs['significativo'] == True) , 'geometry']\nll.plot(ax = filas , color = 'blue')\n\n# AGEB's del tipo Bajo-Bajo (Azul Oscuro)\nlh = agebs.loc[(agebs['cuadrante'] == 2) & (agebs['significativo'] == True) , 'geometry']\nlh.plot(ax = filas , color = '#83cef4')\n\n# AGEB's del tipo Alto-Bajo (Rojo Claro)\nhl = agebs.loc[(agebs['cuadrante'] == 4) & (agebs['significativo'] == True) , 'geometry']\nhl.plot(ax = filas , color = '#e59696')\n\n<AxesSubplot: >\n\n\n\n\n\nEl mapa anterior muestra las AGEB’s de la Ciudad de México y las colorea en función cómo han sido identificadas según los métodos LISA, únicamente si son considerados estadísticamente significativos por los mismos. De color rojo muy saturado se encuentran las AGEB’s con números muy altos de Personas sin Derechohabiencia, que tienden a concentrarse en los límites de la ciudad; de color rojo pastel, aquellos que tienen una alta cantidad de esta población, pero que se encuetran en zonas donde éste no es el patrón buscado; de color azul, zonas donde hay una cantidad muy pequeña de personas sin derechohabiencia, que se concentran en el interior de la ciudad; por último, de azul pastel, las zonas donde esta población es pequeña, pero el patrón en la vecindad es que sea alta.\nPara que los resultados obtenidos cobren sentido, es necesario relacionar lo observado con el objetivo del proyecto con el que se esté trabajando. Comúnmente, este tipo de análisis se realiza para comprobar, a través de cierta rigidez estadística, que la componente espacial tiene una influencia importante sobre los valores observados; en otras palabras, sería posible concluir a partir de las pruebas anteriores que el Número de Personas sin Derechohabiencia que se registren en un AGEB depende, de una forma u otra, de la ubicación del AGEB.\nClaro está que esta conclusión no es completamente determinante, pues, dado que son datos obtenidos de situaciones reales, existen un sinnúmero de factores que pueden influir sobre la variable estudiada, y que no necesariamente se están considerando dentro del procedimiento. Sin embargo, la ventaja de los métodos anteriores es que ofrecen una base estadísticamente válida para continuar trabajando con otros métodos del Análisis Espacial, y así acercarse más al objetivo deseado.\n\n9.4.2.1 Ejercicio Opcional\nGenera un mapa similar al anterior, pero esta vez únicamente mostrando las AGEB’s clasificadas como Alto-Alto (HH), ignorando cualquier otro Valor Atípico Espacial. ¿Qué es lo que se puede concluir? ___ ## Para Practicar… Replica todo el análisis anterior para alguna de las otras variables contenidas en el ShapeFile original, siendo: * p_nescu - Población de 15 a 24 años que no asiste a la escuela * p_hacin - Población que vive en Hacinamiento * p_analf - Población de 15 años o más analfabeta\nPara la variable que hayas escogido, realiza lo siguiente: * Crea un Mapa de Coropletas. * Obten la Matriz de Pesos Espaciales, utilizando el criterio (Contigüidad, Distancia, Bloques) que consideres más óptimo. Justifica la elección del criterio. * Estandariza la variable elegida, y obten su rezago espacial. * Crea la Gráfica de Moran * Realiza un Análisis LISA y genera un mapa de los resultados obtenidos. Responde a la pregunta, ¿cuál es el patrón observado?"
  },
  {
    "objectID": "parte_2.html",
    "href": "parte_2.html",
    "title": "Geoinformática en R",
    "section": "",
    "text": "Esta parte del libro cubre el manejo de datos espaciales utilizando R\nEN CONSTRUCCIÓN"
  },
  {
    "objectID": "parte_2/intro_R.html",
    "href": "parte_2/intro_R.html",
    "title": "10  Introducción a R",
    "section": "",
    "text": "EN CONSTRUCCIÓN"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]